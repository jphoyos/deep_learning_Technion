var store = [{
        "title": "Using the course HPC servers",
        "excerpt":"Those of you who are officially enrolled to the course and requested accessusing the server registration form will have access to dedicatedhigh-performance computing servers (lambda1-5) provisioned by ComputerScience faculty IT department. Running on the faculty servers will give youaccess to more computing power and also fast GPUs (which will greatlyaccelerate your deep-learning tasks). This should significantly speed up yourworkflow when performing the course homework assignments and when implementingyour final project. These servers are mainly suited for running batch jobs which you can submit todedicated job queues and be notified upon completion.  We therefore recommendyou install and work on the assignments locally (on your own machine), and use the faculty servers mainly to run long model training tasks (we willspecify in the assignment). Logging in Logging in is performed with your Technion Single Sign-On (SSO) credentials.Usually this means the username and password of your @campus or @technionemail address. If your username is e.g. user, login like so ssh user@lambda.cs.technion.ac.ilor, use the server’s IP directly if you’re connecting over a VPN or receive anunknown hostname error: ssh user@132.68.39.159Notes:   Your credentials will only work after we pass a final list ofregistered students to the faculty IT department.This will happen during the first 2-3 weeks of the semester.  These servers are only directly accessible from within the Technion networks.If connecting over WiFi, do not use the TechPublic network, as it won’tallow you to connect. The TechSec network will work, as well as othernon-open faculty networks (e.g. CS-WIFI).  lambda is a gateway server that you connect to in order to run jobs on theactual compute nodes (lambda1-5) as explained below.You should not run any computations on lambda itself as it does not have aGPU and is limited in CPU resources.  In some internal Technion networks, and when connecting through VPN, the DNSlookup does not find the lambda hostname. If you get a could notresolve hostname error, use the second option (directly with IP).Connecting from home To connect from home you’ll need to configure a VPN connection to the Technion. See theinstructionson the Technion CIS website regarding how to set this up. After you connectthough the VPN, you can connect to the server as normal. Note that we cannot provide you with technical support regarding how tosetup/use the VPN. You should contact CIS if you need VPN support. Server Usage General Storage and Environment Your home directory on the gateway server lambda (e.g. /home/user) isautomatically mounted on all the computation nodes lambda1-5. This ensuresthat any programs you install locally under your home folder (for example aconda environment) will be available for jobs running on these nodes. In fact, the first thing you should do after connecting for the first timeis to install conda and the course conda environment within your homefolder. Follow the instructions for linux on the getting started page. Note that your home folder will be deleted from the server at some point afterthe semester ends, likely without prior notice. Please make sure to backup your files. Computation The faculty HPC server cluster is composed of a gateway server, lambda, into whichyou log in with SSH, and five compute nodes lambda1-5 which run the actualcomputations. The gateway server is relatively weak and has no attached GPUs, soit should not be used for running computations. Again, to be clear: do not run any computations directly on lambda! afterlogging in, only run computations through slurm as described below. The computation tasks are manged by a job scheduling system calledslurm.  The system manages the computationnodes and resources and allocates them to jobs submitted by users into a queue.If you wish, you can read the slurm quick startguide to get a better understandingof the system and the available commands. The most useful slurm commands for our needs are,   sbatch  srun  squeue  scancelRunning interactive jobs An interactive job allows you to view it’s output and interact with it inreal time, as if it were running on the machine you’re loggedin to. Submitting an interactive job is performed with the srun command. Required resourcescan be specified and if they’re available the job starts running immediately. Example Let’s see how to run an ipython console session as an interactive job with anallocated GPU (notice that the course conda env is already active). (cs236781-hw) avivr@lambda:~/cs236781-hw$ srun -c 2 --gres=gpu:1 --pty ipythonPython 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05)Type 'copyright', 'credits' or 'license' for more informationIPython 7.19.0 -- An enhanced Interactive Python. Type '?' for help.In [1]: import torchIn [2]: torch.cuda.is_available()Out[2]: TrueIn [3]: t = torch.tensor([1,2,3], dtype=torch.float).cuda()In [4]: t.dot(t)Out[4]: tensor(14., device='cuda:0')Here the -c 2 and --gres=gpu:1 options specify that we want to allocate 2 CPUcores and one GPU to the job, the --pty option is required for the sessionto be interactive and the last argument ipython is the command to run. You canspecify any command and also add command arguments after it. Notes:   You should use interactive jobs for debugging or running short tasks likelaunching jupyter. If you need to run something long, it’s better tosubmit a batch job instead (see below).  When you submit an interactive job, your shell is blocked (by srun) untilit completes. If you terminate srun, it will cancel your job. Crucially,this means that if you log out of the machine while running an interactivejob, the job will terminate (as with regular processes you invoke from theshell). You can get around this by either,          Using terminal managers e.g. screen and tmux;      Running with nohup;      Running a batch job instead (preferred). See below.        You should activate your conda env before running an interactive job ifyou need to run python.The shell environment variables will be passed to the process that will runyour job on the compute node, so therefore the conda env will effectivelyalso be active there.  You can specify bash as the command to run in an interactive job to get ashell on one of the compute nodes.  Jobs may be terminated after running for more than 24 hours due to policy.Running batch jobs A batch job is submitted to the queue with the sbatch command.It runs non-interactively when resources are available and sends it’s output tofiles that you can specify. Additionally, it can notify you by email when thejob starts and finishes. Running jobs with sbatch is useful for long-running processes such astraining models or running experiments. While the job is running, it’s notconnected to any specific shell session and thus it keeps running if you logout of the machine. To view output from a batch job, you’ll need to read itfrom the file it writes to. To use sbatch, you need to create a script for it to run. It can be any scriptwith a valid shebang line (#!) at the top, e.g. a bash script or a pythonscript. Example Lets create a file ~/myscript.sh on the server with the following contents: #!/bin/bash# Setup envsource $HOME/miniconda3/etc/profile.d/conda.shconda activate cs236781-hwecho \"hello from $(python --version) in $(which python)\"# Run some arbitrary pythonpython -c 'import torch; print(f\"i can haz gpu? {torch.cuda.is_available()}\")'Then we can run the script as a slurm batch job as follows: # Run a batch job with slurmavivr@lambda:~$ sbatch -c 2 --gres=gpu:1 -o slurm-test.out -J my_job  myscript.shSubmitted batch job 3550# Check the job status in the queueavivr@lambda:~$ squeue              JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)              3550       all   my_job    avivr  R       0:01      1 lambda2# See the output from the job by following the output file contents(cs236781-hw) avivr@lambda:~/cs236781-hw$ tail -f slurm-test.out/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)hello from Python 3.8.6 in /home/avivr/miniconda3/envs/cs236781-hw/bin/pythoni can haz gpu? True^CHere the -c 2 and --gres=gpu:1 options specify that we want to allocate 2 CPUcores and one GPU to the job, the -o slurm-test.out option specifies whereto write the output from the process and -J my_job is an arbitrary name we canassign to the job. Viewing status After submitting a batch job, you can use squeue to view it’sstatus in the queue, as shown in the example above. You can see the job name andit’s id there. Viewing output Each job you submit causes a text file you be created in your current directory,named slurm-&lt;jobid&gt;.out. To view the output from a job in real time, you can use tail -f or less -r+F on the output file for the relevant job. less allows you to also scrollback. Canceling To cancel a batch job you’ve submitted (whether it’s running or waiting in thequeue), run scancel &lt;job-id&gt; where &lt;job-id&gt; is the id you received whenstarting the batch job. Course helper script To slightly simplify your workflow on the server, we provide you with a simplescript to run python code from the course conda env as a slurm batch job. The homework assignment repos contain a script called py-sbatch.sh. You canuse this script as if it were the python command, and it will active theconda env for you and execute your provided python code with sbatch. For example, let’s say we want to run all our notebooks with the main.pyscript. Instead of conda activate cs236781-hwpython main.py run-nb *.ipynbwhich will run on the gateway server, do this ./py-sbatch.sh main.py run-nb *.ipynbThis will take care of activating the conda env and run the script on the morepowerful compute nodes as a batch job. The script has some declared variableswhich you can edit to configure the sbatch parameters such as computationalresources, notification email address and others. Note that for the above example it may have been more straightforward to use aninteractive job (srun). However this script may be useful when you need tocreate a batch job running a python script, for example to run long trainingtasks. In any case, this script is completely optional since you can always usesbatch directly as shown in the previous section. Running jupyter You can run jupyter on a compute node by running a small script what weprovide, jupyter-lab.sh. You should run this script via srun, like so: (cs236781-hw) avivr@lambda:~/cs236781-hw$ srun -c2 --gres=gpu:1 jupyter-lab.sh[I 22:47:14.620 LabApp] JupyterLab extension loaded from /home/avivr/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/jupyterlab[I 22:47:14.621 LabApp] JupyterLab application directory is /home/avivr/miniconda3/envs/cs236781-hw/share/jupyter/lab[I 22:47:14.624 LabApp] Serving notebooks from local directory: /home/avivr/cs236781-hw[I 22:47:14.624 LabApp] Jupyter Notebook 6.1.4 is running at:[I 22:47:14.624 LabApp] http://132.68.39.38:8888/?token=5d426de30111ee82c7af4789077dbc4cf7a996f9e46476e6[I 22:47:14.624 LabApp]  or http://127.0.0.1:8888/?token=5d426de30111ee82c7af4789077dbc4cf7a996f9e46476e6[I 22:47:14.624 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 22:47:14.632 LabApp]    To access the notebook, open this file in a browser:        file:///home/avivr/.local/share/jupyter/runtime/nbserver-20063-open.html    Or copy and paste one of these URLs:        http://132.68.39.38:8888/?token=5d426de...     or http://127.0.0.1:8888/?token=5d426de...The connection URL in the console will show the IP of the compute node that theserver is actually running on. Use the first url to open jupyter in yourbrowser. Note: Please be considerate of other students and use the computingresources only as needed. Do not leave unattended jupyter notebooks justrunning without a reason. Tips Public-key based authentication You can use a public-key based authentication to prevent the need for typingyour password when connecting to remote servers over SSH.   Generate an SSH key pair using the ssh-keygen tool. More detailed instructions for all platforms can be foundhere.  Copy the public key. By default it’s in ~/.ssh/id_rsa.pub. Make sure you copy it exactlywithout any extra spaces or newlines.  Connect to your user on the machine and paste the public key contents into a new line in ~/.ssh/authorized_keys.Notes:       On macOS and linux, there’s a utility you can use to automate steps 2-3.After generating the key pair, copy the public key to the server like so:      ssh-copy-id user@lambda.cs.technion.ac.il            If you use an intermediate server to connect from home, make sure to firstalso copy your public key to that server.   After generating your key pair, you should also add it to your githubaccount.After that, you can use the SSH remote-URLs (instead of HTTPS) to clone reposand prevent the need to specify your username and password whenpushing, pulling and fetching. Transferring files to and from the server The rsync tool can be helpful. It can automatically sync between local andremote folders, only uploading/downloading modified files. For example, to send files or a directory you can do rsync -Crvz path/to/local/file_or_dir user@lambda:/home/user/path/to/remote/file_or_dirTo download files from the server to your computer, simply change the order ofthe last two arguments in the above example. GUI option for macOS users Cyberduck is a free remote file browser that you canuse to copy files to/from the server using a GUI. GUI option Windows users Many people recommend MobaXterm asa good graphical ssh client for windows. Additional Information Please also read the Faculty’s lambda help page. It contains other important information you need to know:   Limits on resources for jobs (number of hours, maximal GPUs, etc).  How jobs are prioritized between different students of the course,and between different courses.  Code of conduct. Failure to meed this code will cause revocation of youraccount.","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/assignments/hpc-servers",
        "teaser":null},{
        "title": "Getting started with the course assignments",
        "excerpt":"This document will help you get started with the course homework assignments.Please read it carefully as it contains crucial information. General The course homework assignments are mandatory and a large part of the grade.They entail writing code in python using popular third-party machine-learninglibraries and also theoretical questions. The assignments are implemented in part on a platform called Jupyter notebooks.Jupyter is a widely-used tool in the machine-learningecosystem which allows us to create interactive notebooks containing live code,equations and text. We’ll use jupyter notebooks to guide you through theassignments, explain concepts, test your solutions and visualize their outputs. To install and manage all the necessary packages and dependencies for theassignments, we use conda, a popular package-manager forpython.  The homework assignments come with an environment.yml file whichdefines what third-party libraries we depend on. Conda will use this file tocreate a virtual environment for you. This virtual environment includes pythonand all other packages and tools we specified, separated from any preexistingpython installation you may have. Detailed installation instructions are below.We will not support any other installation method other than the onedescribed. For working on the code itself, we recommend usingPyCharm, however you can use any othereditor or IDE if you prefer. You can obtain the professional version of PyCharmfor free by using your Technion student email (seehere). Project structure Each assignment’s root directory contains the following files and folders:   cs236781: Python package containing course utilities and helper functions.You do not need to edit anything here.  hwN where N is the assignment number: Python package containing theassignment code. All your solutions will be implemented here, includinganswers to questions.  PartN_XYZ.ipynb where N is a number and XYZ is some name:A set of jupyter notebooks that contain the instructions that will guide youthrough the assignment. You do not need to edit these. However, youshould write your name(s) at the beginning of Part0.  main.py: A script providing some utilities via a CLI.You’ll run it to create your submission after completing theassignment.  environment.yml: A file for conda, specifying the third-party packages itshould install into the virtual environment it creates. Note that everyassignment could define a different environment.Environment set-up       Install the python3 version of miniconda.Follow the installation instructionsfor your platform.     For example, on linux you should do:      curl -fsSLO https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh # Accept EULA # Install in default directory # Select no for editing .bashrc # Update your bashrc like so: echo \"source $HOME/miniconda3/etc/profile.d/conda.sh\" &gt;&gt; ~/.bashrc        On macOS it’s similar but with a different script URL      curl -fsSLO https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh  bash Miniconda3-latest-MacOSX-x86_64.sh # Rest is the same        On Windows, download the installer and follow the instructions on the condawebsite. See also the Windows-specific notes below before you proceed.     Configure conda to use strict channel priority (will speed up solving theenvironment) and to not automatically activate the default env (will forceyou to activate a specific env):     conda config --set channel_priority strict conda config --set auto_activate_base False            Use conda to create a virtual environment for the assignment.From the assignment’s root directory, run      conda env update -f environment.yml        This will install all the necessary packages into a new conda virtualenvironment named cs236781-hwN (where N is the assignment number).         Activate the new environment by running e.g.      conda activate cs236781-hw1        (change hw1 to hw2, etc. for each assignment).     Activating an environment simply means that the path to its python binaries(and packages) is placed at the beginning of your $PATH shell variable.Therefore, running programs installed into the conda env (e.g. python) willrun the version from the env since it appears in the $PATH before any otherinstalled version.     To check what conda environments you have and which is active, run      conda env list        or, you can run which python and you should see the python binary is in asubfolder of ~/miniconda3/envs/cs236781-hwN/.     You can find more useful info about conda environmentshere.   General Notes       You should to do steps 1 (installing conda) once, not for each assignment.         However, the third-party package dependencies (in the environment.yml file)might slightly change from one assignment to the next.To make sure you have the correct versions, always install the environmentagain (step 2 above) from the assignment root directory every time a newassignment is published and then activate the environment with the assignmentnumber.         Always make sure the correct environment is active! It will revert to itsdefault each new terminal session. If you want to change the default env youcan add a conda activate in your ~/.bashrc.         If you use PyCharm or any other IDE, you should configure the interpreter pathof the IDE to the path of the python executable within the condaenv folder. For example, point the interpreter path to~/miniconda3/envs/cs236781-hwN/bin/python.This is under Settings -&gt; Project -&gt; Project Interpreter.         You’ll need to install the conda env within your user folder on the courseserver. The installation procedure is exactly the same, just follow theinstructions for linux.   Notes for Windows Users       On Windows, you can run these commands from the Anaconda Prompt programthat is installed with miniconda. If you also add the conda installationto the Windows PATH variable, you can run these commands from the regularwindows command prompt.         Also on Windows, you need to install Microsoft’s Build Tools for VisualStudiobefore the conda environment.  Make sure “C++ Build Tools” is selected duringinstallation. This only needs to be done once.   Working on the assignment Running Jupyter Make sure that the active conda environment is cs236781-hwN (see above), andrun jupyter labThis will start a jupyter labserver and open your browser at the local server’s url. You can now start working.Open the first notebook (Part0) and follow the instructions. If you’re new to jupyter notebooks, you can get started by reading theUI guideand also about how to use notebooks inJupyterLab. Note that if you are familiar with and prefer the regular jupyter notebook youcan use that instead of jupyter lab. Implementing your solution and answering questions   The assignment is comprised of a set of notebooks and accompanying codepackages.  You only need to edit files in the code package corresponding to theassignment number, e.g. hw1, hw2, etc.  The notebooks contain material you need to know, instructions about what to doand also code blocks that will test and visualize your implementations.  Within the notebooks, anything you need to do is marked with a TODO besideit. It will explain what to implement and in which file.  Within the assignment code package, all locations where you need to write codeare marked with a special marker (YOUR CODE). Additionally, implementationguidelines, technical details and hints are in some cases provided in acomment above.  Sometimes there are open questions to answer. Your answers should also bewritten within the assignment package, not within the notebook itself. Thenotebook will specify where to write each answer.Notes:       You should think of the code blocks in the notebooks as tests. They test yoursolutions and they will fail if something is wrong.  As such, if youimplement everything and the notebook runs without error, you can beconfident about your solution.         You may edit any part of the code, not just the sections marked withYOUR CODE. However, note that there is always a solution which requiresediting only within these markers.         When we check your submission, we’ll run the original notebook files ofthe assignment, together with your submitted code (from the hwN) package.Therefore, any changes you do to the notebook files (such as changing thetests) will not affect the results of our grading. If you rely on notebookmodifications to pass the tests, the tests will fail when we grade your workand you will lose marks.         Please don’t put other files in the assignment directory. If you do, theywill be added to your submission which is automatically generated from thecontents of the assignment folder.         Always make sure the active conda env is cs236781-hwN (where N is theassignment number). If you get strange errors or failing import statements,this is probably the reason. Note that if you close your terminal sessionyou will need to re-activate since conda will use it’s default baseenvironment.   Submitting the assignment What you’ll submit:   All notebooks, after running them clean from start to end, with all outputspresent.  An html file containing the merged content of all notebooks.  The code package (hwN), with all your solutions present.You don’t need to do this manually; we provide you with a helper CLI program torun all the notebooks and combine them into a single file for submission. Generating your submission file To generate your submission, run (obviously with different id’s): python main.py prepare-submission --id 123456789 --id 987654321The above command will:   Execute all the notebooks cleanly, from start to end, regenerating all outputs.  Merge the notebook contents into a single html file.  Create a zip file with all of the above and also with your code.If there are errors when running your notebooks, it means there’s a problem withyour solution or that you forgot to implement something. Additionally, you can use the --skip-run flag to skip running your notebooks(and just merge them) in case you already ran everything and you’re sure thatall outputs are present: python main.py prepare-submission --skip-run --id 123456789 --id 987654321Note however that if some of the outputs are missing from your submission you’lllose marks. Note: The submission script must also be run from within the same conda env asthe assignment. Don’t forget to activate the env before running the submissionscript! Submitting a partial solution If you are unable to solve the entire assignment and wish to submit a partialsolution you can create a submission with errors by adding an allow-errorsflag, like so: python main.py prepare-submission --allow-errors --id 123456789 --id 987654321Uploading the solution The .zip file you generate should be uploaded using the assignments tab in thewebcourse system. Grades will also be reported there. Only a submission generated by the course script is considered valid. Anyother submissions, e.g. submitting only the notebooks or the code files willnot be graded. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/assignments/getting-started",
        "teaser":null},{
        "title": "Homework 1",
        "excerpt":"Submission date: November 18th, 2021 Topics   Working with data in PyTorch  PyTorch tensor API and broadcasting  Dataset splits  Cross validation  Optimizing loss functions with SGD  Linear SVM  Linear Regression with nonlinear featuresDownloading The assignment is availablehere(use the download button at the top). FAQ Make sure to read the getting started pagebefore getting started. Q: Will this assignment require use of course servers?A: No. This assignment does not involve very heavy or long-runningcomputations. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/assignments/hw1",
        "teaser":null},{
        "title": "Homework 2",
        "excerpt":"Submission date: December 23rd, 2021 Topics   Multilayer perceptrons  Decision boundaries  Threshold selection  Convolutional neural networks  Residual blocks  Network architectureDownloading The assignment is availablehere(use the download button at the top). Update 1: Downloadhere. FAQ Make sure to read the getting started pageand the guide for using course servers (relevant for part 3). Please ask all questions regarding the assignment on the course Piazza. Updates Update 1   Changed the method for comparing actual vs expected outputs for CNNs andResNets in part 2. This should provide more rubustness to numericaldifferences across machines.  Updated the jupyter-lab.sh script.No code changes should be required on your part.To use this update, make sure to replace everything except for your code files(hw2/hw2/*.py). You can unzip it to a new folder and copy only the code files withyour solutions from the previous version to the new version’s folder. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/assignments/hw2",
        "teaser":null},{
        "title": "Homework 3",
        "excerpt":"Submission date: January 16th, 2022 Topics   Backpropagation  Optimization algorithms for deep learning  Training  Sequence models for text generationDownloading The assignment is availablehere(use the download button at the top). FAQ Make sure to read the getting started pageand the guide for using course servers. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/assignments/hw3",
        "teaser":null},{
        "title": "Homework 4",
        "excerpt":"Submission date: March 13th, 2022 Topics   Deep reinforcement learning based on policy gradients.  Image generation with a Variational Autoencoder  Generative adversarial networks  Course summary questions on various topics.Downloading The assignment is availablehere(use the download button at the top). Technical Notes This assignment requires a different conda environment, which should beinstalled prior to starting. Before installing the new environment, it isrecommended to run conda update condaconda config --set channel_priority strictAnd then install and activate the new environment as usual, conda env update -f environment.ymlconda activate cs236781-hw4FAQ Make sure to read thegetting started page,theguide for using course serversand ourcollaboration policybefore starting the assignment. Q: What is the _final checkpoint file?A: You must use this to create your final submission with result video fromyour best-trained model. When you get results that your happy with, rename thecheckpoint file by appending _final. You don’t need to submit thecheckpoints/ folder (the main.py script will ignore them). Q: Should the results/ directory be part of the submission?A: Yes. The submission script will include it for you. This is OK. Do notput any unnecessary files in this directory apart from the results filesgenerated by the notebooks. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/assignments/hw4",
        "teaser":null},{
        "title": "Lecture 2: Supervised Learning",
        "excerpt":"The goal of this lecture is to formalize the supervised regime of learningproblems, which is by far the most common used type of learning.Let us start with a concrete learning problem that we will use as anillustration throughout this lecture. A financial institution issues creditcards to its customers. Each customer has a profile with a bunch of numericvalues indicating, e.g., their salary, total outstanding debt, credit score,years in the last residence, etc.  Based on this information, the institutionhas to decide whether to approve the credit and decide on the size of the creditline. The former problem of assigning each customer a discrete decision (yes/noin this case) is known as classification. The latter problem of assigning acontinuous value (dollar amount) is called regression. In what follows, we formalize the supervised learning problem. Ingredients of Supervised Learning Instance space Let us represent the user data as an $n$-dimensional column vector$\\bb{x} = (x_1,\\dots,x_n)^\\Tr$ and denote the space in which thesedata reside as $\\mathcal{X}$. $\\mathcal{X}$ is ofter called the input orinstance space, and a point in it is called an instance.  We will furtherdefine a probability measure $P(\\bb{X})$ on $\\mathcal{X}$ and thinkof an instance as a realization of the random vector $\\bb{X}$distributed with $P$. Label space Let us denote by $\\mathcal{Y}$ the target or label space in which thepossible decisions about a customer reside. For example, in the case of ourbinary classification, $\\mathcal{Y}$ is simply ${0,1}$; in the case ofregression, this may be $\\mathcal{Y}=[0,\\infty)$. In general, this space maycontain continuous and vector-valued objects. We will still refer to a point in$\\mathcal{Y}$ as to a label even in the case of regression problems. Target function We assume that there exists some target function $f : \\mathcal{X} \\rightarrow\\mathcal{Y}$ assigning to each instance $\\bb{x}$ a label $y$. Thisfunction is, obviously, unknown; yet, we can think of it as a black box intowhich we can throw an instance $\\bb{x}$ and get the correspondinglabel $y = f(\\bb{x})$. In practice, it often happens that the target“function” is not a function at all. For example, it might not be deterministic,so that two customers with exactly the same profiles receive differentdecisions. A way to correctly model this situation is by defining a targetdistribution $P(Y | \\bb{X})$, a conditional probability measuretelling us how the label (a random variable) is distributed given an instance$\\bb{X} = \\bb{x}$. We can therefore think of another blackbox generating pairs $(\\bb{x},y)$ from the joint distribution$P(\\bb{X},Y) = P(\\bb{X}) P(Y|\\bb{X})$.Alternatively, we can think of a noisy target function of the form$f(\\bb{x}) = \\mathbb{E} (Y|\\bb{X}=\\bb{x})$ plusnoise $Y - f(\\bb{x})$. The first term in this definition isdeterministic accounting for the part of $y$ that can be explained in the termsof $\\bb{X}$, while the second term is stochastic accounting forwhatever cannot be explaind by $\\bb{X}$ (e.g., the bad mood of afinancial officer making the final decision about credit approvals). Training set While the target function $f$ (alternatively, the conditional distribution$P(Y|\\bb{X})$) is latent, we assume to be given a finite sample \\[\\{ (\\bb{x}_i,y_i) \\}_{i=1}^N\\]of labeled instances with each$\\bb{x}_i$ drawn from $P(\\bb{X})$ and $y_i =f(\\bb{x}_i)$ (alternatively, $(\\bb{x}_i,y_i)$ are drawnfrom the joint distribution $P(\\bb{X},Y) = P(\\bb{X})P(Y|\\bb{X})$). The goal of supervised learning is to estimate the target function (or thetarget distribution) from the training examples. Hypothesis class Since the target function can be arbitrarily complex, in order to make thelearning problem manageable, we will restrict our estimates to some (usually,parametric) family of functions which we will refer to as a hypothesis class$\\mathcal{H}$. A function $h \\in \\mathcal{H}$ is a map $h : \\mathcal{X}\\rightarrow \\mathcal{Y}$ assigning each instance in $\\mathcal{X}$ a label in$\\mathcal{Y}$. For example, the class of linear hypotheses that we will encounter in thesequel is defined as \\[\\mathcal{H} = \\{ h : \\mathcal{X} \\rightarrow \\mathcal{Y} \\, | \\,h(\\bb{x} = \\bb{w}^\\Tr \\bb{x} + w_0 :\\bb{w} \\in \\mathbb{R}^{n+1}  \\}.\\]A concrete choice of the weights $\\bb{w}$ gives a specific hypothesisfrom the class $\\mathcal{H}$. Loss functions The task of supervised learning consists therefore of picking a singlehypothesis function from $\\mathcal{H}$ that best estimates the target function$f$. But “best” in what sense? A very standard way of quantifying the quality ofthe fit is by defining a functional of the form $L(h,f)$ accepting a hypothesis$h$ and the target function $f$ and returning a numerical value indicating thedeviation of $h$ from $f$.  $L$ is usually referred to as the loss, risk orcost function and should be as small as possible. In these terms, the task ofmachine learning can be formulated as the minimization of the loss over thehypothesis class, \\[h^\\ast = \\mathrm{arg} \\min_{h \\in \\mathcal{H}} L(h,f).\\]Loss functions are (almost) always constructed from a pointwise definition,i.e., we actually define $\\ell(\\hat{y},y)$ accepting the estimated label$\\hat{y}=h(\\bb{x})$ of an instance $\\bb{x}$ and its truelabel given by the target function $y=f(\\bb{x})$. For example, inregression problems the squared error \\[\\ell( \\hat{y},y ) = ( \\hat{y} - y )^2\\]is often used as the loss, while in binary classification, the choice could bethe binary error \\[\\ell( \\hat{y},y ) = [ \\hat{y} \\ne  y],\\]with $[a]$ denoting the indicator function of the condition $a$ accepting thevalue of $1$ when $a$ is satisfied and $0$ otherwise. As a general rule and a matter of good practice, the choice of the loss functionis application-specific and should be provided by the user. In the real world,however, this rarely happens. Two alternatives are possible as the secondchoice. The first one is to use a plausible loss function that has a merit andcan be rigorously defined asserting a certain assumption (the validity of whichcan be typically debated ad infinitum). For example, the assumption of whiteadditive Gaussian noise leads to the squared error. While the latter fact can beproved mathematically, the former assumption is rarely perfectly (or at all)correct. Similarly, the cross-entropy error can plausibly replace the binaryerror. The second alternative is to pragmatically choose a loss function not based onsome particular merit or justification, but rather because it is easy to use.For example, the squared error in combination with the linear hypothesis classleads to a simple closed form expression; in the same setting, the cross-entropyloss, while not leading to a closed-form expression yields a convex objectivethat can be minimized globally. Just to reiterate: the choice of the loss function is crucial to the success oflearning and should be done with as much domain-specific understanding of theproblem as possible. In-sample loss Once the point-wise loss function has been fixed, we can define the overall loss$L$ by averaging the pointwise losses. Averaging on the training set leads tothe empirical or in-sample loss \\[L_{\\mathrm{in}}(h) =  \\frac{1}{N} \\sum_{i=1}^N \\ell( h(\\bb{x}_i),f(\\bb{x}_i) ) =  \\frac{1}{N} \\sum_{i=1}^N \\ell(h(\\bb{x}_i), y_i ),\\]where we omitted the dependence on $f$ for convenience. Note that the loss onlydepends on the instances in the training set and their corresponding labels.This is the only type of loss that we can actually compute without knowing thetarget function! So let us state our learning problem more precisely: inconsists of minimizing the in-sample loss, \\[h^\\ast = \\mathrm{arg} \\min_{h \\in \\mathcal{H}} L_{\\mathrm{in}}(h) =\\mathrm{arg} \\min_{h \\in \\mathcal{H}} \\frac{1}{N} \\sum_{i=1}^N \\ell(h(\\bb{x}_i), y_i).\\]Simple examples In what follows, we will examine several very simple examples of supervisedlearning based on the linear hypothesis class. To simplify notation, we willassume that every instance has an additional dimension $x_0 = 1$, such that theaffine term in $\\bb{w}^\\Tr \\bb{x} + w_0$ can be absorbedinto the inner product (in some literature this trick goes by the name ofhomogeneous coordinates). In this notation, the linear class is defined as \\[\\mathcal{H} = \\{ h : \\mathcal{X} \\rightarrow \\mathcal{Y} \\, | \\, h(\\bb{x} = \\bb{w}^\\Tr \\bb{x} : \\bb{w} \\in \\mathbb{R}^{n+1}  \\}.\\]In binary classification, this class of models in known as linear perceptron. Linear regression Let us examine first the regression problem using the linear regressor$h(\\bb{x}) = \\bb{w}^\\Tr \\bb{x}$.  Assuming thesquared error loss, we obtain \\[L_{\\mathrm{in}} =  \\frac{1}{N} \\sum_{i=1}^N (h(\\bb{x}_i) - y_i )^2 = \\frac{1}{N} \\sum_{i=1}^N (\\bb{w}^\\Tr \\bb{x}_i - y_i )^2.\\]Arranging the training instances into the columns of an $(n+1) \\times N$ matrix$\\bb{X}$ and the training labels into an $N$-dimensional vector$\\bb{y}$ yields \\[L_{\\mathrm{in}} =  \\frac{1}{N} \\|  \\bb{X}^\\Tr \\bb{w} -\\bb{y} \\|^2.\\]Differentiating w.r.t. $\\bb{w}$ and requiring vanishing gradientyields \\[0 = \\bb{X}(\\bb{X}^\\Tr \\bb{w} - \\bb{y}) =\\bb{X} \\bb{X}^\\Tr \\bb{w} - \\bb{X}\\bb{y},\\]from where \\[\\bb{w}^\\ast =(\\bb{X}^\\Tr)^\\dagger \\bb{y} =(\\bb{X}\\bb{X}^\\Tr )^{-1} \\bb{X}\\bb{y}.\\]Linear binary classification Linear classification is very similar to linear regression, with the exceptionthat a classifier only retains the sign of the linear function$h(\\bb{x}) = \\mathrm{sign}(\\bb{w}^\\Tr \\bb{x})$.Geometrically, this corresponds to splitting the space into two regions by ahyperplane whose normal is defined by $(w_1,\\dots,w_n)$. We could naively ignorethe sign function and learn the classifier using the squared error applied to$y_i \\in  {\\pm 1}$. However, because of the linear form of the function underthe sign, this loss will artificially penalize correcy hypotheses at pointdistant from the decision boundary. Logistic regression A better alternative is to model directly the conditional probability$P(Y|\\bb{X})$ as \\[P(Y=1|\\bb{X}=\\bb{x}) = \\frac{1}{1+e^{ \\bb{w}^\\Tr \\bb{x}}}\\]and \\[P(Y=0|\\bb{X}=\\bb{x}) = 1- P(Y=1|\\bb{X}=\\bb{x}) = \\frac{e^{ \\bb{w}^\\Tr \\bb{x}} }{1+e^{ \\bb{w}^\\Tr \\bb{x}}}\\]Label $y=0$ is assigned if \\[\\frac{ P(Y=0|\\bb{X}=\\bb{x}) }{ P(Y=1|\\bb{X}=\\bb{x}) } = e^{ \\bb{w}^\\Tr \\bb{x}} &gt; 1.\\]The label $y=1$ is assigned otherwise. In order to solve the regression problem, we maximize the likelihood of theobserved labels in the training data given the corresponding instances, \\[\\bb{w}^\\ast = \\mathrm{arg}\\max_{ \\bb{w}} \\prod_{i=1}^n P(y_i | \\bb{x}_i) = \\mathrm{arg}\\min_{ \\bb{w}} \\sum_{i=1}^n -\\log P(y_i | \\bb{x}_i)\\]The negative likelihood can be written as the loss function \\[L(\\bb{w}) = \\sum_{i=1}^n -\\log P(y_i | \\bb{x}_i)  = \\sum_{i=1}^n -y_i \\log P(y_i=1 | \\bb{x}_i) - (1-y_i) \\log P(y_i=0 | \\bb{x}_i),\\]utilizing the fact that $y_i$ can only assume binary values.  Substituting thelinear hypothesis yields \\[L(\\bb{w}) =   \\sum_{i=1}^n(y_i-1)\\bb{w}^\\Tr\\bb{x}_i + \\log ( 1+e^{ \\bb{w}^\\Tr \\bb{x}_i} ).\\]Note that if training labels are expressed as delta-distributions, the logisticregression loss is nothing but the cross-entropy between the true labels and thelogistic model for $P(Y|\\bb{X})$. While the minimizer of this loss does not admit a closed-form expression, it canbe found using an iterative solver. The convexity of the loss warrants globalconvergence. Non-linear data transformations vs. non-linear models The utility of linear models is limited since they can only express lineardecision boundaries. However, by transforming the instance data by somenon-linear map $\\Phi$ and applying a linear model (regressor or classifier) onthe obtained feature space can create arbitrarily complex decision boundaries.Note that while a regressor of the form $h(\\bb{x}) =\\bb{w}^\\Tr \\Phi(\\bb{x})$ is non-linear in$\\bb{x}$, it is still linear in $\\bb{w}$ and, therefore,can be solved for as before. However, while non-linear data transformations allow designing complexdecision boundaries, the choice of the map $\\Phi$ is hand-crafted ratherthan learned. Alternatively, a class of parametric hypothesis functionsthat depend non-linearly on the parameters allow to learn the featuresthemselves direclty from the training examples. In this course, suchnon-linear models will be deep neural networks. Why does learning work? Why does learning work at all? What prevents us from choosing a sufficientlyrich hypothesis class that will simply memorize each training sample in a hugetable and whenever $h$ is queried with the instance $\\bb{x}_i$, itwill return the memorized $y_i$.  Obviously, such a hypothesis will result in$L_{\\mathrm{in}} = 0$, but will be completely useless on previously unseeninstances. Generalization error In order to perform well on unseen data, the hypothesis should to generalizeover the underlying probability distribution. This can be formalized by definingthe generalization error (aka out-of-sample loss) as \\[L_{\\mathrm{out}}(h)  = \\mathbb{E} \\, \\ell( h(\\bb{X}), f(\\bb{X}) ),\\]where the expectation is taken over $P(\\bb{X})$, or,alternatively, in terms of the joint distribution, \\[L_{\\mathrm{out}}(h)  = \\mathbb{E} \\, \\ell( h(\\bb{X}), Y ),\\]where the expectation is over $P(\\bb{X},Y)$. In order to generalize well, our learning process should pick up$h \\in \\mathcal{H}$ with the smallest out-of-sample loss. However, wecannot really solve this problem, since we cannot compute$L_{\\mathrm{out}}$ as $P(\\bb{X},Y)$ is unknown. Hoeffding inequality Is the learning problem solvable at all? We are stuck with theminimization of the empirical (in-sample) loss, while we want tominimize the out-of-sample loss. Can we say anything useful about thegeneralization gap, i.e., the difference between the generalizationerror and the training error? Note that the difference between$L_{\\mathrm{in}}$ and $L_{\\mathrm{out}}$ is that while in the former weuse a finite-sample average, the latter is defined with a trueexpectation. Given a hypothesis, the in-sample loss is a random variable(each realization of the sample will give it a different value), whilethe out-of-sample loss is a deterministic quantity. If we assume that the training samples are drawn i.i.d. from theunderlying distribution, by the law of large number the finite-sampleaverage will asymptotically concentrate about the expected value. One ofthe strong forms of the law of large numbers is known as the Hoeffdinginequality setting a bound on the probability of the finite-sampleaverage deviating from the expected value by more than $\\epsilon$. Inour terms, for a given hypothesis, it can be expressed \\[\\mathbb{P}( | L_{\\mathrm{out}}(h) - L_{\\mathrm{in}}(h) | &gt; \\epsilon) \\le 2 e^{-2\\epsilon^2 N }.\\]This concentration inequality states that$L_{\\mathrm{out}}(h) = L_{\\mathrm{in}}(h)$ is probably approximatelycorrect (PAC). Probably in the sense that it can be violated with anegligibly small probability (that decays exponentially in the samplesize $N$); approximately in the sense that $L_{\\mathrm{out}}(h)$ isallowed to deviate from $L_{\\mathrm{in}}(h)$ by a specified tolerance$\\epsilon$. The smaller is this constant, the more training samples arerequired to maintain the same level of certainty about the result. Forexample, decreasing $\\epsilon$ by $10$ times requires to increase $N$ by$100$ times. A naïve generalization bound The bound we derived so far applies to a single hypothesis. So it isuseful to verfiy whether a given hypothesis will generalize well.However, recall that the learning process actually involves a searchover many hypotheses. In order to accomodate for this, let us derive thefollowing worst-case bound \\[\\begin{align*}\\mathbb{P}( | L_{\\mathrm{out}}(h^\\ast) - L_{\\mathrm{in}}(h^\\ast) | &gt; \\epsilon) &amp; \\le\\mathbb{P}( \\sup_{h \\in \\mathcal{H} } | L_{\\mathrm{out}}(h) - L_{\\mathrm{in}}(h) | &gt; \\epsilon) \\\\ &amp; =\\mathbb{P}\\left( \\bigcup_{h \\in \\mathcal{H} } \\{ | L_{\\mathrm{out}}(h) - L_{\\mathrm{in}}(h) | &gt; \\epsilon \\} \\right).\\end{align*}\\]Using the union bound inequality yields \\[\\mathbb{P}( | L_{\\mathrm{out}}(h^\\ast) - L_{\\mathrm{in}}(h^\\ast) | &gt; \\epsilon) \\le \\sum_{ h \\in \\mathcal{H}  } \\mathbb{P}\\left(  | L_{\\mathrm{out}}(h) - L_{\\mathrm{in}}(h) | &gt; \\epsilon \\right) \\le  2 | \\mathcal{H} | e^{-2\\epsilon^2 N }.\\]However, using the cardinality of $\\mathcal{H}$ is a very lousy idea.First of all, it clearly fails even for such a simple infinite class ofhypotheses as the linear class we discussed earlier. This happensbecause given two hypotheses $h_1,h_2 \\in \\mathcal{H}$, we treat theevents $| L_{\\mathrm{out}}(h_1) - L_{\\mathrm{in}}(h_1) | &gt; \\epsilon$ and$| L_{\\mathrm{out}}(h_2) - L_{\\mathrm{in}}(h_2) | &gt; \\epsilon$ asindependent, while, clearly, the dependence of these events shouldsomehow depend on how $h_1$ and $h_2$ are close to each other in somesense. Hypothesis class complexity More delicate tools exist to get a better grasp of the complexity (orcapacity of the hypothesis class, leading to tighter and more usefulgeneralization error bounds. The first such analytic tool derived in the70’s was the Vapnik–Chervonenkis (VC) dimension. A more modern andpowerful tool is the Rademacher complexity that measures how well ahypothesis class fits random noise. We will not get into the details of these tools, for the mere reasonthat they are not very helpful when applied to deep learning.Practically useful deep neural networks architectures have a huge VCcapacity, yet practice shows that they can generalize really well whentrained on comparatively tiny training sets (compared to the size thatwould satisfy the bounds). Currently, there is no theoretical answerexplaining the successful generalization of DNNs, and there is strongevidence ruling out the classical explanation based on the VC dimension(and similar reasoning). For example, the effects of explicitregularization (weight decay, dropout, data augmentation) as well asimplicit regularization produced by the use of stochastic gradientdescent are presently poorly understood. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_02/",
        "teaser":null},{
        "title": "Lecture 3: Multi-layer Perceptron",
        "excerpt":"The Perceptron In the last lecture, we discussed supervised learning with a linear hypothesisclass of the form \\[y = \\bb{w}^\\Tr \\bb{x}+b\\]parametrized by $n$ weights $\\bb{w} = (w_0,w_1,\\dots,w_n)$ and abias $b$. In the machine learning literature, this family of functions (or“architecture” as we shall call it in the sequel) is known as a (linear)perceptron. We have seen that in the case of binary logistic regression (which, despite thename, is a binary classification problem) the scalar output $y$ of thehypothesis was further fed into the logistic (a.k.a.  sigmoid function \\[\\psi(t) = \\frac{e^t}{1+e^t}.\\]This can be viewed as a two-dimensional output of the form \\[\\bb{y} = \\left(  \\frac{e^{ \\bb{w}^\\Tr \\bb{x}+b} }{1+e^{ \\bb{w}^\\Tr \\bb{x} + b}}, \\frac{1}{1+e^{ \\bb{w}^\\Tr \\bb{x}+b}} \\right)\\]which can be interpreted as the vector of probabilities of the instance$\\bb{x}$ belonging to each of the two classes. Using this perspective, the linear perceptron model can be generalizedto the $k$ class cases according to \\[\\bb{y} = \\frac{ e^{ \\bb{W}^\\Tr\\bb{x}+\\bb{b} }  }{ \\bb{1}^\\Tr e^{\\bb{W}^\\Tr \\bb{x} + \\bb{b}}  } =\\displaystyle{\\left(  \\frac{ e^{ \\bb{w}_1^\\Tr \\bb{x} + b_1} }{\\sum_{i=1}^k e^{ \\bb{w}_i^\\Tr \\bb{x} + b_i  } } ,\\dots,  \\frac{ e^{ \\bb{w}_n^\\Tr \\bb{x}  + b_n }}{\\sum_{i=1}^k e^{ \\bb{w}_i^\\Tr \\bb{x} + b_i  } }  \\right)},\\]where $\\bb{W}$ is a $k \\times n$ weight matrix whose rows aredenoted as $\\bb{w}_i$, $\\bb{b}$ is a$k$-dimensional bias vector, and $\\bb{1}$ is anappropriately-sized vector of ones. This generalization of the logisticfunction used to normalize the output intot the form of a vector ofprobabilities is known as softmax. Softmax is a function of the form \\[\\psi(\\bb{z}) =  \\frac{ e^{ \\bb{z}}  }{\\bb{1}^\\Tr e^{ \\bb{z}}  }\\]that highlights the maximal value in the vector $\\bb{z}$ andsuppresses other elements that are significantly lower than the maximum. Adding layers The linear perceptron model is rather limited due to its linearity. For example,it cannot produce the XOR function. A much more powerful family of functions isobtained by applying a non-linearity to the output of a linear perceptron andconcatenating several such models. We define the $i$-th layer as \\[\\bb{y}_i = \\varphi_{i} ( \\bb{W}_i \\bb{y}_{i-1} + \\bb{b}_i ),\\]for $i=1,\\dots,L$, where $\\bb{y}_{i-1}$ is an$n_{i-1}$-dimensional input, $\\bb{y}_i$ is an$n_{i}$-dimensional output, $\\bb{W}_i$ is an$n_i \\times n_{i-1}$ matrix of weights (whose columns are denoted as$\\bb{w}^{i}_1,\\dots, \\bb{w}^{i}_{n_{i-1}}$),$\\bb{b}_i$ is an $n_i$-dimensional bias vector, and$\\varphi_i : \\RR \\rightarrow \\RR$ is is a non-linear function appliedelement-wise. Setting $\\bb{y} = \\bb{y}_L$ and$\\bb{y}_0 = \\bb{x}$, a multi-layer perceptron(MLP) with $L$ layers is obtained. MLP can be described by the followinginput-to-output map \\[\\bb{y}=\\varphi_L \\left(   \\bb{W}_{L} \\varphi_{L-1}( \\bb{W}_{L-1}  \\varphi_{L-2}(  \\cdots  \\varphi_1(\\bb{W}_1 \\bb{x})   \\cdots  )    ) \\right).\\]parametrized by the weight matrices${ \\bb{W}_1,\\dots,\\bb{W}_L }$ and bias vectors${ \\bb{b}_1,\\dots,\\bb{b}_L }$ which we willcollectively denote as a pseudo-vector $\\bb{\\Theta}$. Graphically, the $i$-th layer can be thought of a weighted directed graphconnecting each of the $n_{i-1}$ inputs to $n_i$ sum nodes with theweights given by elements of $\\bb{W}_i$. The output of eachsum node undergoes a non-linearity and together the $n_i$ outputs formthe input of the following layer. Because of its (deliberate)resemblance to biological neural networks, MLP is called an (artificial)neural network. In the jargon of artificial neural networks, eachsub-graph of the form$y^i_j = \\varphi_i (  \\bb{y}_{i-1}^\\Tr \\bb{w}^{i}_j + b_j  )$is called a neuron (the $j$-th neuron in $i$-th layer), itsnon-linearity $\\varphi_i$ is called an activation function, and itsoutput $y^i_j$ an activation. MLP is a feedforward neural network,since the graph is acyclic – the data flow forward from the input to theoutput without feedback loops. Unlike their single-layered linear counterparts, MLPs constitute apotent hypothesis class. In fact, even with just two layers, MLPs wereshown to be universal approximators – their weights can be selected toapproximate any function under mild technical conditions, provided theyhave enough degrees of freedom (sufficiently large number of weights). Non-linearity Various functions can be used as the element-wise nonlinearities(activation function) of the MLP. Older neural networks used thelogistic function (a.k.a. sigmoid) \\[\\varphi(t) = \\frac{1}{1+e^{-t}}\\]saturating the input in $\\RR$ between $0$ and $1$, or its shifted andscaled version \\[\\varphi(t) =  \\frac{e^t  - e^{-t}}{e^t +e^{-t}} = \\mathrm{tanh}\\, t.\\]The arctangent function also has a sigmoid-like behavior. However, due to numerical issues that will be discussed in the sequel, thesefunctions were nowadays almost universally replaced by the rectifier function(a.k.a. rectified linear unit or ReLU) \\[\\varphi(t) = [t]_+ = \\max\\{t,0\\}.\\]Note that this function has the derivative of exactly $0$ on $(-\\infty,0)$,exactly $1$ on $(0,\\infty)$, and is non-smooth at $0$. These facts justifyingits choice will be discussed in the sequel. In addition to element-wise non-linearities, modern neural networks sometimesuse “horizontal” non-linearities acting on the entire activation vector. Onetypical choice of such a non-linearity adopted in classification networks is asoftmax function applied to the activation of the last (output) layer. Othernon-linearities of this kind are pooling operations that will be discussed inthe sequel. Supervised training Now equipped with a new richer hypothesis class, let us zoom out to see thewhole picture. In the supervised learning problem, we are given a finite sampleof labeled training instances ${  (\\bb{x}_i, y_i) }_{i=1}^N$. Wethen select a hypothesis that minimizes the empirical (in-sample) loss function, \\[h^\\ast =  \\mathrm{arg} \\min_{h \\in \\mathcal{H}} \\frac{1}{N} \\sum_{i=1}^N \\ell( h(\\bb{x}_i), y_i).\\]In our terms, this minimization problem can be written as \\[\\bb{\\Theta}^\\ast =  \\mathrm{arg} \\min_{ \\bb{\\Theta} }\\frac{1}{N} \\sum_{i=1}^N \\ell_i ( h_{ \\bb{\\Theta}}(\\bb{x}_i) ),\\]where $h_{ \\bb{\\Theta}}$ is the MLP parametrized by the pseudo-vector$\\bb{\\Theta}$. Note that to simplify notation we dropped thedependence of the $i$-th pointwise loss term on $y_i$, denoting it by $\\ell_i$.We will henceforth denote the loss function as \\[L(\\bb{\\Theta}) = \\frac{1}{N} \\sum_{i=1}^N \\ell_i ( h_{ \\bb{\\Theta}} (\\bb{x}_i) )\\]emphasizing that we are interested in its dependence on the model parameters$\\bb{\\Theta}$. Let us now discuss how to minimize it with respect to$\\bb{\\Theta}$. Global and local minima Let us assume that $L$ is a function of an $m$-dimensional argument$\\bb{\\theta}$ defined on all $\\RR^m$ (we can always parse all thedegrees of freedom of our neural network into an $m$-dimensional vector). Apoint $\\bb{\\theta}^\\ast$ is called a global minimizer of $L$ if forany $\\bb{\\theta}$, $L(\\bb{\\theta}) \\ge L(\\bb{\\theta}^\\ast)$.The corresponding value of the function,$L(\\bb{\\theta}^\\ast)$, is called a global minimum. The latter termis often (strictly speaking, erroneously) used to denote the minimizer as well.A point $\\bb{\\theta}^\\ast$ is called a local minimizer of $L$ ifthere exists $\\epsilon &gt; 0$ such that $\\bb{\\theta}^\\ast$ is a globalminimizer of $L$ on the ball $B_\\epsilon(\\bb{\\theta}^\\ast)$. Unless $L$ satisfied special properties (such as convexity), finding its globalminimizer is an unsolvable problem. On the other hand, finding a local minimizeris a much easier task, since local minimizers can be characterized using localinformation (i.e., derivatives). Assuming $L$ is $\\mathcal{C}^1$, fromelementary multivariate calculus we should recollect the first-order necessarycondition for $\\bb{\\theta}^\\ast$ being a local minimizer: \\[\\nabla_{ \\bb{\\theta}} L(\\bb{\\theta}^\\ast) = \\bb{0}.\\]Obviously, this is not a sufficient condition – in fact, a local maximum and asaddle point also satisfy it. However, the latter two types of extremal points(characterized by negative curvature) are unstable, which will allow methodssuch as stochastic gradient descent not to remain stuck at such points. As a reminder, the gradient of a multi-variate function is an operator$\\nabla L : \\RR^m \\rightarrow \\RR^m$. At a given point$\\bb{\\theta}$, it produces a vector$\\bb{g} = \\nabla L(\\bb{\\theta})$ satisfying \\[dL = \\langle \\bb{g}, \\bb{dx} \\rangle = \\bb{g}^\\Tr \\bb{d\\theta};\\]in other words, an inner product of the argument change$\\bb{d\\theta}$ with the gradient yields the differential $dL$. Gradient descent We can therefore suggest a very simple iterative strategy for finding a localminimum, which can be summarized as the following “algorithm”: Starting with some initial guess $\\bb{\\theta}_0$, repeat for$k=1,2,\\dots$   Select a descent direction $\\bb{d}_k$  Select a step size $\\eta_k$  Update$\\bb{\\theta}_k = \\bb{\\theta}_{k-1} + \\eta_k \\bb{d}_k$  Check optimality condition at $\\bb{\\theta}_k$ and stop ifminimum is reached(In practice, rather than checking the optimality condition, we will run thealgorithm for a fixed number of iterations and stop it prematurely based on thevalue of cross-validation loss – these details will be discussed further in thecourse.) The main ingredient of the above “algorithm” is the choice of the descentdirection, i.e., a direction a (small) step in which decreases the value of thefunction. Let $\\bb{\\theta}$ be our current iterate (we drop theiteration subscript) and let $\\bb{d}$ be a direction. Once adirection is choses, we can consider a one-dimensional “section” of the$m$-dimensional function $L$, \\[f(\\eta) = L(\\bb{\\theta}+\\eta \\bb{d}).\\]The quantity \\[f'(0) = \\left. \\frac{d L(\\bb{\\theta}+\\eta \\bb{d})}{d\\eta} \\right|_{\\eta = 0} = \\nabla L(\\bb{\\theta})^\\Tr\\bb{d}\\]is known as the directional derivative of $L$ at point$\\bb{\\theta}$ in the direction $\\bb{d}$. A negativedirectional derivative indicates that a small step in the direction$\\bb{d}$ decreases the value of the function.  Geometrically, thismeans that a descent direction forms an obtuse angle with the gradient (or anacute angle with the negative gradient). Let us now approximate our function linearly around$\\bb{\\theta}$, \\[L(\\bb{\\theta}+\\bb{d}) \\approx L(\\bb{\\theta}) +\\nabla L(\\bb{\\theta})^\\Tr \\bb{d}\\]and ask ourselves what direction minimizes the difference$L(\\bb{\\theta}+\\bb{d}) -  L(\\bb{\\theta})\\approx \\nabla L(\\bb{\\theta})^\\Tr \\bb{d}$ – we could callsuch a direction the steepest descent direction.  Obviously, this linearapproximation is unbounded, so we need to normalize the length of$\\bb{d}$. Different choices of the norm lead to different answers (sothere are many steepest directions); in the $\\ell_2$ sense we obtain \\[\\bb{d} = -\\nabla \\bb{d}.\\]This choice of the descent direction leads to a family of algorithms known asgradient descent. Our next goal is to select the step size $\\eta$. Ideally, once we have thedirection $\\bb{d}$, we would like to solve for \\[\\eta = \\mathrm{arg}\\min_{\\eta} L(\\bb{\\theta}+\\eta \\bb{d}).\\]While there exist various methods known as line search to solve such aone-dimensional minimization problem, usually they come at the expense ofunaffordable extra complexity. In deep learning, a much more common choice is touse a vanishing sequence of weights that start with some initial $\\eta_0$ whichis kept for a certain number of iterations and then gradually reduced as $1/k$.Using the statistical mechanics metaphor, such a reduction in the step sizeresembles a decrease in temperature and is therefore referred to as annealing. Gradient descent can be thus summarized as Starting with some initial guess $\\bb{\\theta}_0$, repeat for$k=1,2,\\dots$   Select a step size $\\eta_k$  Update$\\bb{\\theta}_k = \\bb{\\theta}_{k-1} - \\eta_k \\nabla L(\\bb{\\theta}_{k-1})$  Check optimality condition at $\\bb{\\theta}_k$ and stop ifminimum is reachedWe will discuss variants of the gradient descent algorithm that are usedin practice in the sequel. Error backpropagation The main computation ingredient in the gradient descent algorithm is thegradient of the loss function w.r.t. the network parameters$\\bb{\\theta}$. Obviously, since an MLP is just a composition ofmulti-variate functions, the gradient can be simply computed invoking the chainrule. However, recall that the output of the network is usually a$k$-dimensional vector, whereas the parameters are a collection of$n_i \\times n_{i-1}$ weight matrices and $n_i$-dimensional bias vectors. Thegradient of a vector with respect to a matrix (formally termed the Jacobian) isa third-order tensor, which is not exactly nice to work with. A much more elegant approach to apply the chain rule takes advantage of thelayered structure of the network. As an illustration, we start with a two-layerMLP of the form \\[\\bb{y} = \\varphi( \\bb{A}  \\phi(\\bb{B} \\bb{x} ) ),\\]where $\\varphi$ and $\\phi$ are the two non-linearities, and$\\bb{A}$ and $\\bb{B}$ are the two weight matrices.We are ignoring the bias terms for the sake of exposition clarity. Toanalyze the influence of the last (second) layer, we denote its input as$\\bb{y}’ =  \\phi(\\bb{B} \\bb{x} )$, andthe input to the second layer activation function as$\\bb{z} = \\bb{A}\\bb{y}’$. In thisnotation, we have$\\bb{y} = \\varphi(\\bb{A} \\bb{y}’)$.According to the chain rule, \\[\\frac{\\partial L}{\\partial \\bb{A}} =\\frac{\\partial \\bb{y} }{\\partial \\bb{A}} \\frac{\\partial L}{\\partial \\bb{y}} =\\sum_{j=1}^k \\frac{\\partial y_j }{\\partial \\bb{A}} \\frac{\\partial L}{\\partial y_j}.\\]For convenience, let us adopt the standard deep learning notation,according to which the derivative of the loss w.r.t. to a parameter$\\bb{*}$ is denoted as $\\delta \\bb{\\ast}$. In ourcase, \\[\\delta \\bb{y} = \\frac{\\partial L}{\\partial \\bb{y}} =\\left( \\frac{\\partial L}{\\partial y_1},\\dots,  \\frac{\\partial L}{\\partial y_k} \\right)^\\Tr\\]is the gradient of the loss w.r.t. its input, and$\\delta \\bb{A}$ is a matrix whose elements are$\\frac{\\partial L}{\\partial a_{ij} }$, etc. In this notation, we canrewrite \\[\\delta \\bb{A} =\\sum_{j=1}^k \\frac{\\partial y_j }{\\partial \\bb{A}} \\, \\delta  y_j.\\]We can write $\\frac{\\partial y_j }{\\partial \\bb{A}}$ as amatrix of the size of $\\bb{A}$, filled with zeros except the$j$-th row, which is given by$\\varphi’(z_j) \\bb{y}^{\\prime \\Tr}$. Substituting this resultinto the former sum yields \\[\\delta \\bb{A} =\\left(    \\begin{array}{c}        \\delta y_1 \\varphi'(z_1)  \\bb{y}^{\\prime \\Tr} \\\\ \\vdots \\\\ \\delta y_k \\varphi'(z_k)        \\bb{y}^{\\prime \\Tr}    \\end{array}\\right) =\\mathrm{diag}\\{ \\delta \\bb{y} \\}  \\, \\mathrm{diag}\\{ \\varphi'(\\bb{z}) \\} \\left(    \\begin{array}{c}        \\bb{y}^{\\prime \\Tr} \\\\        \\vdots \\\\        \\bb{y}^{\\prime \\Tr}    \\end{array}\\right) =\\mathrm{diag}\\{ \\delta \\bb{y} \\}  \\, \\mathrm{diag}\\{\\varphi'(\\bb{z}) \\} \\bb{1} \\bb{y}^{\\prime \\Tr}.\\]To analyze the influence of the first layer, we denote $\\bb{z}’ =\\bb{B}\\bb{x}$. To derive the gradient of the loss w.r.t.the first layer parameter $\\bb{B}$, we again invoke the chain rule \\[\\frac{\\partial L}{\\partial \\bb{B}} = \\frac{\\partial\\bb{y}' }{\\partial \\bb{B}} \\frac{\\partial L}{\\partial\\bb{y}'} = \\frac{\\partial \\bb{y}' }{\\partial\\bb{B}} \\frac{\\partial \\bb{y} }{\\partial\\bb{y}'}  \\frac{\\partial L}{\\partial \\bb{y}} = \\sum_{j=1}\\frac{\\partial y'_j }{\\partial \\bb{B}} \\, \\delta y'_j.\\]As before, $\\frac{\\partial y’_j }{\\partial \\bb{B}}$ is a matrix ofthe size of $\\bb{B}$, filled with zeros except the $j$-th row, whichis given by $\\phi’(z’_j) \\bb{x}^\\Tr$, so \\[\\delta \\bb{B} =\\mathrm{diag}\\{ \\delta \\bb{y}' \\}  \\, \\mathrm{diag}\\{\\phi'(\\bb{z}') \\}  \\bb{1} \\bb{x}^\\Tr.\\]It remains to derive \\[\\partial \\bb{y}' =\\frac{\\partial L}{\\partial \\bb{y}'} =\\frac{\\partial \\bb{y} }{\\partial \\bb{y}'}\\frac{\\partial L}{\\partial \\bb{y}}.\\]From $\\bb{y} = \\varphi(\\bb{A} \\bb{y}’)$, wehave \\[\\frac{\\partial \\bb{y} }{\\partial \\bb{y}'} = \\diag\\{\\varphi'(\\bb{z} )  \\} \\bb{A}^\\Tr,\\]from where \\[\\delta \\bb{y}' =   \\diag\\{ \\varphi'(\\bb{z} )  \\} \\bb{A}^\\Tr \\delta \\bb{y}.\\]We can therefore summarize the chain rule in our two-layer MLP asfollows: First, we propagate the data forward through the network,computing \\[\\begin{aligned}\\bb{z}' &amp;=&amp; \\bb{B}\\bb{x}  \\\\\\bb{y}' &amp;=&amp; \\phi( \\bb{z}' ) \\\\\\bb{z} &amp;=&amp; \\bb{A}\\bb{y}'  \\\\\\bb{y} &amp;=&amp; \\varphi( \\bb{z} ).\\end{aligned}\\]Then,we propagate the derivatives backward through the network: \\[\\begin{aligned}\\delta \\bb{y} &amp;=&amp; \\nabla L( \\bb{y} ) \\\\\\delta \\bb{A} &amp;=&amp;  \\mathrm{diag}\\{ \\delta \\bb{y} \\}  \\, \\mathrm{diag}\\{ \\varphi'(\\bb{z}) \\}   \\bb{1} \\bb{y}^{\\prime \\Tr} \\\\\\delta \\bb{y}' &amp;=&amp;   \\diag\\{ \\varphi'(\\bb{z} )  \\} \\bb{A}^\\Tr \\delta \\bb{y} \\\\\\delta \\bb{B} &amp;=&amp;  \\mathrm{diag}\\{ \\delta \\bb{y}' \\}  \\, \\mathrm{diag}\\{ \\phi'(\\bb{z}') \\}   \\bb{1} \\bb{x}^\\Tr.\\end{aligned}\\]The entire procedure, known as error backward propagation orbackpropagation for short can be applied recursively for any number oflayers. Forward pass: Starting with $\\bb{y}_0 = \\bb{x}$, compute for$k=1,\\dots, L$       $\\bb{z}_k = \\bb{W}_k \\bb{y}_{k-1}$         $\\bb{y}_k = \\varphi_k(\\bb{z}_k)$   and output $\\bb{y} = \\bb{y}_L$. Backward pass: Starting with $\\delta {y}_L = \\nabla L( \\bb{y} )$, computefor $k=L,L-1,\\dots, 1$       $\\delta \\bb{W}_k =  \\mathrm{diag}{ \\delta \\bb{y}_k } \\, \\mathrm{diag}{ \\varphi’_k (\\bb{z}_k) } \\bb{1} \\bb{y}_{k-1}^\\Tr$         $\\delta \\bb{b}_k =  \\mathrm{diag}{ \\delta \\bb{y}_k }\\varphi’_k (\\bb{z}_k)$         $\\delta \\bb{y}_{k-1} =   \\diag{ \\varphi’_k(\\bb{z_k} ) }\\bb{W}_k^\\Tr \\delta \\bb{y}_k$   We remind that $\\delta \\bb{W}_k$ and$\\delta \\bb{b}_k$ are blocks of coordinates of the gradientof the loss $L$ with respect to the network parameters. Exploding and vanishing gradients Backpropagation allows a recursive calculation of the loss gradient w.r.t. theparameters of the network without the need to ever construct the Jacobianmatrices of each layer’s output w.r.t. its input. Note, however, that in orderto compute the gradient w.r.t. the first layer, $\\delta \\bb{W}_1$,one need to compute the product of $\\varphi’_L(\\bb{z}_L),\\dots,\\varphi’_1 (\\bb{z}_1)$.  This may lead tonumerical instabilities. For example, in a network with $L=20$ layers, a slopeof$\\varphi’ = 2$ in each activation function would be amplified by $10^6$.Similarly, a slope of $\\varphi’ = 0.5$ would diminish to $10^{-6}$ – practicallyto zero. This problem is known as vanishing and exploding gradients, and itprevented end-to-end supervised training of deep neural networks from randominitialization. The introduction of ReLU activations mitigated this problem. In ReLU, thederivative is $1$ for positive arguments and $0$ for negative ones.  Thisimplies that depending on the path through the network from the output back tothe inputs, the product of the activation derivatives will always be either $0$or $1$. The $0$ derivative for negative arguments could still lead to vanishinggradients, but practice shows that, on the contrary, it helps optimization andpromotes sparse solutions. ReLU was probably one of the few significant algorithmic changes in theclassical neural networks that enabled deep learning. Convolutional neural networks The layers on MLP described so far are termed fully connected in the deeplearning literature, due to the fact that every layer input is connected(through some weight) to every output. For large input and output dimensions,such an architecture results in a vast number of degrees of freedom, whichincreases the network complexity and requires more data to train. Weight sharing and shift invariance Weight sharing is a strategy aiming at reducing the layer complexity byreusing the same weights at different parts of the input. For the sake of thefollowing discussion, we assume the input to be discrete and infinitelysupported (i.e., a sequence $\\bb{x} = { x_i },~{i \\in \\mathbb{Z}}$).The output is also assumed to be a sequence,$\\bb{y} = { y_i },~{i \\in \\mathbb{Z}}$.Let us consider the output of the $i$-th neuron, \\[y_i = \\varphi\\left( \\sum_{j \\in \\mathbb{Z}} w_{ij} x_j + b+i \\right).\\]In many cases such as audio signals, images, etc., it is reasonable to assumethat the same operation is valid at different parts of the signal.Mathematically, this can be expressed by asserting that the action of the neuroncommutes with the action of a translation group.  This leads to demanding \\[\\varphi\\left( \\sum_{j \\in \\mathbb{Z}} w_{i-m,j} x_j + b_{i-m} \\right) =\\varphi\\left( \\sum_{j \\in \\mathbb{Z}} w_{ij} x_{j-m} + b_i \\right)\\]for every input $\\bb{x}$. Since the non-linearity is appliedelement-wise, the equivalent condition holds on its arguments as well, \\[\\sum_{j \\in \\mathbb{Z}} w_{i-m,j} x_j  + b_{i-m} =\\sum_{j \\in \\mathbb{Z}} w_{ij} x_{j-m} + b_i =\\sum_{j' \\in \\mathbb{Z}} w_{i,j'+m} x_{j'} + b_i.\\]This implies $b_i = \\mathrm{const}$ and $w_{i-m,j} = w_{i,j+m}$; inother words, if we consider $w_{ij}$ to be the elements of an inifiniteweight matrix, it will have equal elements on each of its diagonals.Another way to express is is by saying that $w_{ij}$ is a function of$i-j$. Toeplitz operators and convolution A linear operator exhibiting the above structure is called Toeplitz.  Theoutput of a shift-invariant (Toeplitz) neuron can be written as \\[y_i = \\varphi\\left( \\sum_{j \\in \\mathbb{Z}} w_{i-j} x_j + b \\right).\\]Note that the weights $\\bb{w}$ can now be considered as a window thatis applied to the input at a certain location to produce an output at the samelocation, and then is slided to a different input location to produce thecorresponding output. This operation (the application of the Toeplitz operator)called convolution, denoted as \\[(w \\ast x)_i =\\sum_{j \\in \\mathbb{Z}} w_{i-j} x_j =\\sum_{j \\in \\mathbb{Z}} w_j x_{i-j} =(x \\ast w)_i.\\]In this notation, the action of our layer can be written as \\[\\bb{y} =\\varphi\\left( \\bb{w} \\ast \\bb{x} + b \\right).\\]In the signal processing jargon, we can say that the input signal$\\bb{x}$ is filtered by a filter with the impulse response$\\bb{w}$. Convolutional layer Neural networks making use of shift-invariant linear operations are calledconvolutional neural networks (CNNs). A convolutional layer accepts an$m$-dimensional vector-valued infinitely supported signal$\\bb{x} = (\\bb{x}^1,\\dots, \\bb{x}^m) ={ (x_i^1,\\dots, x_i^m) }_{i \\in \\mathbb{Z}}$;each input dimension is called a channel or feature map.The layer produces an $n$-dimensional infinitely supported signal$\\bb{y} = (\\bb{y}^1,\\dots, \\bb{y}^n) ={ (y_i^1,\\dots, y_i^n) }_{i \\in \\mathbb{Z}}$ by applying a bank of filters, \\[\\bb{y}^j =\\varphi\\left(  \\sum_{i=1}^m \\bb{w}^{ij} \\ast \\bb{x}^{i}  \\right),\\]or, explicitly, \\[y^j_k = \\varphi\\left(  \\sum_{i=1}^m \\sum_{p} w^{ij}_p x^i_{k-p}  \\right).\\]In practice, each filter $w^{ij}$ is supported on some small fixeddomain. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_03/",
        "teaser":null},{
        "title": "Lecture 5: Recurrent Neural Networks",
        "excerpt":"Multi-layered perceptrons require the input to be of a fixed dimension,and produce an output of a fixed dimension. While CNNs can overcome thislimitation, they still lack the notion of persistence that is sotypical to the human thinking process. For example, when trying toclassify what event is happening at every frame in a video, traditionalneural networks lack the mechanism to use the reasoning about previousevents to inform the later ones. A natural way to introduce such apersistence is by using feedback or recurrence. Enter recurrent neuralnetworks (a.k.a. RNNs). Basic building blocks of an RNN Probably the most useful metaphor to describe a recurrent neural networkis that of a (nonlinear) dynamical system. The network receives asequential input $\\bb{x} _t \\in \\RR^n$ (we will henceforth denotesequences using the time parameter $t$ which will be assumed discrete)and a previous hidden state vector $\\bb{h} _{t-1} \\in \\RR^k$. Thenetwork applies some shift-invariant (that is, time-independent)parametric function $f _{\\bb{\\Theta}}$ to produce the next hidden statevector, \\[\\bb{h} _t = f _{\\bb{\\Theta}} ( \\bb{h} _{t-1}, \\bb{x} _t ).\\]Thehidden state is initialized with some $\\bb{h} _0$ which can be learned,but more frequently is set to zero. If the network is to produce someoutput, an additional parametric function (e.g., a fully-connectedlayer) is applied to the current hidden state $\\bb{h} _t$ to produce theoutput $\\bb{y} _t \\in \\RR^m$, \\[\\bb{y} _t = g _{\\bb{\\Theta}} ( \\bb{h} _{t} ).\\]The simplest RNN that has been proposed in the literature has the formof a fully-connected layer \\[\\bb{h} _t = \\varphi(\\bb{W} _{hh} \\bb{h} _{t-1} + \\bb{W} _{xh} \\bb{x} _t  + \\bb{b}),\\]usually with the hyperbolic tangent activation $\\varphi(x) = \\tanh(x)$,to produce the state update, and another fully connected linear layer ofthe form \\[\\bb{y} _t = \\bb{W} _{hy} \\bb{h} _t.\\]Here, the weight matrices$\\bb{W} _{hh} \\in \\RR^{k \\times k}$, $\\bb{W} _{xh} \\in \\RR^{k \\times n}$,$\\bb{W} _{hy} \\in \\RR^{m \\times k}$ and the bias vector$\\bb{b} \\in \\RR^{k}$ constitute the network parameters that wecollectively denote as $\\bb{\\Theta}$. Various settings There are several ways to use an RNN that depend on the specific task: Many-to-one The network consumes a sequence of inputs${ \\bb{x} _1,\\dots, \\bb{x} _T}$ and produces a sequence of hidden states${ \\bb{h} _1,\\dots, \\bb{h} _T}$ (starting with some $\\bb{h} _0$ that iseither learned or left fixed). The last state $\\bb{h} _T$ is fed to theoutput layer producing a single output $\\bb{y}$ for the entire inputsequence. This approach is typically used for classifying varying lengthinputs such as in sentiment analysis of text. One-to-many The network consumes a single input vector $\\bb{x}$ and an initial state$\\bb{h} _0$ to produce the first hidden state $\\bb{h} _1$. It thenproduces a sequence of hidden states ${ \\bb{h} _2,\\dots, \\bb{h} _T}$without taking any additional input. Alongside, a sequence${ \\bb{y} _1,\\dots, \\bb{y} _T}$ of outputs is generated. Thisarchitecture is often used in image annotation, when the input is asingle image (actually, more frequently, its representation in the formof a feature vector produced by a CNN) and the output is a (variablelength) text sequence describing the image. Many-to-many Two networks are concatenated in the form of an encoder-decoderarchitecture. The many-to-one encoder network consumes a sequentialinput producing a sequence of state vectors. The last state is fed intoa one-to-many decoder network (usually, this state is given as theinitial state of the decoder, which receives no input), and the networkproduces an output sequence. This architecture is common in tasks wherethe length of the input and that of the output are both variable and,potentially, distinct. A bold example is machine translation. Forexample, translating the English sentence A cat ate a mouse (5 words)into the Italian sentence Il gatto ha mangiato un topo (6 words, notethat the word “ate” corresponds to two word “ha mangiato”) or to theGerman sentence Die Katze hat eine Maus gegessen (6 words, note thedifferent order of the verb). Layered RNN The RNN takes its strength from the sequential behavior – in fact, onecan unroll the network action into a very long (infinite) feedforwardnetwork. However, nothing prevents from adding depth to increase thedescriptive capacity of the network in producing the output sequence. Tothat end, we can stack $L$ RNNs one on top of the other, each with itsown hidden state ${ \\bb{h} _t^l }$ and parameters $\\bb{\\Theta}^l$. Thelowest (input) layer receives the input sequence${ \\bb{y} _t^0 = \\bb{x} _t }$ and produces the output sequence${ \\bb{y} _t^1 }$ that is fed as the input (at the same times) to thesubsequent layer. The output of the final layer${ \\bb{y} _t^L = \\bb{y} _t }$ serves as the output sequence of thenetwork. \\[\\begin{aligned}\\bb{h} _t^1 &amp;=&amp; f _{\\bb{\\Theta}^1} ( \\bb{h}^1 _{t-1}, \\bb{x} _t ) \\\\\\bb{y} _t^1 &amp;=&amp; g _{\\bb{\\Theta}^1} ( \\bb{h} _{t}^1 ) \\\\\\bb{h} _t^2 &amp;=&amp; f _{\\bb{\\Theta}^2} ( \\bb{h}^2 _{t-1}, \\bb{y}^1 _t ) \\\\\\bb{y} _t^2 &amp;=&amp; g _{\\bb{\\Theta}^2} ( \\bb{h} _{t}^2 ) \\\\\\vdots \\\\\\bb{h} _t^L &amp;=&amp; f _{\\bb{\\Theta}^L} ( \\bb{h}^L _{t-1}, \\bb{y}^{L-1} _t ) \\\\\\bb{y} _t &amp;=&amp; g _{\\bb{\\Theta}^L} ( \\bb{h} _{t}^L ).\\end{aligned}\\]A trained RNN can also be used as a generative model – we will discussthis in the sequel when dealing with generative models. Training RNNs Let us now discuss the training a recurrent network. Let us assume thatthe RNN is given by the recursive relation \\(\\begin{aligned}\\bb{h} _t &amp;=&amp; f _{\\bb{\\Theta}} ( \\bb{h} _{t-1}, \\bb{x} _t ) \\\\\\bb{y} _t &amp;=&amp; g _{\\bb{\\Theta}} ( \\bb{h} _{t} )\\end{aligned}\\) and the lossfunction is evaluated as the sum of individual losses over each timesample of the output sequence, \\[L(\\bb{\\Theta}) = \\sum _{t &gt; 0} \\ell _t( \\bb{y} _t )\\](for example, wecan apply a softmax function to the outputs and evaluate a cross-entropyloss). In order to compute the gradient of the loss w.r.t. the networkparameters $\\bb{\\Theta}$, one needs to perform the forward pass on theentire input sequence. In theory, the input is of infinite length; inpractice, it has some finite length $T$ (possibly very big). The chainrule yields \\[\\delta \\bb{\\Theta} =  \\frac{\\partial L(\\bb{\\Theta}) }{\\partial \\bb{\\Theta}} = \\sum _{1 \\le t \\le T}   \\frac{ \\partial  \\ell _t  }{\\partial \\bb{\\Theta}},\\]with\\(\\frac{ \\partial  \\ell _t }{\\partial \\bb{\\Theta}}= \\frac{\\partial \\ell _t }{\\partial   \\bb{y} _t} \\left(  \\frac{\\partial^+ \\bb{y} _t }{\\partial \\bb{\\Theta}} +  \\sum _{1 \\le i \\le t}   \\frac{ \\partial \\bb{y} _t }{\\partial \\bb{h} _t }     \\frac{ \\partial \\bb{h} _t }{\\partial \\bb{h} _i }   \\frac{ \\partial^+ \\bb{h} _i }{\\partial \\bb{\\Theta}}  \\right),\\) where \\[\\frac{ \\partial^+ \\bb{h} _i }{\\partial \\bb{\\Theta}} = \\frac{ \\partial  f  }{\\partial \\bb{\\Theta}} ( \\bb{h} _{i-1} , \\bb{x} _i  )\\]refers to the “immediate” derivative of $\\bb{h} _i$ w.r.t. $\\bb{\\Theta}$,assuming $\\bb{h} _{t-1}$ constant, and \\[\\frac{ \\partial^+ \\bb{y} _t }{\\partial \\bb{\\Theta}} = \\frac{ \\partial  g  }{\\partial \\bb{\\Theta}} ( \\bb{h} _{t}  )\\]refers to the “immediate” derivative of $\\bb{y} _t$ w.r.t. $\\bb{\\Theta}$,assuming $\\bb{h} _{t}$ constant. We can immediately susbstitute thepartial derivative \\[\\frac{ \\partial \\bb{y} _t }{\\partial \\bb{h} _t }  =  \\frac{ \\partial  g  }{\\partial \\bb{h}} ( \\bb{h} _{t}  ).\\]The Jacobian of $\\bb{h} _t$ w.r.t. $\\bb{h} _i$ is evaluated invoking thechain rule \\(\\frac{ \\partial \\bb{h} _t }{\\partial \\bb{h} _i }  = \\frac{ \\partial \\bb{h} _t }{\\partial \\bb{h} _{t-1} }  \\frac{ \\partial \\bb{h} _{t-1} }{\\partial \\bb{h} _{t-2} }   \\cdots    \\frac{ \\partial \\bb{h} _{i+1} }{\\partial \\bb{h} _{i} }=  \\frac{ \\partial  f  }{\\partial \\bb{h}}( \\bb{h} _{t-1} , \\bb{x} _t) \\, \\frac{ \\partial  f  }{\\partial \\bb{h}}( \\bb{h} _{t-2} , \\bb{x} _{t-1}) \\cdots  \\frac{ \\partial  f  }{\\partial \\bb{h}}( \\bb{h} _{i} , \\bb{x} _{i+1}).\\) Note that each term $\\frac{ \\partial  \\ell _t }{\\partial \\bb{\\Theta}}$has the same form, and the behavior of these terms determine thebehavior of the entire sum. Every gradient component$\\frac{ \\partial  \\ell _t }{\\partial \\bb{\\Theta}}$ is, in turn, also asum whose terms \\(\\frac{\\partial \\ell _t }{\\partial   \\bb{y} _t}\\frac{ \\partial \\bb{y} _t }{\\partial \\bb{h} _t }     \\frac{ \\partial \\bb{h} _t }{\\partial \\bb{h} _i }   \\frac{ \\partial^+ \\bb{h} _i }{\\partial \\bb{\\Theta}}\\) can be interpretedas temporal contributions, measuring how $\\bb{\\Theta}$ at step $i$affects the loss at step $t &gt; i$. The factors$\\frac{ \\partial \\bb{h} _t }{\\partial \\bb{h} _i }$ have the role oftransporting the error “back in time” from $t$ to $i$. Long-termcontributions correspond to $i \\ll t$, while short-term contributionsto $i \\sim t$. In the sequel, we will analyze the dynamics of thesefactors to unearth major numerical issues associated withbackpropagation through RNNs. Backpropagation through time The metaphor of an RNN unrolled into a (very long) feed-forward networkallows to see immediately that the chain rule we saw before is exactlythe backpropagation rule we have already encountered for MLPs. The onlydifference is that now each layer depends on the same parameters,hence the additional sum over $i$ arises in the calculations. Instead ofcomputing individual gradients w.r.t. the parameters of each layer as wedid in MLPs or CNNs, they are accumulated into a single gradient of theshared parameters. Such a backward step is known under the name ofbackpropagation through time or BPTT. Since RNN input sequences might be very long in practice (consider, forexample, the entire set of wikipedia text), BPTT is rarely used as is.Instead, the forward pass is performed in chunks of a fixed number oftime samples (still keeping the state created from the beginning of theinput sequence), followed by backpropagation performed for the samenumber of steps backwards in time. This training strategy is known astruncated backpropagation through time (TBPTT) and is much morepractical computationally. Vanishing and exploding gradients Substituing the particular parametrization \\(\\begin{aligned}\\bb{z} _t &amp;=&amp; \\bb{W} _{hh} \\bb{h} _{t-1} + \\bb{W} _{xh} \\bb{x} _t + \\bb{b} \\\\\\bb{h} _t &amp;=&amp; \\varphi( \\bb{z} _t ) \\\\\\bb{y} _t &amp;=&amp;  \\bb{W} _{hy} \\bb{h} _t\\end{aligned}\\) we can write \\[\\frac{ \\partial \\bb{y} _t }{\\partial \\bb{h} _t }  = \\bb{W} _{hy}^\\Tr.\\]and \\[\\frac{ \\partial \\bb{h} _i }{\\partial \\bb{h} _{i-1} }  =  \\bb{W} _{hh}^\\Tr \\, \\mathrm{diag}\\{ \\varphi'( \\bb{z} _i ) \\}.\\]The immediate derivative of $\\bb{h} _t$ w.r.t. the weight matrix$\\bb{W} _{hh}$ is a rank-3 tensor, so we will write its product with thegradient of the loss w.r.t. $\\bb{h} _t$ \\[\\frac{ \\partial^+ \\bb{h} _i }{\\partial \\bb{W} _{hh}} \\delta \\bb{h} _i =  \\mathrm{diag}\\{ \\delta \\bb{h} _i \\} \\mathrm{diag}\\{ \\varphi'( \\bb{z} _i ) \\} \\bb{1}\\bb{h} _{i-1}^\\Tr\\]where $\\delta \\bb{h} _t = \\frac{\\partial \\ell _t }{\\partial   \\bb{h} _t}$.Using these calculations, we can express each temporal contribution tothe gradient as \\(\\frac{\\partial \\ell _t }{\\partial   \\bb{y} _t}\\frac{ \\partial \\bb{y} _t }{\\partial \\bb{h} _t }     \\frac{ \\partial \\bb{h} _t }{\\partial \\bb{h} _i }   \\frac{ \\partial^+ \\bb{h} _i }{\\partial \\bb{W} _{hh}}  =  \\mathrm{diag}\\{\\delta \\bb{y} _t \\} \\bb{W} _{hy}^\\Tr  \\bb{W} _{hh}^\\Tr \\, \\mathrm{diag}\\{ \\varphi'( \\bb{z} _{t} ) \\}  \\cdots  \\bb{W} _{hh}^\\Tr \\, \\mathrm{diag}\\{ \\varphi'( \\bb{z} _{i+1} ) \\}  \\mathrm{diag}\\{ \\varphi'( \\bb{z} _i ) \\} \\bb{1}\\bb{h} _{i-1}^\\Tr.\\)Observe that due to the term$\\frac{ \\partial \\bb{h} _t }{\\partial \\bb{h} _i } $, the weight matrix$\\bb{W} _{hh}$ and the diagonal matrix $\\mathrm{diag}{ \\varphi’ }$appear $t-i-1$ times in the product. Let us now analyze the influence of the above product on the long-term($i \\ll t$) contributions. For the sake of simplicity, we assume that$\\varphi = \\mathrm{id}$, leaving us with $(\\bb{W} _{hh}^\\Tr)^l$,$l=t-i \\gg 1$. We define the spectral radius of the matrix,$\\rho( \\bb{W} _{hh} )$, as its maximum absolute eigenvalue. Simple linearalgebra suggests that if $\\rho( \\bb{W} _{hh} ) &lt; 1$, then$(\\bb{W} _{hh}^\\Tr)^l$ vanishes exponentially as $l$ approaches infinity,and, hence, the long-term contribution to the gradient will vanish. Inthe opposite situation when $\\rho( \\bb{W} _{hh} ) &gt; 1$,$(\\bb{W} _{hh}^\\Tr)^l$ will magnify exponentially some directionscorresponding to the eigenvalues bigger than $1$ in the absolute value;however, directions corresponding to eigenvalues smaller that $1$ willshrink. Hence, $\\rho( \\bb{W} _{hh} ) &gt; 1$ is a necessary condition forthe exploding long-term contributions, but not a sufficient one. If we now assume that $\\varphi$ is not identity, yet has a boundedderivative, $|\\varphi’| &lt; \\gamma$, the above result straightforwardlysuggests $\\rho( \\bb{W} _{hh} ) &lt; 1/\\gamma$ being a sufficient conditionfor vanishing long-term gradients, and $\\rho( \\bb{W} _{hh} ) &gt; 1/\\gamma$a necessary condition for exploding long-term gradients. Numerical tricks Vanishing and exploding gradients have been plaguing RNNs for manyyears, not allowing their efficient training. Several numerical trickshave been proposed to partially overcome these problems. One of thestraightforward ideas is to control the gradient scale by the followingheuristic: if $| \\bb{g} | &gt; \\tau$, modify it to$\\frac{\\tau \\bb{g}}{| \\bb{g} | }$. The insights into the importance of the spectral radius of the weightmatrix suggests a way to safely initialize it: Suppose $\\bb{W} _{hh}$ isinitialized to some random values. We compute the spectral radius$\\rho = \\rho(\\bb{W _{hh}})$ and scale it by $\\frac{c}{\\gamma \\rho}$ with$c \\approx 1.1$. With such a setting, the gradients are guaranteed notto vanish but will neither explode too rapidly. Another heuristic comes from the empirical observation that when thegradients explode, they do so along some direction, and the curvature ofthe loss function (expressed via the corresponding second-orderdirectional derivative) also explodes. Hence, second-order optimizationmethods relying on the Hessian matrix should be capable of scaling thegradient components according to the curvature and make small steps inthe direction of the exploding gradient/curvature. This explains thesuccess of second-order methods in RNN training. While a full Newtonstep is prohibitively expensive, its truncated version or theGauss-Newton method approximating the Hessian via an outer product ofgradients are known to perform very well and be less sensitive to theexploding gradients issues. Yet another way to avoid exploding or vanishing gradients is by means ofregularization made such that the back-propagated gradients neitherincrease or decrease too much in magnitude. This can be achieved byadding a regularization term of the form \\(R = \\sum _{k} \\left(  \\frac{\\left\\|  \\delta \\bb{h} _{k+1}  \\frac{\\partial \\bb{h} _{k+1}}{ \\partial \\bb{h} _{k} }  \\right\\| }{\\| \\delta \\bb{h} _{k+1}  \\| } - 1\\right)^2.\\) To make the regularization tractable, its derivatives areapproximated only as the “immediate” derivatives w.r.t. $\\bb{W} _{hh}$. Gated recurrent units Another way to address the vanishing gradients problem is by modifyingthe structure of the recurrent cell in order to avoid the product with$\\bb{W} _{hh}$ in the backward step. A basic gated recurrent unit (GRU)consists of two gate signals, the update gate \\[\\bb{z} _t = \\sigma( \\bb{W} _{hz} \\bb{h} _{t-1} + \\bb{W} _{xz} \\bb{x} _t + \\bb{b} _z  )\\]and the reset gate \\[\\bb{r} _t = \\sigma( \\bb{W} _{hr} \\bb{h} _{t-1} + \\bb{W} _{xr} \\bb{x} _t + \\bb{b} _r  )\\]computed using a linear transfer function followed by the sigmoidactivation with dedicated parameters. The sigmoid scales each gatesignal between $0$ and $1$. A candidate state update is calculated from the current input and theprevious state as usual, with the only difference that now thecontribution of the previous state is gated by the reset gate signal, \\[\\bb{q} _t = \\varphi( \\bb{r} _t \\odot \\ \\bb{W} _{hh} \\bb{h} _{t-1} + \\bb{W} _{xh} \\bb{x} _t  + \\bb{b} _q ).\\]Here $\\odot$ denotes Hadamard (element-wise) product and$\\varphi = \\tanh$. This gating can be thought of as a soft (and,consequently, differentiable) version of an if…then condition allowingto selectively use only some coordinates of the transformed previousstate and thus forget how much of the previous context to carry to thenext time step. The final state update is computed by blending the candidate new stateand the previous state, with the blending controlled element-wise by theupdate gate, \\[\\bb{h} _t = \\bb{z} _t \\odot \\bb{h} _{t-1} + (1-\\bb{z} _t) \\odot \\bb{q} _t.\\]The update gate controls how much of the past information to forget. Invoking the chain rule again, we obtain \\(\\begin{aligned}\\frac{\\partial \\bb{h} _t}{\\partial \\bb{h} _{t-1} } &amp;=&amp; \\mathrm{diag}\\{ \\bb{z} _t  \\} +   \\bb{W} _{hz}^\\Tr \\, \\mathrm{diag}\\{ \\sigma'( \\bb{W} _{hz} \\bb{h} _{t-1} + \\bb{W} _{xz} \\bb{x} _t + \\bb{b} _z ) \\} \\mathrm{diag}\\{ \\bb{h} _{t-1} - \\bb{q} _t \\}  \\\\&amp;&amp; + \\bb{W} _{hh}^\\Tr \\, \\mathrm{diag}\\{ \\sigma'( \\bb{W} _{hh} \\bb{h} _{t-1} + \\bb{W} _{xh} \\bb{x} _t + \\bb{b} _h ) \\} \\mathrm{diag}\\{ 1-\\bb{z} _{t-1} \\}.\\end{aligned}\\)Note that, as before, this term contains terms containing$\\bb{W} _{hh}^\\Tr$, however, now a free term depending only on $\\bb{z} _t$is added. Therefore, when the update gate is open $\\bb{z} _t \\approx 1$,the gradients flow backward in time uninmpeded. This allows a gated RNNto learn long-term context without significant numerical issues. Variants of the gated architecture exist, with the most popular onebeing the Long Short Term Memory (LSTM) cell. Attention Informally, an attention mechanism equips a NN with the ability tofocus on a subset of its inputs (or intermediate features) by selectingor emphasizing specific inputs. This resembles, for example, to humanvisual attention – the foveal region of the retina where the samplingresolution is the highest has an angular aperture of only 1 degree. Itis the constant motion of our sight that allows us to actually see. Thepattern of the motion is driven by an attention mechanism that dependson content of what we see. A similar mechanism can be built into a neural network. The basic ideaof attention is similar to gating, except that it happens in parallerrather than being serial as in GRUs. Consider a regular one-to-many RNNreceiving a single input $\\bb{x}$ and producing a sequence of hiddenstates ${ \\bb{h} _t }$ via the function \\[\\bb{h} _t = f _{\\bb{\\Theta}}(\\bb{h} _{t-1}, \\bb{x} _t) = \\varphi( \\bb{W} _{hh} \\bb{h} _{t-1} + \\bb{W} _{xh} \\bb{x} _t  + \\bb{b} _h).\\]For example, such a network could produce a textual annotation of animage. In an attention network, instead of feeding $\\bb{x}$ directly tothe RNN, a gated version thereof is computed, \\[\\bb{z} _t = \\bb{x} _t \\odot \\bb{g} _t\\]and the network is applied to$\\bb{z}$, \\[\\bb{h} _t = \\varphi( \\bb{W} _{hh} \\bb{h} _{t-1} + \\bb{W} _{xh} \\bb{z} _t  + \\bb{b} _h).\\]Note that the gate signal is applied simultaneously to all elements ofthe input and can me thought of as a soft (and, consequently,differentiable) version of masking. The gate $\\bb{g}$ itself is computed from the input and the currentstate by means of another parametric function \\[\\bb{a} _t = \\varphi( \\bb{W} _{ha} \\bb{h} _{t-1} + \\bb{W} _{xa} \\bb{z} _t  + \\bb{b} _a).\\]This intermediate signal is then soft-maxed (with the temperatureparameter $\\alpha$ controlling the sharpness of the softmax), \\[\\bb{g} _t = \\frac{e^{ \\alpha \\bb{a} _t }}{\\bb{1}^\\Tr re^{ \\alpha \\bb{a} _t }}.\\]Obviously, and RNN can use both gating (e.g., LSTM) and attentionmechanisms – they serve different purposes. Alternatives to RNNs While the invention of gated RNNs such as LSTM and some numericalheuristics mentioned before have significantly improved the ability totrain RNNs and learn long-term dependencies in the thousands of timesamples, recurrent networks still suffer from their inherentlysequential structure, which is bad both for efficient training andinference. Today, a growing amount of evidence suggests that properlydesigned CNNs constitute a formidable alternative to RNNs. Using asignal processing metaphor, RNNs resemble infinite impulse response(IIR) filters that are sequential and are based on recursion, whilealternatives are more like finite impulse response filters (FIRs) thatare easily parallelizable. While RNNs can in theory learn arbitrarilylong term dependencies, this never happens in practice, and asufficiently deep CNN can achieve similar performance. In what follows,we briefly overview the time convolutional network (TCN) architecture. Time convolutional network (TCN) TCN can be viewed as a FIR equivalent of an RNN. The main element of TCNis the dilated convolution defined as \\[(\\bb{x} \\ast _d \\bb{w} ) _n = \\sum _{k} w _k x _{n - dk},\\]where $\\bb{x}$is the input sequence, $\\bb{w}$ is the filter impulse response, and$d \\in \\bb{N}$ is the dilation factor. Dilation factor $d$ essentiallyintroduces a fixed step between the filter taps. For $d=1$, dilatedconvolution reduces to its regular counterpart. For time signals, thefilters are made causal ($w _k = 0$ for $k&lt;0$). The receptive field of adilated convolution with the filter of length $K$ is $Kd$. Usingexponentially increasing dilation factor in a deep network, $d = 2^l$,with $l$ being the layer number, essentially allows a relatively shallownetwork to have a very long history in time. Causal dilatied convolutions can be combined with other standard CNNarchitectural choices such as residual connections, weight decay, batchnormalization, drop out, etc. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_05/",
        "teaser":null},{
        "title": "Lecture 6: Unsupervised learning and generative models",
        "excerpt":"The only training regime we have seen so far for deep neural models wassupervised: the network was given a set of example instances andtrained so to minimize a discrepancy between the output produced by thenetwork and the corresponding desired outputs. The latter outputs couldbe either class labels in classification problems, or some continuousscalars or vectors in regression problems. In what follows, we extendour machine learning arsenal to the unsupervised setting, in whichonly example instances and no example outputs are provided. The goal ofunsupervised learning is to discover and model the structure of theinstance space. One of the most useful applications of such models is togenerate new examples of instances. Autoencoders One of the most powerful ideas that humans have invented to model natureis that of parsimony. The most famous statement of this heurisitc isattribuited to the Franciscan friar William of Ockham, who suggestedthat when presented with two competing hypothetical answers to aproblem, one should select the one that makes the fewest assumptions. Inthe realm of data science, Ockham’s razor can be translated to theassertion that many types of data such as signals and images despitebeing naturally embedded in a very high dimensional space, possess amuch smaller number of degrees of freedom. For example, the space of all$255 \\times 255$ $8$-bit images theoretically admits about $10^{157826}$distinct instances, the overwhelming majority of which do not look likea natural image. It can therefore be posited that natural images can bedescribed as a low-dimensional manifold embedded into the$65536$-dimensional space. Techniques for discovering this manifold (or,in the probabilistic setting, a probability distribution supported on alow-dimensional manifold) are called dimensionality reduction. Deep learning offers a formidable tool for dimensionality reduction inthe form of autoencoders. An autoencoder can be viewed as acomposition of two neural networks, and encoder$\\Phi : \\mathcal{X} \\rightarrow \\mathcal{Z}$ and a decoder$\\Psi : \\mathcal{Z} \\rightarrow \\mathcal{X}$. Given an instance$\\bb{x} \\in \\mathcal{X}$, the encoder produces a code vector$\\bb{z} = \\Phi(\\bb{x})$ called a latent representation (the space$\\mathcal{Z}$ of such representations is called the latent space). Therole of the decoder is to reproduce an instance from its encoding,$\\bb{x} = \\Psi(\\bb{z})$. Ideally, an autoencoder should be such that the concatenaton of theencoder with the decoder results in an identity transformation,$\\Psi \\circ \\Phi \\approx \\mathrm{id}$. As long as the dimension of thelatent space $\\mathcal{Z}$ is lower than that of the instance space$\\mathcal{X}$, $\\Psi \\circ \\Phi$ cannot be a trivial identity map andhas to actually reveal the hidden (and, potentially, very non-trivial)degrees of freedom in the data. The combination of adimensionality-reducing encoder followed by a dimensionality-increasingencoder forces the data to pass through a bottleneck. Both networksare trained on example instances minimizing \\[\\min _{ \\bb{\\alpha}, \\bb{\\beta} } \\sum _{i} D( \\bb{x} _i , \\Psi _{\\bb{\\alpha}'}( \\Phi _{\\bb{\\alpha}}( \\bb{x} _i ))  ),\\]where $\\bb{\\alpha}$ and $\\bb{\\beta}$ are the parameters of the encoderand the decoder, respectively, and $D$ is some useful distance on$\\mathcal{X}$, e.g., $D(\\bb{x},\\bb{x}’) = |  \\bb{x} - \\bb{x}’ | _2^2$.As we will see in the sequel, the choice of this distance has a crucialimportance. The minimization of a loss of the form$| \\bb{x} - \\Psi _{\\bb{\\alpha}’}( \\Phi _{\\bb{\\alpha}}( \\bb{x} ))  |^2$might seem as a regression problem; in regression as well, we seek some(often parsimonious) parameteric model of the map between the instancespace and the label space. Here, however, the label space is equal tothe input space, and we restrict the regressor to contain a bottleneck.By minimizing the above loss, the encoder tries to extract only thosefeatures of the data that allow its most faithful reconstruction, thuscreating a low-dimensional model thereof. Convolutional autoencoder CNN architecture are often used in autoencoders. Typically, an encoderis a CNN that uses strided convolutions (or pooling) to gradually reducethe signal dimension toward the bottleneck. One or more fully connectedlayers may also be used before the output layer, which produces theencoding in the latent space. A decoder can be thought of as atransposed version of the encoder, in which the dimensionalitygradually increases toward the output. Though the decoder does notnecessarily need to match the same dimensions (in reversed order) of theencoder’s intermediate layers, such symmetric architectures are veryfrequent. In what follows, we remind the working of a convolutionallayer and describe how to formally transpose it. Convolutional layer Recall that a convolutional layer accepts an $m$-dimensionalvector-valued (infinitely supported) signal$\\bb{x} = (\\bb{x}^1,\\dots, \\bb{x}^m) = { (x _i^1,\\dots, x _i^m) } _{i \\in \\mathbb{Z}}$,each input dimension of which is called a channel or feature map.The layer produces an $n$-dimensional (infinitely supported) signal$\\bb{y} = (\\bb{y}^1,\\dots, \\bb{y}^n)  = { (y _i^1,\\dots, y _i^n) } _{i \\in \\mathbb{Z}}$by applying a bank of filters, potentially retaining only every $d$-thoutput sample (a fact known as striding), \\[\\bb{y}^j = \\downarrow _{d} \\left( \\sum _{i=1}^m \\bb{w}^{ij} \\ast \\bb{x}^{i}   \\right) ,\\]where $\\downarrow _d$ denotes striding (a.k.a. compression ordown-sampling), $(\\downarrow _d \\bb{x} ) _{k} = \\bb{x} _{dk}$. Obviously, abias and an element-wise non-linear activation are applied to $\\bb{y}$. Explicitly, the action of the convolutional layer can be written as \\[y^j _k =    \\sum _{i=1}^m \\sum _{p} w^{ij} _p x^i _{dk-p}.\\]Typically,each filter $w^{ij}$ is supported on some small fixed domain. Though wethink of the signal $\\bb{x}$ as of a function of a one-dimensional“time” index $n \\in \\mathbb{Z}$, the same notation is perfectly validfor images; in the latter case, time indices become higher-dimensionalmulti-indices such as $\\bb{n} \\in \\mathbb{Z}^2$. In the case of finitely-supported time signals that can be representedas vectors $\\bb{x}^1\\dots,\\bb{x}^m \\in \\mathbb{R}^M$, the action of aconvolutional layer with $d=1$ can be described as \\[\\bb{y}^j =   \\bb{W}^{1j} \\bb{x}^1 + \\cdots + \\bb{W}^{mj} \\bb{x}^m,\\]where each $\\bb{W}^{ij}$ is an $M \\times M$ Toeplitz (diagonal-constant)matrix (a block-Toeplitz matrix in the case of images). $d$-stridedconvolution has the same form, but now $\\bb{W}^{ij}$ are$\\frac{M}{d} \\times M$ obtained by retaining every $d$-th row from theoriginal Toeplitz matrix. Denoting the striding (sub-sampling) by thematrix $\\bb{S} _d$, we obtain \\[\\begin{aligned}\\bb{y}^1 &amp;=&amp;   \\bb{S} _d \\bb{W}^{11} \\bb{x}^1 + \\cdots + \\bb{S} _d \\bb{W}^{m1} \\bb{x}^m \\nonumber\\\\\\vdots &amp; &amp; \\vdots  \\nonumber\\\\\\bb{y}^n &amp;=&amp;   \\bb{S} _d \\bb{W}^{1n} \\bb{x}^1 + \\cdots + \\bb{S} _d \\bb{W}^{mn} \\bb{x}^m.\\end{aligned}\\]Transposed convolutional layer A transposed convolutional layer (often incorrectly referred to as“deconvolutional” in the literature) can be thought of as a formaladjoint operator1 of the above linear operator. We will denote the input by the $m$-dimensional vector-valued signal$\\bb{y} = (\\bb{y}^1,\\dots, \\bb{y}^n) = { (y _i^1,\\dots, y _i^n) } _{i \\in \\mathbb{Z}}$,and the output by the $n$-dimensional signal$\\bb{x} = (\\bb{x}^1,\\dots, \\bb{x}^m)  = { (x _i^1,\\dots, x _i^m) } _{i \\in \\mathbb{Z}}$.The linear part of the layer’s action is expressed by the formaltranposition of the action of the convolutional layer, \\[\\begin{aligned}\\bb{x}^1 &amp;=&amp;   \\overline{\\bb{W}}^{11}  \\bb{S} _d^\\Tr \\bb{y}^1 + \\cdots +  \\overline{\\bb{W}}^{1n}  \\bb{S} _d^\\Tr \\bb{y}^n \\nonumber\\\\\\vdots &amp; &amp; \\vdots  \\nonumber\\\\\\bb{x}^m &amp;=&amp;    \\overline{\\bb{W}}^{m1}  \\bb{S} _d^\\Tr \\bb{y}^1 + \\cdots +   \\overline{\\bb{W}}^{mn}  \\bb{S} _d^\\Tr \\bb{y}^n,\\end{aligned}\\]where $\\overline{\\bb{W}}^{ij} = (\\bb{W}^{ij})^\\Tr$ is the Toeplitzmatrix formed by the mirrored filter$\\overline{w}^{ij} _k = w^{ij} _{-k}$, and $\\bb{S} _d^\\Tr$ is a$d$-upsampling (a.k.a. expansion or dilation) operation, \\[(\\uparrow _d y) _k = \\left\\{ \\begin{array}{ll} y _{k/d} &amp; \\mathrm{if}\\, k \\in d \\bb{Z}; \\\\0 &amp; \\mathrm{else}.\\end{array} \\right.\\]Note that since the order of $\\bb{S} _d$ and$\\bb{W}^{ij}$ is reversed after transposition, the input signal $\\bb{y}$is first up-sampled and then convolved with the mirrored filters. The(linear) action of the transposed convolutional layer can be thereforesummarized as \\[\\bb{x}^i =  \\sum _{j=1}^n \\overline{\\bb{w}}^{ij} \\ast  \\left( \\uparrow _{d}  \\bb{y}^{j}  \\right)   .\\]Note that despite superficial similarity, this is not dilatedconvolution! While transposed strided convolution is$\\overline{\\bb{w}} \\ast (\\uparrow _d \\bb{y} )$, dilated convolution is$(\\uparrow _d \\bb{w}  ) \\ast \\bb{y}$. It is important to note that while in terms of dimensionality thetransposed convolutional layer is an inverse of the convolutional layer,it is the adjoint and not the inverse of the latter (this is why theterm “deconvolution” is inappropriate here). Since convolutional andtransposed convolutional layers of a convolutional autoencoder areparametrized independently, the weights of the transposed layer in thedecoder need not match those of its counterpart in the encoder. Pooling and unpooling As an alternative to striding, CNN architectures sometimes use poolingto reduce the output dimensionality and obtain invariance properties. A$d$-pooling layer takes a sequence ${x _i} _{i \\in \\mathbb{Z}}$ as theinput and produces a new sequence ${y _i} _{i \\in \\mathbb{Z}}$ as theoutput replacing non-overlapping windows of size $d$ in its input,$(x _{di},\\dots,x _{(d+1)i-1})$, with a scalar $y _i$. For example,average pooling produces \\[(x _{di},\\dots,x _{(d+1)i-1}) \\mapsto y _i = \\frac{1}{d} \\sum _{j=0}^{d-1} x _{di + j}\\]in which each output sample is the average of the samples in thecorresponding input window2. Similarly, max pooling is thewindow-wise maximum of the input, \\[(x _{di},\\dots,x _{(d+1)i-1}) \\mapsto  y _i = \\max \\{ x _{di}, x _{di+1}, \\dots, x _{(d+1)i -1} \\}.\\]In order to perform backpropagation, max pooling keeps the index of theselected maximum, \\[k^\\ast _i =  \\mathrm{arg}\\max _{j \\in \\{ 0,\\dots,d-1 \\} } x _{di+j}.\\]When the input is vector-valued (i.e., has multiple channels), poolingis applied channel-wise. When the input is multidimensional (e..g, animage), the pooling window is created as an appropriate Cartesianproduct. The transposed version of the pooling layer (referred to as unpoolingin the literature) can be used in the decoder network. A $d$-unpoolinglayer takes an input sequence ${y _i} _{i \\in \\mathbb{Z}}$ and producesthe output sequence ${x _i} _{i \\in \\mathbb{Z}}$, such that each sample$y _{i}$ in the input corresponds to a non-overlapping window$(x _{di},\\dots,x _{(d+1)i-1})$ in the output. Average unpooling can beseen as the map \\[y _i \\mapsto \\left( \\frac{y _i}{d}, \\dots, \\frac{y _i}{d} \\right).\\]Maxunpooling, on the other hand, is the map \\[y _i \\mapsto \\bb{e} _{k^\\ast _i},\\]where $\\bb{e} _k$ is the $k$-thstandard basis vector of $\\mathbb{R}^d$ containing $1$ at $k$-thcoordinate and zeros elsewhere, and $k _i^\\ast$ is the index kept by thecorresponding max pooling counterpart. Variational autoencoders While regular autoencoders can produce very intricate data models, it isstill unclear how to generate new instances from the model. Thedistribution of the latent variable $\\bb{z}$ produced by the encoder canbe very complicated, and sampling from it may result impractical. A verypopular variant of autoencoders combines this type of neural networkswith variational Bayesian methods. In what follows, we will revisit ourproblem from this perspective and see how this reformulation leads tothe variational autoencoder (VAE). Decoder Let us start from looking at the generative (decoder) part of the modelfrom a probabilistic perspective. The latent variable $\\bb{Z}$ is arandom vector distributed according to some prior distribution$p(\\bb{Z})$. The instance $\\bb{X}$ is a random vector over $\\mathcal{X}$and is described by the probability distribution $p(\\bb{X} | \\bb{Z})$conditioned on $\\bb{Z}$. In the Bayesian jargon, this conditionalprobability is known as the likelihood. Note that in this formulation,given a specific realization of the latent variable, $\\bb{Z}=\\bb{z}$,the output of the encoder is still a stochastic quantity distributedaccording to $p(\\bb{X} | \\bb{z})$. It is customary to set$p _{\\bb{\\beta}}(\\bb{X} | \\bb{z}) = \\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$,where $\\Psi _{\\bb{\\beta}}$ is a deterministic map between the latentspace and the instance space (in practice, our encoder network), and$\\sigma$ is a fixed hyper-parameter. This allows$p _{\\bb{\\beta}}(\\bb{X} | \\bb{Z})$ to be computable and continuous in itsparameters $\\bb{\\beta}$. Since any distribution can be mapped into any other distribution by asufficiently complicated map, we can fix the prior distribution to benormal, $p(\\bb{Z}) = \\mathcal{N}(\\bb{0},\\bb{I})$. We can now generatenew instances by drawing $\\bb{z}$ at random from the multivariate normaldistribution, calculating the mean vector $\\Psi _{\\bb{\\beta}}(\\bb{z})$,and then drawing an $\\bb{x}$ instance from$\\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$. Encoder The inference (encoder) part of the model aims at inferring the latentvariable given the observed instance, which in the Bayesian languageamounts to calculating the posterior \\[p(\\bb{Z}|\\bb{X}) = \\frac{p(\\bb{X}|\\bb{Z}) p(\\bb{Z}) }{p(\\bb{X})}.\\]The denominator expresses the probability distribution on the instancespace, known as the evidence in the Bayesian terminology. While it hasa simple expression due to the total probability formula, \\[p(\\bb{X}) = \\int p(\\bb{X}|\\bb{z}) p(\\bb{z}) d\\bb{z},\\]However, thelatter integral is intractable in practice, since its accurateapproximation in a relatively high-dimensional latent space mapping to acomplicated instance distributiuon would require an enormously largesample. The main idea of VAEs is to approximate the posterior with a parametricfamily of distributions, $q(\\bb{Z} | \\bb{X})$. A typical choice is,again, normal$q(\\bb{Z} | \\bb{x}) \\sim \\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) )$,where the mean and covariance parameters of the distribution areproduced by the encoder neural network parametrized by $\\bb{\\alpha}$. Inpractice, $\\bb{\\Sigma}$ is constrained to be a diagonal matrix, so$q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x}) \\sim \\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\mathrm{diag}{ \\bb{\\sigma} _{\\bb{\\alpha}}(\\bb{x}) } )$. Evidence lower bound In order to ensure that the variational posterior$q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})$ approximates well the true posterior$p(\\bb{Z} | \\bb{X})$, we can measure the Kullback-Leibler divergence3 \\[\\begin{aligned}\\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left\\|\\, p(\\bb{Z} | \\bb{X})\\right.\\right) &amp;=&amp; \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }( \\log q _{\\bb{\\alpha}}(\\bb{z} | \\bb{X}) - \\log  p(\\bb{z} | \\bb{X}) ) \\\\&amp;=&amp;  \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }( \\log q _{\\bb{\\alpha}}(\\bb{z} | \\bb{X}) - \\log  p(\\bb{X} | \\bb{z})    - \\log  p(\\bb{z})  ) + \\log p(\\bb{X});\\end{aligned}\\]note that $\\log p(\\bb{X})$ is outside the expectation since it does notdepend on $\\bb{z}$. This expression can be rewritten as \\[\\log p(\\bb{X}) - \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left\\|\\, p(\\bb{Z} | \\bb{X})\\right.\\right) =  \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }(  \\log  p(\\bb{X} | \\bb{z}) )-  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\\]Note that while we still cannot evaluate the left-hand-side second term(because of the $p(\\bb{X})$ appearing in the KL divergence), we observethat$\\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left|\\, p(\\bb{Z} | \\bb{X})\\right.\\right)$is non-negative and small if the model $q _{\\bb{\\alpha}}$ is rich enough.We can therefore express the following lower bound on the log evidence \\[\\log p(\\bb{X}) \\ge \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }( \\log  p _{\\bb{\\beta}}(\\bb{X} | \\bb{z}) )-  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right).\\]Loss function Recall that $P(\\bb{x})$ expresses the probability of a given instance$\\bb{x}$ under the entire generative process. Since we aim at maximizingthe probability of each instance, our goal is to minimize the followingloss function: \\[L =-\\mathbb{E} _{\\bb{x}} \\log P(\\bb{x}) \\le \\mathbb{E} _{\\bb{x}}  \\left( \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }( -\\log  p _{\\bb{\\beta}}(\\bb{x} | \\bb{z}) )+  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\\right).\\]With some abuse of notation, we will refer to the latterupper bound as our target loss function. Taking the gradient w.r.t. theparameters $\\bb{\\alpha},\\bb{\\beta}$ allows to move the gradient operatorunder the expectation \\[\\nabla _{\\bb{\\alpha},\\bb{\\beta}} L =  \\mathbb{E} _{\\bb{x}}  \\left( \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }( \\nabla _{\\bb{\\alpha},\\bb{\\beta}} (-\\log  p _{\\bb{\\beta}}(\\bb{x} | \\bb{z}) ))+ \\nabla _{\\bb{\\alpha},\\bb{\\beta}}  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\\right).\\]Using stochastic gradient, we can sample a single value$\\bb{x}$ of $\\bb{X}$ from the training set, sample a single value$\\bb{z}$ of $\\bb{Z}$ from the distribution$q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})$, and compute the gradient of a pointloss term \\[\\begin{aligned}\\ell &amp;=&amp; -\\log  p _{\\bb{\\beta}}(\\bb{x} | \\bb{z}) + \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{z} | \\bb{x})\\,\\left\\|\\, p(\\bb{z} )\\right.\\right) \\\\&amp;=&amp; \\frac{1}{2\\sigma^2}\\| \\bb{x}- \\Psi _{\\bb{\\beta}}(\\bb{z}) \\| _2^2 + \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{z} | \\bb{x})\\,\\left\\|\\, p(\\bb{z} )\\right.\\right).\\end{aligned}\\]First, we observe that the second term of the loss has a simpleclosed-form expression of the KL divergence between Gaussiandistributions, \\[\\begin{aligned}\\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{z} | \\bb{x})\\,\\left\\|\\, p(\\bb{z} )\\right.\\right) &amp;=&amp;  \\mathcal{D} _{\\mathrm{KL}}\\left(\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) )\\,\\left\\|\\, \\mathcal{N}(\\bb{0},\\bb{I})\\right.\\right) \\\\&amp;=&amp; \\frac{1}{2} \\left( \\mathrm{tr}\\,\\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) +  \\|\\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})\\|^2 _2 - k - \\log\\det \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x})  \\right),\\end{aligned}\\]where $k$ denotes the dimension of the latent space. This term can beviewed as a regularization added to the loss. The first term is trickier. Note that in the original loss function itdepended both on $\\bb{\\beta}$ (parametrizing $p(\\bb{x} | \\bb{z})$) aswell as $\\bb{\\alpha}$ (since $\\bb{z}$ was drawn from $q _{\\bb{\\alpha}}$);the dependence on $\\bb{\\alpha}$ somehow evaporated when computing thegradient. The network works fine as long as the output is averaged overmany samples, producing a correct expected value; however, whencomputing the gradient, we need to backpropagate through a layer thatsamples $\\bb{z}$ from $q _{\\bb{\\alpha}}$ which is not even continuous andthus has no gradient. While being able to handle stochastic inputs,stochastic gradient cannot handle stochastic operations within thenetwork. To overcome this problem, the sampling is moved to the inputlayer. Given the mean and the covariance,$\\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x})$ of$q _{\\bb{\\alpha}}$, we can sample from$\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) )$by first sampling $\\bb{u}$ from $\\mathcal{N}(\\bb{0},\\bb{I})$(independent of $\\bb{\\alpha}$) and then computing the deterministictransformation$\\bb{z} = \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}) + \\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}$.After this reparametrization trick, the loss term becomes the followingtractable expression: \\[\\mathbb{E} _{\\bb{u} \\sim \\mathcal{N}(\\bb{0},\\bb{I})} \\, \\frac{1}{2\\sigma^2}\\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  + \\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2.\\]To summarize, the VAE is trained by minimizing the following point lossfunction using stochastic gradient descent: \\[\\ell = \\frac{1}{\\sigma^2}\\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  + \\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2 +  \\mathrm{tr}\\,\\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) +  \\|\\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})\\|^2 _2 - k - \\log\\det \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x})\\]with $\\bb{u}$ drawn from the normal distribution and $\\bb{x}$ is drawnfrom the training data. The first term can be thought of as a datafitting term like in regression, demanding that the encoder-decodercombination is nearly an identity map. The second term appliesregularization on the output of the encoder in the latent space. Thehyper-parameter $\\sigma^2$ governs the relative strength of the twoterms. The smaller is $\\sigma^2$, the less randomness is allowed in thedecoder mapping from $\\mathcal{Z}$ to $\\mathcal{X}$, and, consequently,the regression term dominates over the regularization term. Generative adversarial networks One of the consequences of using the $\\ell _2$ loss in the training of anautoencoder is the so-called regression to the mean problem, whichexplains why generative models trained with this loss tend to produceblurred results. An powerful alternative consists of allowing the lossfunction to train together with the generative model in an adversarialmanner. Let $\\Psi _{\\bb{\\beta}} : \\mathcal{Z} \\rightarrow \\mathcal{X}$ bea generative model (decoder network) taking an input$\\bb{z} \\in p(\\bb{Z})$ and mapping it to the space of instances. As theresult, the generated instances admit a distribution$p _{\\bb{\\beta}}(\\bb{X})$, which might be different from the true datadistribution $p(\\bb{X})$. We also define another network$\\Delta _{\\bb{\\theta}} : \\mathcal{X} \\rightarrow [0,1]$ taking aninstance and returning the probability that it is coming from the datarather than from $p _{\\bb{\\beta}}(\\bb{X})$. The discriminator$\\Delta _{\\bb{\\theta}}$ is trained to maximize the correct label assignedto both the real data coming from $p(\\bb{X})$ and generated data comingfrom $p _{\\bb{\\beta}}(\\bb{X})$, i.e., it should distinguish as clearly aspossible between instances coming from the real data($\\Delta _{\\bb{\\theta}} \\approx 1$) and the “fake” generated distribution(ideally, $\\Delta _{\\bb{\\theta}} \\approx 0$). The generator$\\Psi _{\\bb{\\beta}}$ is simultaneously trained to minimize$\\log (1-\\Delta _{\\bb{\\theta}}(\\Psi _{\\bb{\\beta}} (\\bb{z}) ))$, that is,to “fool” the discriminator and cause it to misclassify as big afraction of generated instances as possible. The training can beexpressed as the following two-player min-max game: \\[\\min _{\\bb{\\beta}} \\max _{\\bb{\\theta}} \\, \\mathbb{E} _{\\bb{x} \\sim p(\\bb{X}) } \\log \\Delta _{\\bb{\\theta}}(\\bb{x})  \\, + \\,  \\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (1-\\Delta _{\\bb{\\theta}}(\\Psi _{\\bb{\\beta}} (\\bb{z}) )).\\]The idea of adversarial training can be applied to training AEs and VAEsas well. We can interpret the maximum, \\[L({\\bb{\\beta}}) = \\max _{\\bb{\\theta}} \\, \\mathbb{E} _{\\bb{x} \\sim p(\\bb{X}) } \\log \\Delta _{\\bb{\\theta}}(\\bb{x})  \\, + \\,  \\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (1-\\Delta _{\\bb{\\theta}}(\\Psi _{\\bb{\\beta}} (\\bb{z}) )),\\]as the loss function minimized during the training of the generator.However, note that for every choice of its parameters, we will beminimizing a different loss, since $\\bb{\\theta}$ will change as well.             Let $\\mathcal{X}$ and $\\mathcal{Y}$ be some spaces equipped withappropriate inner products. Let$\\mathcal{A} : \\mathcal{X} \\rightarrow \\mathcal{Y}$ and$\\mathcal{B} : \\mathcal{Y} \\rightarrow \\mathcal{X}$ two operators.The operator $\\mathcal{B}$ is called the adjoint of $\\mathcal{A}$,denoted as $\\mathcal{B} = \\mathcal{A}^\\ast$, if for every$\\bb{x} \\in \\mathcal{X}$ and $\\bb{y} \\in \\mathcal{Y}$,$\\langle \\mathcal{A} \\bb{x} , \\bb{y} \\rangle _{\\mathcal{Y}}  = \\langle \\bb{x}, \\mathcal{B} \\bb{y}  \\rangle _{\\mathcal{X}}$.Though $\\mathcal{A}^\\ast$ is a map from the co-domain of$\\mathcal{A}$ to its domain, it is not an inverse of $\\mathcal{A}$,which may not even be invertible! The adjoint matches the inverseonly in the case of unitary operators (a generalized notion ofrotation). &#8617;               In this notation, we assumed the window to be causal, which is atypical choice for time signals. For images, the window is typicallysymmetric about $i$. &#8617;               The KL divergence is an (asymmetric) distance betweendistributions defined as\\(\\mathcal{D} _{\\mathrm{KL}}\\left((\\,\\left\\|\\, Q\\right.\\right)||P) = \\mathbb{E} _{z \\sim Q}( \\log Q(z) - \\log P(z)).\\) &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_06/",
        "teaser":null},{
        "title": "Lecture 7: Reinforcement learning",
        "excerpt":"Until now, we have seen two learning regimes: the supervised regime,in which the learning system attempts to learn a latent map based onexample of its input-output pairs, and the unsupervised regime, inwhich the learning system attempts to build a model for the datadistribution. In what follows, we will consider another learningsetting, in which a decision-making system is trained to make optimaldecisions. The basic setting of our problem will be that of an agent acting in anenvironment. At every point of time, the agent observes the state ofthe environment and decides on an action that changes that state. Foreach such action, the agent is given a reward signal (which can benegative). The agent’s role is to maximize the total received reward. Markov decison processes Let us now formalize the above setting. Time $t$ is assumed to bediscrete incremented in steps of $1$. We assume that at every time, theenvironment can be found in one of a finite set of states$s _t \\in \\mathcal{S}$. The state of the environment will be furtherassumed fully observable by the agent. At every time, the agent maytake one of a finite set of actions $a _t \\in \\mathcal{A}$. As the resultof the agent’s action, the environment will transition to a new state$s _{t+1}$ at the next time. The transition rule is, generally,stochastic and can be characterized by the transition probability,which is the probability$\\mathbb{P}(s _{t+1} | s _{t},a _{t}, s _{t-1},a _{t-1},\\dots,s _0,a _0)$ ofthe future state conditional on the present and past states and actions.We assume the random process underlying such transitions to obey theMarkov property implying that the conditional probability of thefuture state depends only on the present state and action, \\[\\mathbb{P}(s _{t+1} | s _{t},a _{t}, s _{t-1},a _{t-1},\\dots,s _0,a _0) = \\mathbb{P}(s _{t+1} | s _{t},a _{t}).\\]In other words, the effect of an action depends only on the presentstate and not the past history. Furthermore, we assume the transitionprobability to be time-invariant (which does not imply processstationarity!) In view of these properties, we will denote by$P _a(s,s’) = \\mathbb{P}(s _{t+1}=s’ | s _t=s, a _t = a)$ the transitionprobability from state $s$ to state $s’$ under the action $a$. As theresult of the transition, the agent receives a scalar immediate reward$r _{t+1} = R(s _{t},a _t)$, which is assumed deterministic (or theexpectation of a stochastic reward). In order to quantify the return (or the total reward) that an agentwill receive, we are tempted to sum the immediate rewards in time.However, this will generally yield an infinite sum. A way to overcomethis is by setting a finite horizon, summing only for a finite set oftime steps into the future: \\[g _t = \\sum _{k = 0}^{n-1} r _{t+1+k},\\]Asmoothed version of a finite horizon reward is known as the cumulativediscounted reward \\[g _t = \\sum _{k \\ge 0} \\gamma^k \\, r _{t+1+k},\\]where$\\gamma \\in [0,1)$ is a discount factor giving lower importance toremote future rewards (vita brevis est). Due to its tractability, thisis a very popular choice for modelling the return. The tuple $(\\mathcal{S},\\mathcal{A},P,R,\\gamma)$ is known as a Markovdecision process (MDP) and can be thought as the set of game rules bywhich the agent is obliged to play. Usually, the state set $\\mathcal{S}$will contain a particular terminal state (or few such states)indicating the end of the game (e.g., the agent has died or won thegame). In such cases, the state-action-reward sequence will terminate atsome point, producing a single game episode \\[s _0,a _0,r _1, \\,\\, s _1,a _1,r _2, \\,\\, \\dots, \\,\\, s _{t-1},a _{t-1},r _{t},\\,\\, s _t.\\]A sub-sequence representing a single state transition and thecorresponding reward, $s _{t},a _{t},r _{t+1},s _{t+1}$ is usually referredto as an experience. Policy How does the agent know which action to take? The behavior of the agentis fully defined by the conditional distribution \\[\\pi(a|s) = \\mathbb{P}(a _t = a | s _t = s)\\]known as a policy. Thisformalism captures both stochastic and deterministic policies (in thelatter case, $a _t = f(s _t)$ and the above conditional distributionbecomes a singletone). Value functions Given an MDP and having the agent behavior fixed to some policy $\\pi$,we may predict how beneficial it is for the environment to be in acertain state, or for the agent to take a certain action in a particularstate. This benefit (=return) is measured by value functions, which,of course, depend on the selected policy. The state value function of an MDP is the expected return of the agentstarting at state $s _t=s$ and following the policy $\\pi$ at allsubsequent time steps, \\[v _\\pi(s) = \\mathbb{E}\\left( g _t  \\left| s _t = s, \\pi \\right.\\right) = \\mathbb{E}\\left( \\sum _{t \\ge 0} \\gamma^t \\, r _{t+1}  \\left| s _0 = s,\\pi \\right.\\right).\\]Note that since our MDP is time-invariant, the exact value of startingtime $t$ is unimportant. The action value function of an MDP is the expected return of theagent starting at state $s _t=s$, taking action $a _t=a$, and thenfollowing the policy $\\pi$ at all subsequent time steps, \\[q _\\pi(s,a) = \\mathbb{E}\\left( g _t  \\left| s _t = s, a _t = a \\pi \\right.\\right) = \\mathbb{E}\\left( \\sum _{t \\ge 0} \\gamma^t \\, r _{t+1}  \\left| s _0 = s, a _0 = a, \\pi \\right.\\right).\\]Expectation equations Let us have an explicit look at the state value function \\[\\begin{aligned}v _\\pi(s) &amp;=&amp;  \\mathbb{E}\\left( g _0  \\left| s _0 = s, \\pi \\right.\\right) \\\\&amp;=&amp; \\mathbb{E}\\left( r _1 + \\gamma r _2 + \\gamma^2 r _3 + \\cdots \\left| s _0 = s, \\pi \\right.\\right) \\\\&amp;=&amp; \\mathbb{E}\\left( r _1 + \\gamma ( r _2 + \\gamma r _3 + \\cdots  ) \\left| s _0 = s, \\pi \\right.\\right) \\\\&amp;=&amp; \\mathbb{E}\\left( r _1 + \\gamma g _{t+1} \\left| s _0 = s, \\pi \\right.\\right) \\\\&amp;=&amp; \\mathbb{E}\\left( r _1 + \\gamma v _\\pi(s _{t+1}) \\left| s _0 = s, \\pi \\right.\\right).\\end{aligned}\\]Note that the function under the expectation decomposes into two terms:the immediate reward $r _{t+1}$ and the discounted value of the successorstate reward $v _\\pi(s _{t+1})$. Spelling out the expectation, we obtain \\[\\begin{aligned}v _\\pi(s) &amp;=&amp; \\sum _{a \\in \\mathcal{A}} \\pi(a|s) \\left(  R(s,a) + \\gamma \\sum _{s' \\in \\mathcal{S}} P(s'|s,a) v _\\pi(s')  \\right) \\\\&amp;=&amp; \\sum _{a \\in \\mathcal{A}}   R(s,a) \\pi(a|s) + \\gamma \\sum _{s' \\in \\mathcal{S}} \\left( \\sum _{a \\in \\mathcal{A}} P(s'|s,a)\\pi(a|s) \\right) v _\\pi(s')\\end{aligned}\\]Expressing $v _\\pi(s)$ over all states $s \\in \\mathcal{S}$ as a vector$\\bb{v} _\\pi$, the first term in the right-hand-side as a vector$\\bb{r} _\\pi$, and the parenthesis in the second term as the matrix$\\bb{P} _\\pi$, we obtain the linear system \\[\\bb{v} _\\pi = \\bb{r} _\\pi + \\gamma \\bb{P} _\\pi \\bb{v} _\\pi,\\]for which aclosed-form solution$\\bb{v} _\\pi = (\\bb{I} -  \\gamma \\bb{P} _\\pi)^{-1} \\bb{r} _\\pi$ isavailable. In the same way, the action value function can be decomposed into \\[\\begin{aligned}q _\\pi(s,a) &amp;=&amp;  \\mathbb{E}\\left( g _0  \\left| s _0 = s, a _0 = 0 \\pi \\right.\\right) \\\\&amp;=&amp; \\mathbb{E}\\left( r _1 + \\gamma r _2 + \\gamma^2 r _3 + \\cdots \\left| s _0 = s, a _0 = a,\\pi \\right.\\right) \\\\&amp;=&amp; \\mathbb{E}\\left( r _1 + \\gamma Q _\\pi(s _{t+1},a _{t+1}) \\left| s _0 = s, a _0 = a,\\pi \\right.\\right) \\\\&amp;=&amp; R(s,a) + \\gamma   \\sum _{s' \\in \\mathcal{S}} P(s'|s,a) \\sum _{a' \\in \\mathcal{A}} \\pi(a'|s') q _\\pi(s',a').\\end{aligned}\\]The state and the action value functions are related to each other via \\[v _\\pi(s) = \\sum _{a \\in \\mathcal{A}} \\pi(a|s) q _\\pi(s,a).\\]Optimal control Both value functions predict future reward. Starting at some initialstate $s _0 = s$ (and, perhaps, some initial action $a _0 = a$) andrunning the game forward in time following a policy $\\pi$, the MDP willrealize a certain trajectory$\\tau = { (s _{t},a _{t},r _{t+1}) } _{t \\ge 0}$ (since it is a stochasticprocess, every game will realize a different trajectory). Each suchtrajectory has a certain probability of being realized, and can beassociated with the return \\[g : \\tau \\mapsto \\sum _{t \\ge 0} \\gamma^t \\, r _{t+1}.\\]Both valuefunctions average the latter quantity over all possible trajectoriesstarting at a state $s$ in the case of $v _\\pi(s)$, or a state-actionpair $(s,a)$ in the case of $q _\\pi(s,a)$. Our desire to maximize thereturn can be translated into an optimal control problem, which can beinformally stated as selecting such a policy making high-returntrajectory more probable. Bellman equation Let us define the so-called Bellman operator $T$ mapping a state valuefunction $u : \\mathcal{S} \\rightarrow \\RR$ to a new state value function \\[(Tu)(s) =  \\max _{a \\in \\mathcal{A}} \\, R(s,a) + \\gamma \\sum _{s' \\in \\mathcal{S}} P(s'|s,a) u(s').\\]Given a value function $u$, we construct a determinisitic policy \\[\\pi^\\ast _u (a|s)= \\left\\{ \\begin{array}{cl} 1 &amp; : \\,  a = \\mathrm{arg}\\max _{\\alpha \\in \\mathcal{A}} \\, R(s,\\alpha) + \\gamma \\sum _{s' \\in \\mathcal{S}} P(s'|s,\\alpha) u(s') \\\\0 &amp; : \\, \\mathrm{else}\\end{array}\\right.\\]It is straightforward to show that the state value functionassociated with this policy is exactly given by the application of theBellman operator to $u$, $v _{\\pi^\\ast _u} = Tu$. It can also be shownquite straightforwardly that $Tu \\ge u$ in the sense that$(Tu)(s) \\ge u(s)$ for every $s \\in \\mathcal{S}$. In other words,replacing a policy $\\pi$ with a new policy $\\pi’ = \\pi^\\ast _{v _\\pi}$improves the policy in the sense that$\\pi^\\ast _{v _\\pi’}= Tv _\\pi \\ge v _\\pi$. This monotonicity property together with the fact that the return isupper-bounded (by $\\max _{s,a} R(s,a)/(\\gamma-1)$) implies that $T^n v$produces a convergent sequence, $T^n v \\uparrow v^\\ast$, with the limitpoint being the fixed point of $T$, \\[v^\\ast(s) = (Tv^\\ast)(s) = \\max _{a \\in \\mathcal{A}} \\, R(s,a) + \\gamma \\sum _{s' \\in \\mathcal{S}} P(s'|s,a) v^\\ast(s').\\]An optimal policy (not necessarily unique!) producing the above optimalstate value function is given by \\[\\pi^\\ast(a|s) = \\pi^\\ast _{v^\\ast} = \\left\\{ \\begin{array}{cl} 1 &amp; : \\,  a = \\mathrm{arg}\\max _{\\alpha \\in \\mathcal{A}} \\, R(s,\\alpha) + \\gamma \\sum _{s' \\in \\mathcal{S}} P(s'|s,\\alpha) v^\\ast(s') \\\\0 &amp; : \\, \\mathrm{else}.\\end{array}\\right.\\]The latter resut is known as the Belman equation orBelman’s optimality principle. Informally, it states that an optimalpolicy has the property that whatever is the initial state and initialdecision, the remaining decisions must constitute an optimal policy withregard to the state resulting from the first decision. In a very similar manner, the Bellman equation can be written in termsof the action value function, \\[q^\\ast(s,a) = R(s,a) + \\gamma \\sum _{s' \\in \\mathcal{S}} P(s'|s,a) \\max _{a' \\in \\mathcal{A}} q^\\ast(s',a'),\\]with the associated optimal policy \\[\\pi^\\ast(a|s) = \\left\\{ \\begin{array}{cl} 1 &amp; : \\,  a = \\mathrm{arg}\\max _{a \\in \\mathcal{A}} \\, q^\\ast(s,a) \\\\0 &amp; : \\, \\mathrm{else},\\end{array}\\right.\\]which is identical to the one associated with the optimalstate value function. The two optimal value functions are related via \\[v^\\ast(s) = \\max _{a \\in \\mathcal{A}} q^\\ast(s,a).\\]Dynamic programming Our previous discussion suggests a very simple recipe for finding theoptimal value function (and the corresponding optimal policy): afixed-point iteration of the Bellman operator. We start with anarbitrary value function $q _0$ and produce a sequence of value functions \\[q _{n+1}(s,a) = R(s,a) + \\gamma \\sum _{s' \\in \\mathcal{S}} P(s'|s,a) \\max _{a' \\in \\mathcal{A}} q _n(s',a')\\]for $n\\ge 0$. This sequence converges to $q^\\ast$ as $n$ approachesinfinity. This technique is typically known under the name of dynamicprogramming, backward induction or value iteration. However, despite its apparent simplicity, dynamic programming iscompletely infeasible for problems with a moderately big state space,not mentioning real-world problems with huge dimensionalities of$\\mathcal{S}$. Approximate solutions using learning systems are knownunder the name of reinforcement learning. In what follows, we willexamine several such approaches. Reinforcement learning balances rewardaccumulation and system identification (model learning) in case ofunknown dynamics ($R$ and $P$ of the MDP). The on-line nature ofreinforcement learning makes it possible to approximate optimal policiesin ways that put more effort into learning to make good decisions forfrequently encountered states, at the expense of less effort forinfrequently encountered states. Value-based learning The idea of value-based learning is to learn a parametric function$q _{\\bb{\\theta}}(s,a)$ (realized by a neural network with the parameters$\\bb{\\theta}$) approximating the optimal value function $q^\\ast$. Sincein most implementations the action value function is used, the method isalso known under the name of $q$-learning (when a deep neural networkis used as the approximator, it is also known as deep $q$ network orDQN). In practice, the network is realized as a vector-valued function$\\bb{q} _{\\bb{\\theta}}(s)$ receiving the state $s$ and producing thevalues of the function for every $a \\in \\mathcal{A}$. For example, anagent playing Pacman receives as the state the set of pixels displayedon the screen, and produces the value of the approximate $q$-functionfor all the four control actions (up,down,left,right). Recall that our goal is to find such a vector of parameters$\\bb{\\theta}$ satisfying the Bellman equation \\[q _{\\bb{\\theta}}(s _t,a _t) = r _{t+1} + \\gamma \\sum _{s \\in \\mathcal{S}} P(s|s _t,a _t) \\max _{a \\in \\mathcal{A}} q _{\\bb{\\theta}}(s,a).\\]We will relax the above equality in the least squares sense and definethe loss function \\[L(\\bb{\\theta}) = \\mathbb{E} _{s,a} \\left( y -  q _{\\bb{\\theta}}(s,a) \\right)^2\\]where the expectation is in practice an empirical average on amini-batch of experiences of the form $(s _t,a _t,r _{t+1},s _{t+1})$; forevery such experience, \\[y = \\left\\{ \\begin{array}{ll} r _{t+1} + \\gamma  \\max _{a \\in \\mathcal{A}} q _{\\bb{\\theta}^-}(s _{t+1},a) &amp; : \\, s _{t+1} \\,\\, \\mathrm{not\\,terminal} \\\\r _{t+1} &amp; : \\, s _{t+1} \\,\\, \\mathrm{terminal}. \\end{array}\\right.\\]Here $\\bb{\\theta}^-$ denotes the previous vector ofparameters to emphasize that $y$ is constant w.r.t the optimizationvariable of the loss $L(\\bb{\\theta})$. Note that we do not average thesecond term over $s \\in \\mathcal{S}$, since the average weights$P(s|s _t,a _t)$ are typically unknown (the agent is discovering the rulesof the game, and the MDP is latent at least initially). Since many such$y$’s are averaged over the mini-batches, the weighting by thetransition probabilities arises naturally. Experience replay The most natura way of constructing mini-batches for $q$-learning is bytaking sequences of consecutive samples, updating the network inbetween. However, this is a very bad idea for several reasons. First,the samples are correlated, which makes the learning inefficient.Second, since the current parameters determine the next trainingsamples, the mini-batches are likely to be biased towards specificstates and actions. Such unhealthy feedback loops are avoided by usingthe experience replay methodology. A replay cache of experiences ofthe form $(s _t,a _t,r _{t+1},s _{t+1})$ is constantly updated as the gameis played. Mini-batches are drawn at random from the cache. In a typical learning scenario, entire episodes are playedconsecutively. With the environment currently present at state $s _t$ attime $t$ in the episode, the greedily optimal action \\[a _t = \\mathrm{arg}\\max _{a \\in \\mathcal{A}}  q _{\\bb{\\theta}}(s _t,a)\\]is selected and is executed agains the emulated environment, whichreturns the next state $s _{t+1}$ and the reward $r _{t+1}$. The tuple$(s _t,a _t,r _{t+1},s _{t+1})$ is inserted into the cache. In order to allow the agent to balance the exploration of new states andactions vs. the exploitation of the learned policy, with some smallprobability $\\epsilon \\in (0,1)$, the greedy optimal action is replacedwith a uniformly random action on $a _t \\sim U(\\mathcal{A})$. Policy-based learning While value-based learning is much more tractable than dynamicprogramming (and also allows to implicitly discover the underlying MDP),the approximate $q$-function might still be very complicated in realsettings. Often, the policy itself is a much simpler function.Policy-based learning methods learn a policy $\\pi _{\\bb{\\theta}}(a|s)$from some parametric family of functions (for deterministic policies,the network has the form $a = \\pi _{\\bb{\\theta}}(s)$, receiving a stateand producing an action $a$). A natural score function to associate with a policy $\\pi _{\\bb{\\theta}}$is the expected return \\[J( \\bb{\\theta} )  =  \\mathbb{E} \\left(  g(\\tau) | \\pi _{\\bb{\\theta}}  \\right) =  \\mathbb{E} \\left(  \\sum _{t \\ge 0} \\gamma^t \\, r _{t+1} | \\pi _{\\bb{\\theta}}  \\right),\\]where the expectation is taken over all trajectories$\\tau = { (s _{t},a _{t},r _{t+1}) } _{t \\ge 0}$ realizable under thepolicy $\\pi _{\\bb{\\theta}}$ with the probability distribution \\[P(\\tau | \\bb{\\theta}) = \\mathbb{P}(  \\{ (s _{t},a _{t},r _{t+1}) \\} _{t \\ge 0} | \\bb{\\theta}) = \\mathbb{P}(s _0) \\prod _{t \\ge 0} \\pi _{\\bb{\\theta}}(a _t|s _t) P(s _{t+1} | s _t,a _t).\\]In these terms, we can re-write the objective as \\[J( \\bb{\\theta} )  = \\mathbb{E} _{\\tau \\sim P(\\tau | \\bb{\\theta}) }  \\,  g(\\tau)  = \\int P(\\tau | \\bb{\\theta}) g(\\tau) d\\tau.\\]Taking the gradient w.r.t. the network parameters results in \\[\\begin{aligned}\\nabla _{\\bb{\\theta} } J( \\bb{\\theta} )  &amp;=&amp; \\int \\nabla _{\\bb{\\theta} } P(\\tau | \\bb{\\theta}) g(\\tau) d\\tau = \\int P(\\tau | \\bb{\\theta})  \\frac{\\nabla _{\\bb{\\theta} } P(\\tau | \\bb{\\theta})}{P(\\tau | \\bb{\\theta}) } g(\\tau) d\\tau\\\\&amp;  = &amp;\\int P(\\tau | \\bb{\\theta})  \\nabla _{\\bb{\\theta} } \\log P(\\tau | \\bb{\\theta})  g(\\tau) d\\tau = \\mathbb{E} _{\\tau \\sim P(\\tau | \\bb{\\theta}) }  \\left( \\nabla _{\\bb{\\theta} } \\log P(\\tau | \\bb{\\theta})  g(\\tau) \\right).\\end{aligned}\\]The latter trick allows to write the seemingly intractable gradient ofthe expectation as an expectation of the gradient of the log conditionaldensity $P(\\tau | \\bb{\\theta})$. Let us now evaluate the latter gradientexplicitly. By observing that in the expression \\[\\begin{aligned}\\log P(\\tau | \\bb{\\theta})  &amp;=&amp; \\log \\mathbb{P}(s _0) + \\sum _{t \\ge 0}\\pi _{\\bb{\\theta}}(a _t|s _t) + \\sum _{t \\ge 0} P(s _{t+1} | s _t,a _t)\\end{aligned}\\]only the second term depends on $\\bb{\\theta}$, we can write \\[\\begin{aligned} \\nabla _{\\bb{\\theta} } \\log P(\\tau | \\bb{\\theta})  &amp;=&amp; \\sum _{t \\ge 0}  \\nabla _{\\bb{\\theta} }\\log \\pi _{\\bb{\\theta}}(a _t|s _t). \\end{aligned}\\]The gradient of the score function reduces to \\[\\begin{aligned}\\nabla _{\\bb{\\theta} } J( \\bb{\\theta} )  &amp;=&amp; \\mathbb{E} _{\\tau  }  \\left( g(\\tau)  \\sum _{t \\ge 0}  \\nabla _{\\bb{\\theta} }\\log \\pi _{\\bb{\\theta}}(a _t|s _t) \\right).\\end{aligned}\\]When working with stochastic gradient, it further simplifies to \\[\\begin{aligned}\\nabla _{\\bb{\\theta} } J( \\bb{\\theta} )  &amp; \\approx &amp;   \\sum _{t \\ge 0}  g(\\tau)  \\nabla _{\\bb{\\theta} }\\log \\pi _{\\bb{\\theta}}(a _t|s _t).\\end{aligned}\\]Making gradient ascent steps with the gradient of this form implies thatfor high-return trajectories, the probability of all incurred actionsshall be increase, while for low-return trajectories, they should bedecreased. A slightly less drastic approach would be to increase the probabilitiesof of an action encountered only by the cumulative discounted futurereward from that state on, \\[\\begin{aligned}\\nabla _{\\bb{\\theta} } J( \\bb{\\theta} )  &amp;\\approx&amp;   \\sum _{t \\ge 0}  \\left( \\sum _{t' \\ge t}  \\gamma^{t'-t} r _{t'}  \\right)  \\nabla _{\\bb{\\theta} }\\log \\pi _{\\bb{\\theta}}(a _t|s _t),\\end{aligned}\\]thus localizing the effect of an action in time. A problem that stillpersists is that the absolute value of the reward is of little meaningin the decision whether to increase or decrease the probability of acertain action; what matters more is whether the action increases thereward already expected in that state. Formally, this can be embodied bysubtracting a baseline $b(s _t)$ \\[\\begin{aligned}\\nabla _{\\bb{\\theta} } J( \\bb{\\theta} )  &amp;\\approx&amp;   \\sum _{t \\ge 0}  \\left( \\sum _{t' \\ge t}  \\gamma^{t'-t} r _{t'}  - b(s _t) \\right)  \\nabla _{\\bb{\\theta} }\\log \\pi _{\\bb{\\theta}}(a _t|s _t),\\end{aligned}\\]which can be, for example, a moving average of the rewards previouslyobserved from state $s _t$. Actor-critic architecture The desire to weight the gradient of $\\log \\pi _{\\bb{\\theta}}(a _t|s _t)$by the difference between the expected future return if action $a _t$ istaken and that expected by all actions taken from state $s _t$ suggeststhat the weighing should be performed by the difference between theaction value function and the state value function, \\[\\begin{aligned}\\nabla _{\\bb{\\theta} } J( \\bb{\\theta} )  &amp;\\approx&amp;   \\sum _{t \\ge 0}  \\left( q _{\\pi _{\\bb{\\theta}}}(s _t,a _t) -  v _{\\pi _{\\bb{\\theta}}}(s _t) \\right)  \\nabla _{\\bb{\\theta} }\\log \\pi _{\\bb{\\theta}}(a _t|s _t),\\end{aligned}\\]Since we do not known the value functions, we can estimate them (or,actually, their difference known as the advantage function$a _{\\pi _{\\bb{\\theta}}}(s _t,a _t)  = q _{\\pi _{\\bb{\\theta}}}(s _t,a _t) -  v _{\\pi _{\\bb{\\theta}}}(s _t)$) using a neural network as we did in value-based learning. To that end,we define another neural network $a _{\\bb{\\phi}}(s,a)$ parametrized by${\\bb{\\phi}}$ aiming at estimating the advantage function$a _{\\pi _{\\bb{\\theta}}}(s _t,a _t)$ and train it simultaenously with thepolicy $\\pi _{\\bb{\\theta}}$. This approach combining value- andpolicy-based learning is known as actor-critic architecture, since theactor decides which action to take (the policy $\\pi _{\\bb{\\theta}}$) andthe critic tells it how beneficial the action was (the value function$a _{\\bb{\\phi}}$). Based on the latter, the actor knows how to adjust itspolicy. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_07/",
        "teaser":null},{
        "title": "Lecture 11: Learning on Non-Euclidean Domains",
        "excerpt":"All learning settings we have encountered thus far had a common(sometimes, tacit) property: they assumed Euclidean geometry of thedata. For example, we could compute standard inner products, subtractone vector from another, apply matrices to vectors, etc. Data like timesignal and images were further discretized on regular Cartesian grids,and we could apply operations like convolution by simply sliding thesame window over the signal and computing inner products. Most of these apparently straightforward notions become lessstraightforward when the domain underlying the data is no longerEuclidean. Such kinds of data arise in a long list of applications. Forexample, in social networks, user information can be modeled as signalson a graph. Sensor networks are also modeled as graphs of distributedinterconnected sensors, whose readings are time-dependent signals on thegraph. In neuroscience, graph models are used to represent anatomicaland functional structures of the brain. In biology, graphs are a commonway to express interactions between genes, proteins, etc. In computergraphics and vision, three-dimensional geometric objects are oftenrepresented as Riemannian manifolds (surfaces) endowed with attributessuch as color texture. It is important to distinguish between two very distinct tasks: learningon non-Euclidean domains vs. learning of non-Euclidean domains. Weencountered the latter problem when discussing unsupervised learning,where our goal was to discover (learn) the latent manifold from whichthe data are sampled. The former problem of analyzing signals on graphsand manifolds is what is going to occupy us in this lecture. In whatfollows, we wil briefly review the main properties of the basicingredients of CNNs on Euclidean domain, specifically, the convolutionoperator and pooling. We will then describe how to generalize thesenotions to graphs. Convolution on Euclidean domains The main ingredient of a CNN is a convolutional layer, describing amapping between $m$-dimensional input signals to $n$-dimensional outputsignals (we use the term signal to generalize the notion of asequence, allowing its elements to be indexed by a $d$-dimensionalmulti-index; this notion includes images and higher-dimensional signalsbesides time series). Formally, a convolutional layer accepts an$m$-dimensional vector-valued (infinitely supported) signal$\\bb{x} = (\\bb{x}^1,\\dots, \\bb{x}^m) = \\{ (x _{\\bb{k}}^1,\\dots, x _{\\bb{k}}^m) \\} _{ {\\bb{k}} \\in \\mathbb{Z}^d}$,each input dimension of which is called a channel or feature map.The layer produces an $n$-dimensional (infinitely supported) signal$\\bb{y} = (\\bb{y}^1,\\dots, \\bb{y}^n)  = \\{ (y _{\\bb{k}}^1,\\dots, y _{\\bb{k}}^n) \\} _{ {\\bb{k}} \\in \\mathbb{Z}^d}$by applying a bank of filters, \\[\\bb{y}^j = \\varphi \\left( \\sum _{i=1}^m \\bb{w}^{ij} \\ast \\bb{x}^{i} + b _j   \\right) ,\\]Explicitly, the action of the convolution$\\bb{z}^j = \\bb{w}^{ij} \\ast \\bb{x}^i$ can be written as \\[z^j _{\\bb{k}} =    \\sum _{i=1}^m \\sum _{\\bb{p} \\in \\mathbb{Z}^d } w^{ij} _{\\bb{p}} x^i _{\\bb{k}-\\bb{p}}.\\]Note the $d$-dimensional multi-indices in the sum. Eigenvectors of Toeplitz operators Since the convolution operation is the main ingredient of a CNN, let usdedicate some attention to listing a few of its properties that will beinstrumental in the generalization of CNNs to non-Euclidean domains. Aswe have already seen, any linear shift-invariant1 (Toeplitz) operator$\\mathcal{W}$ can be represented as the convolution \\[\\mathcal{W}\\bb{x} = \\bb{x} \\ast \\bb{w}.\\]The action of any linearoperator on a vector consists of scaling and rotating the vector.However, there are some privileged directions where no rotation occurs;such directions are called the eigenvectors of the operator.Specifically, for Toeplitz operators, given the input signal \\[\\bb{\\phi}^{\\bb{\\xi}} _{\\bb{n}} = e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{n}}\\]parametrized by the vector $\\bb{\\xi} \\in [0,1]^d$, the output of theoperator is \\[(\\bb{w} \\ast \\bb{\\phi}^{\\xi}) _{\\bb{n}} = \\sum _{ {\\bb{k}}}  w _{\\bb{n}} {\\phi}^{\\bb{\\xi}} _{ {\\bb{n}}-{\\bb{k}}} =  \\sum _{ {\\bb{k}}}  w _{\\bb{k}} e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr (\\bb{n} - \\bb{k})} =  e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{n} } \\sum _{ {\\bb{k}}}  w _{\\bb{k}} e^{-i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{k}}.\\]Recalling the standard inner product on $\\ell^2$, \\[\\langle  \\bb{x}, \\bb{y} \\rangle = \\sum _{\\bb{k}} x _{\\bb{k}} y^\\ast _{\\bb{k}},\\]we can express \\[\\sum _{ {\\bb{k}}}  w _{\\bb{k}} e^{-i\\, 2\\pi \\bb{\\xi} ^\\Tr  \\bb{k} } =  \\sum _{ {\\bb{k}}}  w _{\\bb{k}}  {\\phi}^{\\bb{\\xi}} _{-k}  =  \\sum _{ {\\bb{k}}}  w _{\\bb{k}}  \\left({\\phi}^{ {\\bb{\\xi}}} _{\\bb{k}} \\right)^\\ast = \\langle  \\bb{w}, \\bb{\\phi}^{\\bb{ {\\bb{\\xi}}}} \\rangle = \\hat{\\bb{w}}({\\bb{\\xi}}).\\]In these terms, the output is given by \\[\\bb{w} \\ast \\bb{\\phi}^{\\bb{\\xi}} = \\hat{\\bb{w}}(\\xi) \\bb{\\phi}^{\\bb{\\xi}},\\]which means that $\\bb{\\phi}^{\\bb{\\xi}}$ is an eigenvector of$\\mathcal{W}$ with the corresponding eigenvalues$\\hat{\\bb{w}}(\\bb{\\xi})$. Note that while the eigenvalues depend onthe specific operator (embodied in the sequence $\\bb{w}$ called thekernel of the operator), the eigenvectors are always the same:$\\{\\bb{\\phi}^{\\bb{\\xi}}\\} _{\\bb{\\xi} \\in [0,1]^d}$. Fourier transform The function \\[\\hat{\\bb{w}}(\\bb{\\xi}) = \\langle  \\bb{w}, \\bb{\\phi}^{\\xi} \\rangle _{\\ell^2(\\mathbb{Z}^d)} = \\sum _{\\bb{k}}  w _{\\bb{k}} e^{-i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{k} }\\]is called the (forward) Fourier transform of the sequence $\\bb{w}$. Itis customary to define the operator$\\mathcal{F} : \\ell^2(\\mathbb{Z}^d) \\rightarrow L^2([0,1]^d)$ mapping$\\bb{w}$ to $\\hat{\\bb{w}}$, and refer to the argument $\\bb{\\xi}$ of thelatter as to frequency. The inverse map$\\mathcal{F}^{-1} : L^2([0,1]^d) \\rightarrow \\ell^2(\\mathbb{Z}^d)$,called the inverse Fourier transform, is given by \\[\\bb{w} = \\mathcal{F}^{-1} \\hat{\\bb{w} } = \\int _{[0,1]^d} \\hat{\\bb{w}}(\\bb{\\xi})  \\bb{\\phi}^{\\bb{\\xi}} d\\bb{\\xi} = \\int _{[0,1]^d} \\hat{\\bb{w}}(\\bb{\\xi})  e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{n}} d\\bb{\\xi}.\\]To prove this, observe that \\[\\begin{aligned} \\int _{[0,1]^d} \\hat{\\bb{w}}(\\bb{\\xi})  e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{n}} d\\bb{\\xi} &amp;=&amp;    \\int _{[0,1]^d}   \\left( \\sum _{\\bb{k}}  w _{\\bb{k}} e^{-i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{k}}  \\right)  e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr\\bb{n}}  d\\bb{\\xi} \\\\ &amp;=&amp; \\sum _{\\bb{k}}  w _{\\bb{k}} \\left(   \\int _{[0,1]^d}  e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr (\\bb{n}-\\bb{k})} d\\bb{\\xi} \\right).\\end{aligned}\\]Since the complex exponentials$e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr (\\bb{n}- \\bb{k})}$ have an integer number ofperiods on the domain $[0,1]^d$, the latter integral is zero unless$\\bb{n}-\\bb{k} = \\bb{0}$, in which case it is exactly $1$. Hence, \\[\\int _{[0,1]^d} \\hat{\\bb{w}}(\\bb{\\xi})  e^{i\\, 2\\pi \\bb{\\xi} ^\\Tr \\bb{n}} d\\bb{\\xi} = \\sum _{\\bb{k}}  w _{\\bb{k}} \\delta _{\\bb{n}-\\bb{k}} =  w _{\\bb{n}}.\\]Note that the inverse Fourier transform can also be written as \\[(\\mathcal{F}^{-1} \\hat{\\bb{w}} ) _{\\bb{n}}  = \\langle  \\hat{\\bb{w}},  (\\bb{\\phi}^{\\bb{n}})^\\ast  \\rangle _{L^2(\\mathbb{Z}^d)},\\]which emphasizes the above orthonormality property by essentiallystating that $\\mathcal{F}^{-1} = \\mathcal{F}^\\ast$, where$\\mathcal{F}^\\ast$ denotes the adjoint operator. Geometrically, thismeans that the Fourier transform is unitary, a generalized form ofrotation. The fact that rotations preserve distances leads to thecelebrate Plancherel identity \\[\\langle  \\bb{f}, \\bb{g} \\rangle _{\\ell^2(\\mathbb{Z}^d)} = \\langle  \\hat{\\bb{f}}, \\hat{\\bb{g}} \\rangle _{L^2([0,1]^d)},\\]and, in the particular case of $\\bb{g} = \\bb{f}$, even more celebrateParseval’s identity \\[\\|  \\bb{f} \\| _{\\ell^2(\\mathbb{Z}^d)} = \\| \\hat{\\bb{f}} \\| _{L^2([0,1]^d)}.\\]We can think of the Fourier transform as the transformation of a signal$\\bb{x}$ to the (joint) eigenbasis of (all) Toeplitz operators, that is,describing $\\bb{x}$ as a linear combination of the eigenvectors$\\bb{\\phi}^{\\bb{\\xi}}$, \\[\\bb{x} = \\int _{-\\pi}^\\pi \\hat{\\bb{x}}(\\bb{\\xi})  \\bb{\\phi}^{\\bb{\\xi}} d\\bb{\\xi},\\]with $\\hat{\\bb{x}}(\\bb{\\xi})$ serving as the coordinates in theeigenbasis. In the signal processing parlance, $\\bb{x}$ in the standardbasis is called the impulse response, while in the Fourier basis it isreferred to as the frequency response. The act of converting theimpules response to the frequency response is referred to as analysis,while the inverse is referred to as synthesis. The notion of the Fourier transform allows to apply the operator$\\mathcal{W}$ in its eigenbasis. Indeed, describing the input sequenceas a linear combination of the eigenvectors, \\[\\bb{x} = \\int _{-\\pi}^\\pi \\hat{\\bb{x}}(\\xi)  \\bb{\\phi}^{\\bb{\\xi}} d\\bb{\\xi},\\]we obtain \\[\\bb{w} \\ast \\bb{x} = \\mathcal{W}  \\int _{[0,1]^d} \\hat{\\bb{x}}(\\bb{\\xi})  \\bb{\\phi}^{\\bb{\\xi}} d\\bb{\\xi} =   \\int _{[0,1]^d} \\hat{\\bb{x}}(\\bb{\\xi}) \\mathcal{W} \\bb{\\phi}^{\\bb{\\xi}} d\\bb{\\xi} =  \\int _{[0,1]^d} \\hat{\\bb{w}}(\\bb{\\xi}) \\hat{\\bb{x}}(\\bb{\\xi}) \\bb{\\phi}^{\\bb{\\xi}} d\\bb{\\xi}.\\]The latter can be written as \\[\\bb{w} \\ast \\bb{x} = \\mathcal{F}^{1-} ( \\mathcal{F}\\bb{w}  \\cdot \\mathcal{F} \\bb{x} )\\]or, schematically, \\[\\bb{w} \\ast \\bb{x}  \\mathop{\\longleftrightarrow}^{\\mathcal{F}} \\hat{\\bb{w}} \\cdot \\hat{\\bb{x}}.\\]This result is known as the convolution theorem, stating thatconvolution becomes pointwise product in the Fourier (frequency) domain.This result is the consequence of the fact that the Fourier transformdiagonalizes Toeplitz operators (convolution). In fact, operating in theFourier domain, we can define any Toeplitz operator as a diagonallinear operator (point-wise product), fully defined by the function$\\hat{\\bb{w}}(\\bb{\\xi})$. Non-Euclidean domains In order to generalize the notion of a CNN to the non-Euclidean case,let us first define two types of non-Euclidean domains: manifolds andgraphs (we will think of the latter as of some sort of discretization ofthe former). Manifolds A topological space $\\mathcal{M}$ is called a $d$-dimensional manifoldif every point $p$ in it has a neighborhood topologically equivalent(homeomorphic) to $\\RR^d$. The latter space is referred to as thetangent space at point $p$, denoted as $T _p \\mathcal{M}$. The disjointunion of all tangent spaces is called the tangent bundle, denoted as$T\\mathcal{M}$. Since each point $p$ is now associated with a linearspace $T _p \\mathcal{M}$, we can endow the latter with an inner product$\\langle \\cdot, \\cdot \\rangle _{T _p \\mathcal{M}} : T _p \\mathcal{M}\\times T _p \\mathcal{M}\\rightarrow T _p \\mathcal{M}$(which we assume to depend smoothly on $p$, without further definingpreciselt what it means). This inner product is called a Riemannianmetric and a manifold endowed with it is called a Riemannianmanifold. The metric allows to (locally) measure lengths and angles. Fields A scalar field on $\\mathcal{M}$ is a function of the form$f : \\mathcal{M}\\rightarrow \\RR$. A (tangent) vector field is a map$F : \\mathcal{M}\\rightarrow T \\mathcal{M}$ assigning to every point$p \\in \\mathcal{M}$ a tangent vector $F(p) \\in T _p \\mathcal{M}$. Tangentvectors formalize the notion of infinitesimal displacements that weroutinely use in calculus on Euclidean domains. Next, we define theHilbert spaces of scalar and vector fields on $\\mathcal{M}$ through thefollowing standard inner products: \\[\\begin{aligned}\\langle  f, g \\rangle _{L^2(\\mathcal{M}) } &amp;=&amp; \\int _{\\mathcal{M}} f(p) g(p) dp; \\\\\\langle  F, G \\rangle _{L^2(T \\mathcal{M}) } &amp;=&amp; \\int _{\\mathcal{M}}  \\langle F(p) , G(p) \\rangle _{T _p \\mathcal{M}} dp,\\end{aligned}\\]where the integration is performed w.r.t. the $d$-dimensional volumeelement $dp$ induced by the metric. Differential The notion of a derivative in calculus describes how the value of afunction changes with an infinitesimal change of its argument. One ofthe big differences distinguishing calculus from differential geometryis a lack of a global vector space structure on the manifold, makingexpressions like $f(p+dp)$ meaningless. The conceptual leap that isrequired to generalize calculus to manifolds is the need to express allnotions locally in the tangent spaces. In order to construct calculus on a manifold, we define thedifferential of $f$ as the operator$df : T \\mathcal{M}\\rightarrow \\RR$ on tangent vectors. At every$p \\in \\mathcal{M}$, the differential is defined as the linearfunctional (a.k.a. $1$-form in the differential geometry jargon) \\[df(p) : v \\mapsto \\langle \\nabla f(p), v \\rangle _{T _p \\mathcal{M}},\\]$v \\in T _p \\mathcal{M}$. A vector field $F$ generalizes the notion ofsmall displacements. In fact, we can write \\[df(p)F(p) = \\langle \\nabla f(p), F(p) \\rangle _{T _p \\mathcal{M}},\\]asthe extension of the regular notion of directional derivative inEuclidean spaces, \\[df = f(\\bb{p}+d\\bb{p}) = \\langle \\nabla f(\\bb{p}), d\\bb{p}  \\rangle = \\frac{\\partial f(\\bb{p})}{\\partial p _1}dp _1 + \\cdots + \\frac{\\partial f(\\bb{p})}{\\partial p _d} dp _d.\\]Gradient and divergence The operator $\\nabla f(p) : \\mathcal{M}\\rightarrow T\\mathcal{M}$appearing in the definition of the differential generalizes the notionof the gradient defining the direction of the steepest increase of$f$; the main difference is that on a manifold the latter direction isgiven by tangent vector. The gradient can be viewed as an operator ofthe form $\\nabla : L^2(\\mathcal{M}) \\rightarrow L^2(T\\mathcal{M})$mapping scalar fields to vector fields. Its adjoint is called thedivergence operator,$\\mathrm{div} f(p) : L^2(T \\mathcal{M}) \\rightarrow L^2(\\mathcal{M})$mapping vector fields to scalar fields and satisfying \\[\\langle F, \\nabla f\\rangle _{L^2(T\\mathcal{M})} = \\langle \\nabla^\\ast F,  f\\rangle _{L^2(\\mathcal{M})} = \\langle -\\mathrm{div}\\, F,  f\\rangle _{L^2(\\mathcal{M})}\\](note the minus sign!). As vector fields can be thought of as a model ofa flow on the manifold, the divergence operator measures the net flow ata point. Laplacian The Laplacian (a.k.a. the Laplace-Beltrami operator)$\\Delta : L^2(\\mathcal{M}) \\rightarrow L^2(\\mathcal{M})$ is defined as \\[\\Delta  = \\nabla^\\ast \\nabla = -\\mathrm{div} \\nabla.\\]The Laplacianof a scalar field $f$ at point $p$ can be interpreted as the differencebetween the average value of the field on an infinitesimal sphere around$p$ and the value of $f(p)$. By virtue of the adjoint relation between the gradient and the negativedivergence, the Laplacian is self-adjoint (symmetric), that is, forevery scalar field $f$, \\[\\langle \\nabla f, \\nabla f \\rangle _{L^2(T\\mathcal{M})} = \\langle \\nabla^\\ast \\nabla f,  f \\rangle _{L^2(\\mathcal{M})} = \\langle \\Delta f,  f \\rangle _{L^2(\\mathcal{M})}\\]and \\[\\langle \\nabla f, \\nabla f \\rangle _{L^2(T\\mathcal{M})} = \\langle f,   \\nabla^\\ast \\nabla  f \\rangle _{L^2(\\mathcal{M})} = \\langle  f,  \\Delta f \\rangle _{L^2(\\mathcal{M})}.\\]The expression $\\langle \\Delta f,  f \\rangle _{L^2(\\mathcal{M})}$ isknown as the Dirichlet energy of the field $f$ and measures the“smoothness” of the field on $\\mathcal{M}$. Physically, it can beinterpreted as the potential energy due to the bending of an elasticbody. Graphs We will limit our attention to undirected graphs and view them as adiscrete analog of manifolds. We define the vertex set$V = \\{ 1,\\dots, n\\}$ (it can be any set containing $n$ objects, whichwe canonically map to the above set of natural numbers from $1$ to $n$);the edge set and the edge set $E \\subseteq V \\times V$. Anundirected graph has $(i,j) \\in E \\Leftrightarrow (j,i) \\in E$. Wefurther define the vertex weights as the function$a : V \\rightarrow (0,\\infty)$ and the edge weights as$w : E \\rightarrow \\RR _+$ (in fact, $w$ can be defined on the entire$V \\times V$ with $w _{ij} = 0$ meaning $(i,j) \\notin E$). We refer tothe tuple $\\mathcal{G}= (V,E,a,w)$ as to a weighted undirected graph. Difference operators A vertex field is a function of the form $f : V \\rightarrow \\RR$,while an edge field is a function of the form $F : E \\rightarrow \\RR$.Vertex and edge fields on a graph are the discrete analogs of scalar andvector fields on a manifold (under the tacit assumption that$F _{ij} = -F _{ji}$ for technical reason we are not goind to detail). Asin the case of manifolds, we define the two Hilbert spaces, $\\ell^2(V)$and $\\ell^2(E)$ through the corresponding inner products \\[\\begin{aligned}\\langle  f, g \\rangle _{\\ell^2(V) } &amp;=&amp; \\sum _{i \\in V} a _i f _i g _i ; \\\\\\langle  F, G \\rangle _{\\ell^2(E) } &amp;=&amp; \\sum _{(i,j) \\in E} w _{ij} F _{ij} G _{ij};\\end{aligned}\\]note that the weights play the role of discrete volume elements we hadbefore in the integrals on manifolds. The graph gradient is the operator$\\nabla : \\ell^2(V) \\rightarrow \\ell^2(E)$ defined by \\[(\\nabla f) _{ij} = f _i - f _j.\\]Note that the resulting edge field is,by definition, alternating, that is,$(\\nabla f) _{ij} = -(\\nabla f) _{ji}$. Analogously to manifolds, theadjoint operator, the graph divergence$\\mathrm{div} : \\ell^2(E) \\rightarrow \\ell^2(V)$ is defined as \\[(\\mathrm{div}\\, F) _i = \\frac{1}{a _i} \\sum _{(i,j) \\in E} w _{ij} F _{ij}.\\]It is straightforward to verify that \\[\\langle F, \\nabla f\\rangle _{\\ell^2(E)} = \\langle \\nabla^\\ast F,  f\\rangle _{\\ell^2(V)} = \\langle -\\mathrm{div}\\, F,  f\\rangle _{\\ell^2(V)}.\\]Graph Laplacian Having the gradient and the divergence operators defined, we define thegraph Laplacian $\\Delta :  \\ell^2(V) \\rightarrow \\ell^2(V)$ as$\\Delta = \\nabla^\\ast \\nabla = - \\mathrm{div}\\, \\nabla$, or, explicitly, \\[(\\Delta f) _i = \\frac{1}{a _i} \\sum _{(i,j) \\in E } w _{ij} (f _i - f _j).\\]Observe how this expression manifests the meaning of the Laplacian asthe difference between the value of a field at a vertex and the(weighed) average of its values in the surrounding. Since the vertex set is finite, it is convenient to represent theLaplacian as an $n \\times n$ matrix. For that purpose, we denote theedge weights by the $n \\times n$ matrix $\\bb{W} = (w _{ij})$, the vertexweights by the diagonal matrix$\\bb{A} = \\mathrm{diag}\\{ a _1,\\dots, a _n \\}$, and by$\\bb{D} =  \\mathrm{diag}\\left\\{ \\sum _{j: j\\ne i} w _{ij}  \\right\\}$ thevertex degree matrix. In this notation, the graph Laplacian is givenby \\[\\bb{\\Delta} = \\bb{A}^{-1} (  \\bb{D} - \\bb{W} ).\\]Different choices of $\\bb{A}$ lead to different definitions of aLaplacian. For $\\bb{A} = \\bb{I}$, the unnormalized graph Laplacian \\[\\bb{\\Delta} _{\\mathrm{un}} =  \\bb{D} - \\bb{W}\\]is obtained. The choice$\\bb{A} = \\bb{D}^{-1}$ leads to the random walk Laplacian \\[\\bb{\\Delta} _{\\mathrm{rw}} =  \\bb{I} - \\bb{D}^{-1} \\, \\bb{W}.\\]Theterm $\\bb{D}^{-1} \\, \\bb{W}$ in the definition of the above operator canbe interpreted as a transition probability of random walks on the graph,hence the name. Finally, when the graph is used as a discreteapproximation of the underlying continuous manifold (as is the case ofsimplicial complexes a.k.a. meshes), its weight matrices $\\bb{A}$ and$\\bb{W}$ are obtained from the discretized metric of the manifold. Fourier transform on non-Euclidean domains Thus far, we have constructed two types of non-Euclidean domains,manifolds and graphs, that both had a similarly defined Laplacianoperator. Next, we are going to use the Laplacian to define an analog ofFourier analysis. For convenience, we are going to construct the Fouriertransform on manifolds; the construction for graphs is straighforwardlysimilar. The Laplacian, being a self-adjoint operator, admits an orthogonaleigendecomposition \\[\\Delta \\phi _i = \\lambda _i \\phi _i.\\]Theeigenvalues $\\lambda _i$ (called the spectrum of the Laplacian) can befurthermore shown to be non-negative, a manifestation of the fact thatthe Laplacian is a postive semi-definite operator (by analogy, think ofa matrix defined through $\\bb{\\Delta} = \\bb{\\nabla}^\\Tr \\bb{\\nabla}$).On Euclidean domains, the eigenfunctions of the Laplacian are simplycomplex exponentials. A scalar field $f \\in L^2(\\mathcal{M})$ can be represented in theLaplacian eigenbasis as \\[f = \\sum _{i \\ge 0} \\hat{f} _i \\phi _i\\]with thecoordinates $\\hat{\\bb{f}} = \\{ \\hat{f} _i \\} _{i \\in \\mathbb{Z}}$. Becauseof orthonormality of the eigenfunctions, the coefficients $\\hat{f} _i$are given by \\[\\hat{f} _i = \\langle f, \\phi _i \\rangle _{L^2(\\mathcal{M}) }.\\]We willcall the operator$\\mathcal{F} : L^2(\\mathcal{M}) \\rightarrow \\ell^2(\\mathbb{Z})$ definedas \\[\\mathcal{F} f = \\{  \\langle f, \\phi _i \\rangle _{L^2(\\mathcal{M}) } \\} _{i \\in \\mathbb{Z}}\\]as the Fourier transform (analysis) on $\\mathcal{M}$. The inverse(synthesis) transform$\\mathcal{F}^{-1} : \\ell^2(\\mathbb{Z}) \\rightarrow L^2(\\mathcal{M})$ isgiven by \\[\\mathcal{F}^{-1} \\hat{\\bb{f}} = \\sum _{i \\ge 0} \\hat{f} _i \\phi _i.\\]Asbefore, it is easy to show that the above two operations are adjointw.r.t. the standard inner products on $L^2(\\mathcal{M})$ and$\\ell^2(\\mathbb{Z})$. Convolution on non-Euclidean domains Recall that one of the principal properties the Fourier transformenjoyed on Euclidean domains was the fact that it diagonalized Toeplitzoperators. In fact, we had the property \\[\\mathcal{F}(\\bb{f} \\ast \\bb{g}) =  \\mathcal{F}\\bb{f} \\cdot \\mathcal{F} \\bb{g}.\\]Unfortunately, the absense of a trivially defined translation group ongeneral non-Euclidean domains does not allow to generalize convolution,which makes the left-hand-side of the above equation undefined. However,the right-hand-side, being simply an element-wise product of frequencyresponses, is perfectly defined, so we will use it to defineconvolution on the non-Euclidean domain as \\[f \\ast g = \\mathcal{F}^{-1} (  \\mathcal{F} f \\cdot \\mathcal{F} g ) = \\sum _{i \\ge 0}  \\langle f, \\phi _i \\rangle _{L^2(\\mathcal{M}) }  \\langle g, \\phi _i \\rangle _{L^2(\\mathcal{M}) } \\phi _i.\\]The standard shift-invariance (or, more precisely,translation-equivariance) property of convolution on Euclidean domainsis lost of course. Using a signal processing metaphor, it can beinterpreted as a position-dependent filter, with the impulse responsethat can differ significantly at different locations in the domain. When dealing with discrete domain such as graphs (which we willhenceforth assume for convenience), the Fourier transform and itsinverse have a matrix form. Note that the eigendecomposition of theLaplacian $\\bb{\\Delta}$ can be written as$\\bb{\\Delta} = \\bb{\\Phi} \\bb{\\Lambda} \\bb{\\Phi}^\\Tr$, where $\\bb{\\Phi}$has the eigenvectors as its columns and$\\bb{\\Lambda} = \\mathrm{\\diag}\\{ \\lambda _1,\\dots,\\lambda _n\\}$.Representing vertex fields as $n$-dimensional column vectors, theanalysis (the forward transform) can be written as \\[\\hat{\\bb{f}} = \\mathcal{F} \\bb{f} = \\bb{\\Phi}^\\Tr \\bb{f} = ( \\langle \\bb{f}, \\bb{\\phi} _1  \\rangle, \\dots, \\langle \\bb{f}, \\bb{\\phi} _n  \\rangle )^\\Tr;\\]likewise, the synthesis operator (the inverse transform) assume the form \\[\\bb{f} = \\mathcal{F}^{-1} \\hat{\\bb{f}} = \\bb{\\Phi} \\hat{\\bb{f}}  = \\bb{\\phi} _1 \\hat{f} _1 + \\dots + \\bb{\\phi} _n \\hat{f} _n.\\]In this notation, the convolution of two fields $\\bb{f}$ and $\\bb{g}$can be written as \\[\\bb{f} \\ast \\bb{g} = \\bb{\\Phi} ( (\\bb{\\Phi} \\bb{f}) \\odot (\\bb{\\Phi} \\bb{g}) ),\\]where $\\odot$ denotes the Hadamard (element-wise) product. Convolution on Euclidean domains was an operation commuting with anytranslation-equivariant (Toeplitz) operator, including the Laplacian. Ingeneralizing it to non-Euclidean domains, we only demanded commutativitywith the Laplacian. Spectral CNN The spectral definition of a convolution-like operation on anon-Euclidean domain allows to parametrize the action of a filter as \\[\\mathcal{W} \\bb{f} = \\bb{\\Phi} \\hat{\\bb{W}} \\bb{\\Phi}^\\Tr \\bb{f},\\]where $\\hat{\\bb{W}}$ is a diagonal weight matrix containing the filter’sfrequency response on the diagonal. In the space domain, it amounts toapplying the operator $\\bb{W} = \\bb{\\Phi} \\hat{\\bb{W}} \\bb{\\Phi}^\\Tr$ to$\\bb{f}$, by computing the inner products of $\\bb{f}$ with every row of$\\bb{W}$ and stacking the resulting numbers into a vertex field.Different weight matrices $\\hat{\\bb{W}}$ realize different suchoperators. Note that the definition is basis-dependent: a change in the domain,and, consequently, in $\\bb{\\Phi}$ may translate the same $\\hat{\\bb{W}}$into a completely different operator. Therefore, this construction mustassume the domain fixed; if we learn the weights $\\hat{\\bb{W}}$, theywill typically generalize rather poorly even to similarly-lookingdomains. Siuch a complication did not exist on Euclidean domains. Armed with the notion of a generalized convolution on non-Euclideandomains, we can mimick the construction of a regular CNN. For thispurpose, we construct a spectral convolutional layer accepting an$m$-dimensional vertex field $\\bb{x} = (\\bb{x}^1,\\dots,\\bb{x}^m)$ andoutputting an $m’$-dimensional vertex field$\\bb{y}= (\\bb{y}^1,\\dots,\\bb{y}^{m’})$, whose $i$-the dimension isdefined according to \\[\\bb{y} _j = \\varphi\\left( \\sum _{i=1}^m \\bb{\\Phi} \\hat{\\bb{W}}^{ij} \\bb{\\Phi}^\\Tr \\bb{x}^i \\right),\\]where $\\varphi$ is an element-wise non-linearity such as ReLU, and$\\hat{\\bb{W}}^{ij}$, are diagonal matrices parametrizing the filters ofthe layer. Strided convolution Recall that a typical Euclidean CNN architecture used stridedconvolutions of the form \\[(\\downarrow _{\\bb{p}} \\left( \\bb{w} \\ast \\bb{x} \\right)) _{\\bb{k}} =  (\\bb{w} \\ast \\bb{x} ) _{ \\bb{p} \\odot \\bb{k} } = \\sum _{ (i _1,\\dots,i _d) } w _{i _1,\\dots,i _d} \\, x _{p _1 k _1 - i _1,\\dots,p _d k _d-i _d  },\\]where $\\bb{p} = (p _1,\\dots,p _d)$ is a $d$-dimensional vector of strides.This can be thought of projecting the result of the convolution$\\bb{w}\\ast\\bb{x}$ performed on $\\mathbb{Z}^d$ onto the coarser domain$\\downarrow _{\\bb{p}} \\mathbb{Z}^d$. The subsampling operator$\\downarrow _{\\bb{p}}$ can be thought of as a projection of a signal on$\\mathbb{Z}^d$ onto $\\downarrow _{\\bb{p}} \\mathbb{Z}^d$. The non-Euclidean analog can be constructed along the same lines. Let$\\mathcal{G}$ be the original domain of size $n$ with the Laplacian$\\bb{\\Delta} = \\bb{\\Phi} \\bb{\\Lambda} \\bb{\\Phi}^\\Tr$, and let$\\tilde{\\mathcal{G}}$ be its coarsened (sub-sampled) version containing$\\tilde{n} = \\alpha n &lt; n$ vertices. We denote by$\\tilde{\\bb{\\Delta}} = \\tilde{\\bb{\\Phi}} \\tilde{\\bb{\\Lambda}} \\tilde{\\bb{\\Phi}}^\\Tr$the corresponding Laplacian and its eigendecomposition. To keep theprevious notation, we denote by$\\downarrow _\\alpha : \\mathcal{G} \\rightarrow \\tilde{\\mathcal{G}}$ theprojection onto the coarse domain, i.e., $\\downarrow _\\alpha$ maps avertex field on $\\mathcal{G}$ to a vertex field on$\\tilde{\\mathcal{G}}$. In matrix form, $\\downarrow _\\alpha$ is an$\\tilde{n} \\times n$ matrix whose $i$-th row encodes the position of the$i$-th vertex of the coarse domain $\\tilde{\\mathcal{G}}$ in the finedomain $\\mathcal{G}$. The eigenvectors $\\bb{\\Phi}$ and $\\tilde{\\bb{\\Phi}}$ of the fine and thecoarse Laplacians, $\\bb{\\Delta}$ and $\\tilde{\\bb{\\Delta}}$, satisfy thefollowing multi-resolution property: \\[\\tilde{\\bb{\\Phi}} \\, \\approx \\,\\, \\downarrow _\\alpha \\bb{\\Phi} \\bb{P} _{\\alpha},\\]where the $n \\times \\tilde{n}$ matrix \\[\\bb{P} _\\alpha = \\left( \\begin{array}{c} \\bb{I} _{\\alpha n} \\\\ \\bb{0} \\end{array} \\right)\\]denotes the projection onto the lowest $\\tilde{n} = \\alpha n$frequencies. This property essentially means that only the first$k = \\alpha n$ components of the spectrum can be retained. Thus, thestrided convolutional layer assumes the form \\[\\bb{y} _j = \\varphi\\left( \\sum _{i=1}^m \\tilde{\\bb{\\Phi}} _k \\hat{\\bb{W}}^{ij} \\bb{\\Phi} _k^\\Tr \\bb{x}^i \\right),\\]where $\\bb{\\Phi} _k = (\\bb{\\phi} _1,\\dots\\bb{\\phi} _k)$ is the truncatedeigenbasis of the fine Laplacian containing the first $k$ eigenvectors,and the weight matrices $\\hat{\\bb{W}}^{ij}$ are now$\\tilde{n} \\times \\tilde{n}$. The layer accepts an $m$-dimensionalvertex field $\\bb{x} = (\\bb{x}^1,\\dots,\\bb{x}^m)$ on $\\mathcal{G}$ asthe input and produces and $m’$-dimensional vertex field$\\bb{y} = (\\bb{y}^1,\\dots,\\bb{y}^{m’})$ on $\\tilde{\\mathcal{G}}$ as theoutput. Spatial localization Note that in our construction of a spectral convolutional layer, eachweight matrix has $k=\\mathcal{O}(n)$ degrees of freedom, so that eachlayer has $\\mathcal{O}(nmm’)$ degrees of freedom, unlike the regularCNN, in which the layer was parametrized in the spatial domain by afixed-size kernel with the number of parameters independent on thedomain size $n$. In order to keep the number of parameters under controland avoid overfitting, we would like to impose spatial localization ontothe weights $\\hat{\\bb{W}}^{ij}$, that is, ensure that the vertex fieldsdefined by every row of the operator$\\bb{W} = \\bb{\\Phi} \\hat{\\bb{W}} \\bb{\\Phi}^\\Tr$ are spatially localized. On a Euclidean domain, the spatial localization of a signal$w : \\mathbb{Z} \\rightarrow \\mathbb{R}$ is controlled by the decay ofits moments, defined as \\[\\mu _p^2(w) = \\sum _{k \\in \\mathbb{Z}}  k^{2p} w^2 _k = \\| k^p \\cdot w _k  \\| _{\\ell^2(\\mathbb{Z}) }^2.\\]The faster $\\mu _p^2(w)$ vanishes as $p$ increases, the more localized is$w$. From \\[\\frac{\\partial}{\\partial \\xi} \\mathcal{F}w  = \\frac{\\partial}{\\partial \\xi} \\left( \\sum _{k \\in \\mathbb{Z}} w _k e^{-i 2\\pi \\xi k} \\right) =   \\sum _{k \\in \\mathbb{Z}}  -i 2\\pi k w _k e^{-i 2\\pi \\xi k} =  -i 2\\pi  \\mathcal{F}(k w _k)\\]we obtain the property \\[k^p \\cdot w _k  \\, \\mathop{\\longleftrightarrow}^{\\mathcal{F}} \\, \\left( \\frac{i}{2\\pi} \\right)^p \\frac{\\partial^p \\hat{w}}{\\partial \\xi^p}.\\]Invoking Parseval’s identity, \\[\\mu _p^2(w) = \\| k^p \\cdot w _k  \\| _{\\ell^2(\\mathbb{Z}) }^2 =  \\left\\|   \\left( \\frac{i}{2\\pi} \\right)^p \\frac{\\partial^p \\hat{w}}{\\partial \\xi^p}  \\right\\| _{L^2([0,1]) }^2 = \\frac{1}{(2\\pi)^{2p}} \\int _{[0,1]} \\left|  \\frac{\\partial^p \\hat{w}(\\xi) }{\\partial \\xi^p} \\right|^{2}   d\\xi.\\]This result implies that fast decay of $\\mu _p^2(w)$ implies fast decayof the derivatives of $\\hat{w}$, or, said differently, localization inthe spatial domain is equivalent to smoothness in the frequency domain(the fact that smoothness is opposite to localization brings forth therenowned Heisenberg’s uncertainty principle). Smoothness of thefrequency response $\\hat{w}$ can be asserted by representing it in anunderdetermined smooth basis or, equivalently, specifying it only at asmall set of frequencies and completing the rest via some smoothinterpolation. This idea can be generalized to non-Euclidean domains. The onlycomplication is that while in $\\RR^d$ we had a trivia notion ofsmoothness arising in the spectrum, since the similarity between twobasis functions $\\phi^{\\bb{\\xi}} = e^{i 2 \\pi \\bb{x}^\\Tr \\bb{\\xi}}$ and$\\phi^{\\bb{\\xi}’} = e^{i 2 \\pi \\bb{x}^\\Tr \\bb{\\xi}’}$ could bequantified as the distance $|\\bb{\\xi} - \\bb{\\xi}’ |$, there is notsuch a standard notion in the spectrum of a general non-Euclideandomain. A formal way to define smoothness is by constructing a dualgraph whose weights $w^\\ast _{ij}$ reflect the similarity between theeigenvectors $\\bb{\\phi} _i$ and $\\bb{\\phi} _j$ of the Laplacian of theoriginal (primal) graph. The question of how to define such a dual graphthe smoothness on which will lead to maximal localization on the primalgraph is still open. However, empirical evidence shows that at least insome cases, the simple definition of$w^\\ast _{ij} = | \\lambda _i - \\lambda _j|$ leads to reasonablelocalization. With this notion of smoothness in mind, we fix a set of $q$ smooth basisfunctions $\\beta _1(\\lambda), \\dots, \\beta _q(\\lambda)$ (e.g., cubicsplines) and sample them at$\\lambda \\in \\{ \\lambda _1,\\dots, \\lambda _k \\}$. We arrange the samplesinto a $k \\times q$ matrix $\\bb{B}$ with the elements$b _{rs} = \\beta _s(\\lambda _r)$. The spectral weight matrices$\\hat{\\bb{W}}^{ij}$ can now be defined as \\[\\hat{\\bb{W}}^{ij} = \\mathrm{diag}\\{ \\bb{B} \\bb{\\alpha}^{ij}  \\},\\]where $\\bb{\\alpha}^{ij}$ are $q$-dimensional interpolation coefficients.In order to render the layer complexity independent of the domain size,one has to choose $q = \\mathcal{O}(1)$. Spatial CNN One of the main disadvantages of the spectral construction of aconvolutional layer is its high computational complexity. Themultiplication by $\\bb{\\Phi}$ and $\\bb{\\Phi}^\\Tr$ in the forward andbackward passes require $\\mathcal{O}(n^2)$ operations, which quicklybecomes prohibitively expensive for large domains. Unlike Euclideandomains on which the forward and inverse Fourier transforms can becarried out using FFT in $\\mathcal{O}(n \\log n)$ operations, no suchfast algorithms exist for general non-Euclidean domains. In whatfollows, we will reformulate the convolutional layer in a way free ofthe costly Laplacian eigendecomposition and explicit projection on itsbasis. Let us substitute$\\hat{\\bb{W}} = \\mathrm{diag}\\{ \\bb{B} \\bb{\\alpha}  \\}$ and examine thespatial representation of the linear part of the layer: \\[\\begin{aligned}\\bb{W} &amp;=&amp; \\bb{\\Phi} _k \\hat{\\bb{W}} \\bb{\\Phi} _k^\\Tr  = \\bb{\\Phi} _k \\left( \\begin{array}{ccc}\\sum _{i = 1}^q \\alpha _i \\beta _i(\\lambda _1) &amp; &amp; \\\\&amp; \\ddots &amp; \\\\&amp; &amp;  \\sum _{i = 1}^q \\alpha _i \\beta _i(\\lambda _n) \\end{array}   \\right) \\bb{\\Phi} _k^\\Tr \\end{aligned}\\](note that weassumed $\\beta _i(\\lambda) = 0$ for $\\lambda &gt; \\lambda _k$). Denoting by \\[b(\\lambda) =  \\sum _{i = 1}^q \\alpha _i \\beta _i(\\lambda),\\]we have \\[\\bb{W} = \\bb{\\Phi} \\, \\mathrm{diag}\\{ b(\\lambda _1), \\dots, b(\\lambda _n) \\} \\, \\bb{\\Phi}^\\Tr.\\](note that we assumed $b(\\lambda) = 0$ for $\\lambda &gt; \\lambda _k$). Since $b(\\lambda)$ is typically a polynomial (of degree $3$ in case ofcubic splines), let us examine how to rewrite it directly in the spatialdomain. Let $\\bb{\\Delta} = \\bb{\\Phi} \\bb{\\Lambda} \\bb{\\Phi}^\\Tr$ be theeigendecomposition of the Laplacian, and suppose we woud like to compute$\\bb{\\Delta}^p$ for some integer power $p$. Then, \\[\\bb{\\Delta}^p =\\bb{\\Phi} \\bb{\\Lambda} \\bb{\\Phi}^\\Tr  \\cdots   \\bb{\\Phi} \\bb{\\Lambda} \\bb{\\Phi}^\\Tr =\\bb{\\Phi} \\bb{\\Lambda}^p \\bb{\\Phi}^\\Tr = \\bb{\\Phi} \\, \\mathrm{diag}\\{ \\lambda _1^p, \\dots, \\lambda _n^p \\} \\, \\bb{\\Phi}^\\Tr.\\]Using linearity, we can conclude that for any polynomial \\[b(\\lambda) = \\sum _{i=0}^{r} \\alpha _i \\lambda^i,\\]one has \\[b(\\bb{\\Delta}) =  \\sum _{i=0}^{r} \\alpha _i \\bb{\\Delta}^i =\\bb{\\Phi} \\, \\mathrm{diag} \\left\\{ b(\\lambda _1),\\dots, b(\\lambda _n) \\right\\} \\, \\bb{\\Phi}^\\Tr.\\]In other words, the expensive right-hand-side can be simply evaluated asapplying the polynomial $b$ directly to the Laplacian. The Laplacian istypically a sparse $n \\times n$ matrix with $\\mathcal{O}(1)$ non-zeroentries in every row. In such cases, computing its powers takes$\\mathcal{O}(n)$ operations, and the entire calculation is$\\mathcal{O}(nr)$. Also note that since the Laplacian is a localoperator acting on $1$-rings, its highest power $\\bb{\\Delta}^{r}$ willact on $r$-rings, keeping the operator $b(\\bb{\\Delta})$ spatiallylocalized. Using this observation, we can reformulate the convolutional layerdirectly in the spatial domain as \\[\\bb{y} _j = \\varphi\\left( \\sum _{i=1}^m  \\sum _{k=0}^r \\alpha _k^{ij} \\bb{\\Delta}^k  \\bb{x}^i \\right),\\]            The term shift-invariant is so abundant in the signal processingand machine learning literature that we will not even attempt tochange this unfortunate fact. However, it is worth noting that thecorrect mathematical term would be shift-equivariant. In general,let $f : \\mathbb{U} \\rightarrow \\mathbb{V}$ be an operator mappingfrom some domain $\\mathbb{U}$ to some co-domain $\\mathbb{V}$, andlet $\\mathcal{G}$ be a group of transformations that can be appliedboth to the domain and the co-domain. The operator $f$ is saidinvariant to the action of $\\mathcal{G}$ if $f \\circ \\tau = f$ forevery $\\tau \\in \\mathcal{G}$. On the other hand, the operator isequivariant if $f \\circ \\tau = \\tau \\circ f$. &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_11/",
        "teaser":null},{
        "title": "Lecture 1: Introduction",
        "excerpt":"Origins of deep learning, course goals, overview of machine-learningparadigms, intro to computational acceleration.                   Video   Slides         Lecture Notes This lecture has no accompanying notes. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/01-intro/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec1/dataism-large.jpg"},{
        "title": "Lecture 2: Supervised learning",
        "excerpt":"Supervised learning problem statement, data sets, hypothesis classes, lossfunctions, basic examples of supervised machine learning models, addingnon-linearity.                   Videos   Introductory video    Main lecture video  Slides   Introductory slides          Main lecture slides        Lecture Notes Accompanying notes for this lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/02-supervised/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec2/supervised-bw.jpg"},{
        "title": "Lecture 3: Neural Networks",
        "excerpt":"Linear and multilayer Perceptron, loss functions, activation functions, pooling,weight sharing, convolutional layers, gradient descent, backpropagation.                   Video   Slides         Supplementary Slides   Linear Models          CNNs        Lecture Notes Accompanying notes for this lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/03-neural_nets/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec3/nn-black.jpg"},{
        "title": "Lecture 4: Training and Optimization",
        "excerpt":"Approximation, estimation and optimization errors, regularization, loss surfacecurvature, descent-based optimization methods, second-order methods.                   Videos   Introductory lecture    Main Lecture  Slides         Supplementary Slides         ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/04-optimization/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec4/teaser.jpg"},{
        "title": "Lecture 5: Sequence Models",
        "excerpt":"RNN model, input-output sequences relationships, non-sequential input, layered RNN,backpropagation through time, word embeddings, attention, transformers.                   Video   Supplementary Video Word embeddings   Slides         Supplementary Slides         Lecture Notes Accompanying notes for this lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/05-sequence/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec5/rnn.png"},{
        "title": "Lecture 6: Unsupervised Learning and Generative Models",
        "excerpt":"Subspace models, autoencoders, unsupervised loss,generative adversarial nets, domain adaptation.                   Video   Supplementary video Stabilizing GAN training   Slides         Supplementary slides                         Lecture Notes Accompanying notes for this lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/06-unsupervised/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec6/3.png"},{
        "title": "Lecture 7: Reinforcement Learning",
        "excerpt":"Markov decision process, policies, rewards, value functions, the Bellmanequation, q-learning, policy learning, actor-critic learning, AutoML.                   Video   Slides         Supplementary Slides         Lecture Notes Accompanying notes for this lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/07-rl/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec7/rl2.jpg"},{
        "title": "Lecture 8: Learning on Non-Euclidean Domains",
        "excerpt":"Toeplitz operators, graphs, fields, gradients, divergence, Laplace-Beltramioperator, non-euclidean convolution, spectral and spatial CNN for graphs.                   Video   Slides         Supplementary slides         Lecture Notes Accompanying notes for this lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/08-geometric/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec11/lec11-1.jpg"},{
        "title": "Lecture 9: Object detection",
        "excerpt":"CV-based approaches, R-CNN, RPN, YOLO, SSD, losses, benchmarks and performance metrics.                   Slides         Lecture Notes Accompanying notes are not available for this lecture. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/lectures/09-detection/",
        "teaser":"https://vistalab-technion.github.io/cs236781/semesters/w22/assets/images/lec13/Picture6.png"},{
        "title": "Welcome to CS236781",
        "excerpt":"Welcome to CS236781: Deep Learning on Computational Accelerators! First, please read the info pagecarefully and in full, as it contains crucial information about the course.Next, please read the rest of the posts on this site as they contain importantadministrative information. We hope you enjoy our course and have a productive semester!Aviv &amp; Course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/10/23/welcome/",
        "teaser":null},{
        "title": "Course Piazza",
        "excerpt":"Dear Students, We have opened a Piazza forum for your benefit.This will serve as the only official discussion forum for our course. You can sign up and access Piazzahere, by signing up with yourTechnion @campus email. External (non-Technion) students can also accessPiazza by invitation. Such students can send me an email and request an invitefor Piazza. All questions regarding course material and homework must be posted on thePiazza forum. Please refrain from using email for this purpose. This way allstudents can benefit collectively from the answers we post. We also encourage you to read the welcomepost and find out how you canget a bonus grade by using Piazza! Thanks and enjoy,Aviv &amp; Course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/10/24/course-piazza/",
        "teaser":null},{
        "title": "Course schedule",
        "excerpt":"Dear Students, Starting from Thursday, October 28th, 2021, the course weekly schedule is as follows:       Thursdays, 11:30-12:30, Taub 1.Supplementary lecture, presented by Dr. Chaim Baskin or Prof. Alex Bronstein.         Thursdays at 12:30-13:30, Taub 1.Tutorial, presented by Aviv Rosenberg.         Wednesdays at 11:30-12:30 Taub 9.Tutorial, presented by Yaniv Nemcovsky.   Notes:   Yaniv’s tutorial group starts from the second week of the semester, sothat it comes after each week’s lecture.  The primary lecture material is covered by Alex’s videos. Weeklyvideo-viewing schedule is defined in the syllabus. Make sure to watch the relevant videolecture each week before class.  The in-class lectures provide supplementary material. They will be 1-hourlong and aim to present more advanced topics and new research in the field.  To support international students, all course materials, including lectureand tutorial videos, are provided in English.We wish you a productive semester and hope you enjoy the course! Good luck,Aviv &amp; course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/10/24/course-schedule/",
        "teaser":null},{
        "title": "Grading policy",
        "excerpt":"Dear Students, The homework assignments in this course do not have equal weight.Also, each assignment will be graded by a different HW checker. The grading policy and HW checkers are as follows:             #      Weight      Graded By                  HW1      10%      Yaniv              HW2      20%      Evgenii              HW3      30%      Evgenii              HW4 (mini-project)      40%      Ben      Please note:   Any appeals or inquiries about the grades should be sent directly to therelevant HW checker for the assignment, not to the TAs.  Technical questions about the HW assignments must be posted on Piazza only.Good luck,Course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/10/24/grading-policy/",
        "teaser":null},{
        "title": "Registration for course servers",
        "excerpt":"Dear students, This course requires use of a GPU for some of the homework assignments.We can provide you access to the faculty’s compute cluster which containsGPU-enabled servers. Important: enrollment in the course does not automatically provideaccess to the course servers; if you wish you use the course servers, separateregistration is required using the link below. Deadline for registration is 18/11/2021. After this date the list ofstudents who requested access (and are also officially enrolled to the course)will be passed to the CS faculty IT team and granted access. Students who donot register before the deadline forfeit server access for the entire semester! Please register using this link.Make sure to only provide your @campus email address. Thanks,Course Staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/10/24/registration-for-servers/",
        "teaser":null},{
        "title": "First homework assignment",
        "excerpt":"Dear Students, The first homework assignment is out and can be viewed in the assignments section. Please read the getting started page carefully before starting! Good luck and have fun with the assignment.Aviv ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/11/04/hw1/",
        "teaser":null},{
        "title": "Room Change for Thursday Tutorials",
        "excerpt":"Dear Students, Starting from today (November 11), the Thursday 12:30 tutorials will bepresented in Taub 7. See you,Aviv &amp; Course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/11/11/room-change/",
        "teaser":null},{
        "title": "Access to course servers",
        "excerpt":"Dear Students, Those of you who registered for access to the course servers should now have access.Please read the usage instructions carefully. Also, please note that if you have issues using the Technion VPN, pleasecontact CIS support directly, as we cannot assist you with that. Good luck,Aviv ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/11/17/access-to-servers/",
        "teaser":null},{
        "title": "Second homework assignment",
        "excerpt":"Dear Students, The second homework assignment is out and can be viewed in the assignments section. Those of you who signed up for access to the course servers should have access.Please read the guide for using course servers carefully. Good luck and have fun!Aviv ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/11/28/hw2/",
        "teaser":null},{
        "title": "Update for HW2",
        "excerpt":"Dear Students, We have published an update for HW2.Please see the hw2 assignment page for more details and theupdated download link. Good luck,Aviv ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/12/05/hw2-update1/",
        "teaser":null},{
        "title": "Computational imaging projects",
        "excerpt":"Dear students, The VISTA Lab is looking for excellent students (M.Sc. or final yearundergraduate) to conduct research projects on computational imaging. Our DeepLearning course has strong applicability for these projects. Please find below the project proposals with further details.Interested students can contact Tomer Weiss.   Learning efficient dynamic MRI  Task-driven efficient MIMO Radar ImagingThanks,Course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/12/07/projects_at_vista/",
        "teaser":null},{
        "title": "Third homework assignment",
        "excerpt":"Dear Students, The third homework assignment is out and can be viewed in the assignments section. Enjoy!Aviv ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2021/12/26/hw3/",
        "teaser":null},{
        "title": "Final homework assignment",
        "excerpt":"Dear Students, The final homework assignment is out and can be viewed in the assignments section. Good luck and enjoy,Aviv ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/2022/01/18/hw4/",
        "teaser":null},{
        "title": "Linear Algebra Crash Course",
        "excerpt":"Introduction The purpose of this document is to quickly refresh (presumably) knownnotions in linear algebra. It contains a collection of facts related tovectors, matrices, and geometric entities they represent that we willuse heavily in our course. Even though this quick reminder may seemredundant or trivial to most of you (I hope), I still suggest at leastto skim through it, as it might present less common ways ofinterpretation of very familiar definitions and properties. And even ifyou discover nothing new in this document, it will at least be useful tointroduce notation. Notation In our course, we will deal almost exclusively with the field of realnumbers, which we denote by $\\RR$. An $n$-dimensional Euclidean spacewill be denoted as $\\RR^n$, and the space of $m \\times n$ matrices as$\\RR^{m \\times n}$. We will try to stick to a consistent typesettingdenoting a scalar $a$ with lowercase italic, a vector $\\bb{a}$ inlowercase bold, and a matrix $\\bb{A}$ in uppercase bold. Elements of avector or a matrix will be denoted using subindices as $a_i$ and$a_{ij}$, respectively. Unless stated otherwise, a vector is a columnvector, and we will write $\\bb{a} = (a_1, \\dots, a_n)^\\Tr$ to save space(here, $^\\Tr$ is the transpose of the row vector $(a_1,\\dots, a_n)$). Inthe same way, $\\bb{A} = (\\bb{a}_1,\\dots,\\bb{a}_n)$ will refer to amatrix constructed from $n$ column vectors $\\bb{a}_i$. We will denotethe zero vector by $\\bb{0}$, and the identity matrix by $\\bb{I}$. Linear and affine spaces Given a collection of vectors${ \\bb{v}_1,\\dots,\\bb{v}_m \\in \\bb{R}^n }$, a new vector$\\bb{b} = a_1 \\bb{v}_1 + \\dots + a_m \\bb{v}_m$, with$a_1,\\dots, a_m  \\in \\RR$ some scalars, is called a linear combinationof the $\\bb{v}_i$’s. The collection of all linear combinations is calleda linear subspace of $\\RR^n$, denoted by \\[\\mathcal{L} = \\spn{ \\bb{v}_1,\\dots,\\bb{v}_m } = \\{ a_1 \\bb{v}_1 + \\dots + a_m \\bb{v}_m : a_1,\\dots, a_m  \\in \\RR \\}.\\]We will say that the $\\bb{v}_i$’s span the linear subspace $\\mathcal{L}$. A vector $\\bb{u}$ that cannot be described as a linear combination of$\\bb{v}_1,\\dots,\\bb{v}_m$ (i.e., $\\bb{u} \\notin \\mathcal{L}$) is said tobe linearly independent of the latter vectors. The collection ofvectors ${ \\bb{v}_1,\\dots,\\bb{v}_m }$ is linearly independent if no$\\bb{v}_i$ is a linear combination of the rest of the vectors. In such acase, a vector in $\\mathcal{L}$ can be unambiguously defined by the setof $m$ scalars $a_1,\\dots, a_m$; omitting any of these scalars will makethe description incomplete. Formally, we say that $m$ is the dimensionof the subspace, $\\dim \\,\\mathcal{L} = m$. Geometrically, an $m$-dimensional subspace is an $m$-dimensional planepassing through the origin (a one-dimensional plane is a line, atwo-dimensional plane is the regular plane, and so on). The latter istrue, since setting all the $a_i$’s to zero in the definition of$\\mathcal{L}$ yields the zero vector. Figure 1(left) visualizes this fact. An affine subspace can be defined as a linear subspace shifted by avector $\\bb{b} \\in \\RR^n$ away from the origin: \\[\\mathcal{A} = \\mathcal{L} + \\bb{b} = \\{ \\bb{u} + \\bb{b} : \\bb{u} \\in \\mathcal{L} \\}.\\]Exercise. Show that any affine subspace can be defined as \\[\\mathcal{A} = \\{ a_1 \\bb{v}_1 + \\dots + a_m \\bb{v}_m : a_1 + \\dots + a_m = 1 \\}.\\]The latter linear combination with the scalars restricted to unit sum iscalled an affine combination.                 Figure 1: One-dimensional linear (left) and affine (right) subspaces of $\\RR^2$.  Vector inner product and norms Given two vectors $\\bb{u}$ and $\\bb{v}$ in $\\RR^n$, we define theirinner (a.k.a. scalar or dot) product as \\[\\langle \\bb{u}, \\bb{v} \\rangle = \\sum_{i=1}^n u_i v_i = \\bb{u}^\\Tr \\bb{v}.\\]Though the notion of an inner product is more general than this, we willlimit our attention exclusively to this particular case (and its matrixequivalent defined in the sequel). This is sometimes called the standardor the Euclidean inner product. The inner product defines or induces avector norm \\[\\| \\bb{u} \\| = \\sqrt{ \\langle \\bb{u}, \\bb{u} \\rangle }\\](it is also convenient to write $|\\bb{u}|^2 = \\bb{u}^\\Tr \\bb{u}$),which is called the Euclidean or the induced norm. A more general notion of a norm can be introduced axiomatically. We willsay that a non-negative scalar function$| \\cdot | : \\RR^n \\rightarrow \\RR_+$ is a norm if it satisfies thefollowing axioms for any scalar $a \\in \\RR$ and any vectors$\\bb{u},\\bb{v} \\in \\RR^n$       $|a \\bb{u} | = | a | | \\bb{u} |$ (this property is calledabsolute homogeneity);         $| \\bb{u} + \\bb{v} | \\le | \\bb{u} | + | \\bb{v} |$ (thisproperty is called subadditivity, and since a norm induces ametric (distance function), the geometric name for it is triangleinequality);         $| \\bb{u} | = 0$ iff1 $\\bb{u} = \\bb{0}$.   In this course, we will almost exclusively restrict our attention to thefollowing family of norms called the $\\ell_p$ norms: For $1 \\le p&lt;\\infty$, the $\\ell_p$ norm of a vector $\\bb{u} \\in \\RR^n$is defined as\\(\\| \\bb{u} \\|_p = \\left( \\sum_{i=1}^n |u_i|^p \\right)^{1/p}.\\) Note the sub-index $_p$. The Euclidean norm corresponds to $p=2$ (hence,the alternative name, the $\\ell_2$ norm) and, whenever confusion mayarise, we will denote it by $| \\cdot |_2$. Another important particular case of the $\\ell_p$ family of norms is the$\\ell_1$ norm \\(\\| \\bb{u} \\|_1 = \\sum_{i=1}^n |u_i|.\\) As we will see,when used to quantify errors for example in a regression or modelfitting task, the $\\ell_1$ norm (representing mean absolute error) ismore robust (i.e., less sensitive to noisy data) than the Euclidean norm(representing mean squared error). The selection of $p=1$ is thesmallest among the $\\ell_p$ norms for which the norm is convex. Inthis course, we will dedicate significant attention to this importantnotion, as convexity will have profound impact on solvability ofoptimization problems. Yet another important particular case is the $\\ell_\\infty$ norm\\(\\| \\bb{u} \\|_\\infty = \\max_{i=1,\\dots,n} |u_i|.\\) The latter can beobtained as the limit of $\\ell_p$. Exercise. Show that \\(\\displaystyle{\\lim_{p \\rightarrow \\infty} \\| \\bb{u} \\|_p =\\max_{i=1,\\dots,n} |u_i|}\\). Geometrically, a norm measures the length of a vector; a vector oflength $|\\bb{u}|=1$ is said to be a unit vector (with respect to2that norm). The collection ${ \\bb{u} : | \\bb{u} | = 1 }$ of all unitvectors is called the unit circle (see figure).Note that the unit circle is indeed a “circle” only for the Euclideannorm. Similarly, the collection$B_r = { \\bb{u} : | \\bb{u} | \\le r }$ of all vectors with length nobigger than $r$ is called the ball of radius $r$ (w.r.t. a givennorm). Again, norm balls are round only for the Euclidean norm. Fromfigure 2 we can deduce that for $p &lt; q$, the$\\ell_p$ unit circle is fully contained in the $\\ell_q$ unit circle.This means that the $\\ell_p$ norm is bigger than the $\\ell_q$ norm inthe sense that $| \\bb{u} |_q \\le | \\bb{b}|_p$. Exercise. Show that $| \\bb{u} |_q \\le | \\bb{b}|_p$ for every $p &lt; q$.                 Figure 2: Unit circles with respect to different $\\ell_p$ norms in $\\RR^2$  Continuing the geometric interpretation, it is worthwhile mentioningseveral relations between the inner product and the $\\ell_2$ norm itinduces. The inner product of two ($\\ell_2$-) unit vectors measures thecosine of the angle between them or, more generally,\\(\\langle \\bb{u}, \\bb{v} \\rangle = \\| \\bb{u} \\|_2 \\| \\bb{v} \\|_2 \\cos \\theta,\\)where $\\theta$ is the angle between $\\bb{u}$ and $\\bb{v}$. Two vectorssatisfying $\\langle \\bb{u}, \\bb{v} \\rangle = 0$ are said to beorthogonal (if the vectors are unit, they are also said to beorthonormal) – the algebraic way of saying “perpendicular”. The following result is doubtlessly the most important inequality inlinear algebra (and, perhaps, in mathematics in general): Theorem: Cauchy-Schwartz inequality.Let $| \\cdot |$ be the norm induced by an inner product$\\langle \\cdot, \\cdot \\rangle$ on $\\RR^n$, Then, for any$\\bb{u}, \\bb{v} \\in \\RR^n$, \\[| \\langle \\bb{u}, \\bb{v} \\rangle | \\le \\| \\bb{u} \\| \\| \\bb{v} \\|\\]with equality holding iff $\\bb{u}$ and $\\bb{v}$ are linearly dependent. Matrices A linear map from the $n$-dimensional linear space $\\RR^n$ to the$m$-dimensional linear space $\\RR^m$ is a function mapping$\\bb{u} \\in \\RR^n$ $\\bb{v} \\in \\RR^m$ according to \\[v_i = \\sum_{j=1}^n a_{ij} u_j  \\ \\ \\ i=1,\\dots,m.\\]The latter can beexpressed compactly using the matrix-vector product notation$\\bb{v} = \\bb{A} \\bb{u}$, where $\\bb{A}$ is the matrix with the elements$a_{ij}$. In other words, a matrix $\\bb{A} \\in \\RR^{m \\times n}$ is acompact way of expressing a linear map between $\\RR^m$ and $\\RR^n$. Anmatrix is said to be square of $m=n$; such a matrix defines anoperator mapping $\\RR^n$ to itself. A symmetric matrix is a squarematrix $\\bb{A}$ such that $\\bb{A}^\\Tr = \\bb{A}$. Recollecting our notion of linear combinations and linear subspaces,observe that the vector $\\bb{v} = \\bb{A}\\bb{u}$ is the linearcombination of the columns $\\bb{a}_1,\\dots,\\bb{a}_n$ of $\\bb{A}$ withthe weights $u_1,\\dots,u_n$. The linear subspace$\\bb{A} \\RR^m = { \\bb{A} \\bb{u} : \\bb{u} \\in \\RR^m \\ }$ is called thecolumns space of $\\bb{A}$. The space is $n$-dimensional if the columnsare linearly independent; otherwise, if $k$ columns are linearlydependent, the space is $n-k$ dimensional. The latter dimension iscalled the column rank of the matrix. By transposing the matrix, therow rank can be defined in the same way. The following result isfundamental in linear algebra: Theorem. The column rank and the row rank of a matrix are always equal. Exercise. Prove the above theorem. Since the row and the column ranks of a matrix are equal, we will simplyrefer to both as the rank, denoting $\\rank\\, \\bb{A}$. A square$n \\times n$ matrix is full rank if its rank is $n$, and is rankdeficient otherwise. Full rank is a necessary condition for a squarematrix to possess an inverse (Reminder: the inverse of a matrix$\\bb{A}$ is such a matrix $\\bb{B}$ that$\\bb{A}\\bb{B} = \\bb{B}\\bb{A} = \\bb{I}$; when the inverse exists, thematrix is called invertible and its inverse is denoted by$\\bb{A}^{-1}$). In this course, we will often encounter the trace of a square matrix,which is defined as the sum of its diagonal entries,\\(\\trace\\, \\bb{A} = \\sum_{i=1}^n a_i.\\) The following property of thetrace will be particularly useful: Let $\\bb{A} \\in \\RR^{m \\times n}$ and $\\bb{B} \\in \\RR^{n \\times m}$.Then $\\trace(\\bb{A}\\bb{B}) = \\trace(\\bb{B}\\bb{A})$. This is in sharp contrast to the result of the product itself, which isgenerally not commutative. In particular, the squared norm $| \\bb{u} |^2 = \\bb{u}^\\Tr \\bb{u}$ canbe written as $\\trace(\\bb{u}^\\Tr \\bb{u}) = \\trace(\\bb{u}\\bb{u}^\\Tr)$. Wewill see many cases where such an apparently weird writing is veryuseful. The above property can be generalized to the product of $k$matrices $\\bb{A}_1 \\dots \\bb{A}_k$ by saying that$\\trace(\\bb{A}_1 \\dots \\bb{A}_k)$ is invariant under a cyclicpermutation of the factors as long as their product is defined. Forexample,$\\trace(\\bb{A}\\bb{B}^\\Tr\\bb{C}) = \\trace(\\bb{C}\\bb{A}\\bb{B}^\\Tr) = \\trace(\\bb{B}^\\Tr\\bb{C}\\bb{A})$(again, as long as the matrix dimensions are such that the products aredefined). Matrix inner product and norms The notion of an inner product can be extended to matrices by thinkingof an $m \\times n$ matrix as of a long vector $m \\times n$ vectorcontaining the matrix elements for example, in the column-stack order.We will denote such a vector as$\\vec(\\bb{A}) = (a_{11},\\dots,a_{m1},a_{12},\\dots,a_{m2},\\dots,a_{1n},\\dots,a_{mn})^\\Tr$.With such an interpretation, we can define the inner product of twomatrices as\\(\\langle \\bb{A}, \\bb{B} \\rangle = \\langle \\vec(\\bb{A}), \\vec(\\bb{B}) \\rangle = \\sum_{i,j} a_{ij} b_{ij}.\\) Exercise. Show that $\\langle \\bb{A}, \\bb{B} \\rangle = \\trace( \\bb{A}^\\Tr \\bb{B} )$. The inner product induces the standard Euclidean norm on the space ofcolumn-stack representation of matrices, \\[\\| \\bb{A} \\| = \\sqrt{ \\langle \\bb{A}, \\bb{A} \\rangle }= \\sqrt{ \\langle \\vec(\\bb{A}), \\vec(\\bb{B}) \\rangle } = \\sqrt{ \\sum_{i,j} a_{ij}^2 },\\]known as the Frobenius norm. Using the result of the exercise above,we can write \\[\\| \\bb{A} \\|_\\mathrm{F}^2 = \\trace( \\bb{A}^\\Tr \\bb{A} ).\\]Note the qualifier $_\\mathrm{F}$ used to distinguish this norm from other matrixnorm that we will define in the sequel. There exist another “standard” way of defining matrix norms by thinkingof an $m \\times n$ matrix $\\bb{A}$ as a linear operator mapping betweentwo normed spaces, say,$\\bb{A} : (\\RR^m,\\ell_p) \\rightarrow (\\RR^n,\\ell_q)$. Then, we candefine the operator norm measuring the maximum change of length (inthe $\\ell_q$ sense) of a unit (in the $\\ell_p$ sense) vector in theoperator domain: \\[\\| \\bb{A} \\|_{p,q} = \\max_{\\| \\bb{u} \\|_p = 1} \\| \\bb{A} \\bb{u} \\|_q.\\]Exercise. Use the axioms of a norm to show that $| \\bb{A} |_{p,q}$ is a norm. Eigendecomposition Eigendecomposition (a.k.a. eigenvalue decomposition or in somecontexts spectral decomposition, from the German eigen for “self”)is doubtlessly the most important and useful forms of matrixfactorization. The following discussion will be valid only for squarematrices. Recall that an $n \\times n$ matrix $\\bb{A}$ represents a linear map on$\\RR^n$. In general, the effect of $\\bb{A}$ on a vector $\\bb{u}$ is anew vector $\\bb{v} = \\bb{A}\\bb{u}$, rotated and elongated or shrunk(and, potentially, reflected). However, there exist vectors which areonly elongated or shrunk by $\\bb{A}$. Such vectors are calledeigenvectors. Formally, an eigenvector of $\\bb{A}$ is a non-zerovector $\\bb{u}$ satisfying $\\bb{A}\\bb{u} = \\lambda \\bb{u}$, with thescalar $\\lambda$ (called eigenvalue) measuring the amount ofelongation or shrinkage of $\\bb{u}$ (if $\\lambda &lt; 0$, the vector isreflected). Note that the scale of an eigenvector has no meaning as itappears on both sides of the equation; for this reason, eigenvectors arealways normalized (in the $\\ell_2$ sense). For reasons not so relevantto our course, the collection of the eigenvalues is called thespectrum of a matrix. For an $n\\times n$ matrix $\\bb{A}$ with $n$ linearly independenteigenvectors we can write the following system \\[\\left\\{    \\begin{array}{lcl}        \\bb{A} \\bb{u}_1  &amp; = &amp; \\lambda_1 \\bb{u}_1  \\\\        \\vdots &amp; &amp;  \\vdots \\\\        \\bb{A} \\bb{u}_n  &amp; = &amp; \\lambda_n \\bb{u}_n \\\\    \\end{array}\\right.\\]Stacking the eigenvectors into the columns ofthe $n \\times n$ matrix $\\bb{U} = (\\bb{u}_1,\\dots,\\bb{u}_n)$, anddefining the diagonal matrix \\[\\bb{\\Lambda} = \\diag\\{\\lambda_1,\\dots,\\lambda_n \\} = \\left(                   \\begin{array}{ccc}                     \\lambda_1 &amp;  &amp;  \\\\                               &amp; \\ddots &amp;  \\\\                               &amp;  &amp; \\lambda_n \\\\                   \\end{array}                   \\right),\\]we can rewrite the system more compactly as$\\bb{A}\\bb{U} = \\bb{U}\\bb{\\Lambda}$. Independent eigenvectors means that$\\bb{U}$ is invertible, which leads to$\\bb{A} = \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1}$. Geometrically,eigendecomposition of a matrix can be interpreted as a change ofcoordinates into a basis, in which the action of a matrix can bedescribed as elongation or shrinkage only (represented by the diagonalmatrix $\\bb{\\Lambda}$). If the matrix $\\bb{A}$ is symmetric, it can be shown that itseigenvectors are orthonormal, i.e.,$\\langle \\bb{u}_i, \\bb{u}_j \\rangle = \\bb{u}_i^\\Tr \\bb{u}_j = 0$ forevery $i \\ne j$ and, since the eigenvectors have unit length,$\\bb{u}_i^\\Tr \\bb{u} = 1$. This can be compactly written as$\\bb{U}^\\Tr \\bb{U} = \\bb{I}$ or, in other words,$\\bb{U}^{-1} = \\bb{U}^\\Tr$. Matrices satisfying this property are calledorthonormal or unitary, and we will say that symmetric matricesadmit unitary eigendecomposition$\\bb{A} = \\bb{U} \\bb{\\Lambda}\\bb{U}^\\Tr$. Exercise. Show that a symmetric matrix admits unitary eigendecomposition$\\bb{A} = \\bb{U} \\bb{\\Lambda}\\bb{U}^\\Tr$. Finally, we note two very simple but useful facts abouteigendecomposition:       $\\displaystyle{\\trace\\,\\bb{A} = \\trace(\\bb{U}\\bb{\\Lambda}\\bb{U}^{-1}) =  \\trace(\\bb{\\Lambda}\\bb{U}^{-1}\\bb{U}) = \\trace\\,\\bb{\\Lambda} = \\sum_{i=1}^n \\lambda_i}$.         $\\displaystyle{\\det\\bb{A} = \\det\\bb{U}\\det\\bb{\\Lambda}\\det\\bb{U}^{-1} =  \\det \\bb{U} \\det \\bb{\\Lambda} \\frac{1}{\\det \\bb{U}} = \\prod_{i=1}^n \\lambda_i}$.   In other words, the trace and the determinant of a matrix are given bythe sum and the product of its eigenvalues, respectively. Matrix functions Eigendecomposition is a very convenient way of performing various matrixoperations. For example, if we are given the eigendecomposition of$\\bb{A} = \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1}$, its inverse can be expressedas$\\bb{A}^{-1} = (\\bb{U}\\bb{\\Lambda}\\bb{U}^{-1})^{-1} = \\bb{U}\\bb{\\Lambda}^{-1}\\bb{U}^{-1}$;however, since $\\bb{\\Lambda}$ is diagonal,$\\bb{\\Lambda}^{-1} =\\diag{1/\\lambda_1,\\dots,1/\\lambda_n}$. (This doesnot suggest, of course, that this is the computationally preferred wayto invert matrices, as the eigendecomposition itself is a costlyoperation). A similar idea can be applied to the square of a matrix:$\\bb{A}^2 = \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1} \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1} = \\bb{U}\\bb{\\Lambda}^2\\bb{U}^{-1}$and, again, we note that$\\bb{\\Lambda}^2 =\\diag{\\lambda_1^2,\\dots,\\lambda_n^2}$. By usinginduction, we can generalize this result to any integer power $p \\ge 0$:$\\bb{A}^p = \\bb{U}\\bb{\\Lambda}^p\\bb{U}^{-1}$. (Here, if, say, $p=1000$,the computational advantage of using eigendecomposition might be welljustified). Going one step further, let \\[\\varphi(t) = \\sum_{i \\ge 0} c_i t^i\\]be a polynomial (either of a finite degree or an infinite series). We can applythis function to a square matrix $\\bb{A}$ as follows: \\[\\begin{aligned}\\varphi(\\bb{A}) &amp;=&amp; \\sum_{i \\ge 0} c_i \\bb{A}^i = \\sum_{i \\ge 0} c_i \\bb{U}\\bb{\\Lambda}^p\\bb{U}^{-1} \\nonumber\\\\&amp;=&amp; \\bb{U} \\left( \\sum_{i \\ge 0} c_i  \\bb{\\Lambda}^p \\right) \\bb{U}^{-1} =\\bb{U} \\left( \\sum_{i \\ge 0}  \\diag\\{c_i \\lambda_1^i,\\dots,c_i \\lambda_n^i\\} \\right) \\bb{U}^{-1} \\nonumber\\\\&amp;=&amp; \\bb{U} \\left(                   \\begin{array}{ccc}                    \\sum_{i \\ge 0} c_i \\lambda_1^i &amp;  &amp;  \\\\                               &amp; \\ddots &amp;  \\\\                               &amp;  &amp; \\sum_{i \\ge 0} c_i \\lambda_n^i \\\\                   \\end{array}                 \\right)  \\bb{U}^{-1} \\nonumber\\\\                 &amp;=&amp; \\bb{U} \\diag\\{\\varphi(\\lambda_1),\\dots,\\varphi(\\lambda_n)\\} \\bb{U}^{-1}.\\end{aligned}\\]Denoting by$\\varphi(\\bb{\\Lambda}) = \\diag{\\varphi(\\lambda_1),\\dots,\\varphi(\\lambda_n)}$the diagonal matrix formed by the element-wise application of the scalarfunction $\\varphi$ to the eigenvalues of $\\bb{A}$, we can writecompactly \\[\\varphi(\\bb{A}) = \\bb{U} \\varphi(\\bb{\\Lambda}) \\bb{U}^{-1}.\\]Finally, since many functions can be described polynomial series, we cangeneralize the latter definition to a (more or less) arbitrary scalar function$\\varphi$. The above procedure is a standard way of constructing a matrix function (thisterm is admittedly confusing, as we will see it assuming another meaning); forexample, matrix exponential and logarithm are constructed exactly like this.Note that the construction is sharply different from applying the function$\\varphi$ element-wise! Positive definite matrices Symmetric square matrices define an important family of functions calledquadratic forms that we will encounter very often in this course.Formally, a quadratic form is a scalar function on $\\RR^n$ given by\\(\\bb{x}^\\Tr \\bb{A} \\bb{x} = \\sum_{i,j=1}^n a_{ij} x_i x_j,\\) where$\\bb{A}$ is a symmetric $n \\times n$ matrix, and $\\bb{x} \\in \\RR^n$. A symmetric square matrix $\\bb{A}$ is called positive definite(denoted as $\\bb{A} \\succ 0$) iff for every $\\bb{x} \\ne \\bb{0}$,$\\bb{x}^\\Tr \\bb{A} \\bb{x} &gt; 0$. The matrix is called positivesemi-definite (denoted as $\\bb{A} \\succeq 0$) if the inequality isweak. Positive (semi-) definite matrices can be equivalently defined throughtheir eigendecomposition: Let $\\bb{A}$ be a symmetric matrix admitting the eigendecomposition$\\bb{A} = \\bb{U}\\bb{\\Lambda}\\bb{U}^\\Tr$. Then $\\bb{A} \\succ 0$ iff$\\lambda_i &gt; 0$ for $i=1,\\dots,n$. Similarly, $\\bb{A} \\succeq 0$ iff$\\lambda_i \\ge 0$. In other words, the matrix is positive (semi-) definite if it haspositive (non-negative) spectrum. To get a hint why this is true,consider an arbitrary vector $\\bb{x} \\ne \\bb{0}$, and write \\[\\bb{x}^\\Tr \\bb{A} \\bb{x} =\\bb{x}^\\Tr \\bb{U}\\bb{\\Lambda}\\bb{U}^\\Tr \\bb{x} =(\\bb{U}^\\Tr \\bb{x})^\\Tr \\bb{\\Lambda}(\\bb{U}^\\Tr \\bb{x})\\]Denoting $\\bb{y} = \\bb{U}^\\Tr \\bb{x}$, we have \\[\\bb{x}^\\Tr \\bb{A} \\bb{x}  = \\bb{y}^\\Tr \\bb{\\Lambda} \\bb{y} = \\sum_{i=1}^n \\lambda_i y_i^2.\\]Since $\\bb{U}^\\Tr$ is full rank, the vector $\\bb{y}$ is also anarbitrary non-zero vector in $\\RR^n$ and the only way to make the lattersum always positive is by ensuring that all $\\lambda_i$ are positive.The very same reasoning is also true in the opposite direction. Geometrically, a quadratic form describes a second-order (hence the namequadratic) surface in $\\RR^n$, and the eigenvalues of the matrix$\\bb{A}$ can be interpreted as the surface curvature. Very informally,if a certain eigenvalue $\\lambda_i$ is positive, a small step in thedirection of the corresponding eigenvector $\\bb{u}_i$ rotates the normalto the surface in the same direction. The surface is said to havepositive curvature in that direction. Similarly, a negative eigenvaluecorresponds to the normal rotating in the opposite direction of the step(negative curvature). Finally, if $\\lambda_i = 0$, a step in thedirection of $\\bb{u}_i$ leave the normal unchanged (the surface is saidto be flat in that direction). A quadratic form created by a positivedefinite matrix represents a positively curved surface in alldirections. Such a surface is cup-shaped (if you can imagine an$n$-dimensional cup) or, formally, is convex; in the sequel, we willsee the important consequences this property has on optimizationproblems.             We will henceforth abbreviate “if and only if” as “iff”. Twostatements related by “iff” are equivalent; for example, if one ofthe statements is a definition of some object, and the other is itsproperty, the latter property can be used as an alternativedefinition. We will see many such examples. &#8617;               We will often abbreviate “with respect to” as “w.r.t.” &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/supplements/linear_algebra/",
        "teaser":null},{
        "title": "Multivariate Calculus",
        "excerpt":"Introduction The purpose of this document is to quickly refresh (presumably) knownnotions in multivariate differential calculus such as differentials,directional derivatives, the gradient and the Hessian. These notionswill be used heavily in our course. Even though this quick reminder mayseem redundant or trivial to most of you (I hope), I still suggest atleast to skim through it, as it might present less common ways ofinterpretation of very familiar definitions and properties. And even ifyou discover nothing new in this document, it will at least be useful tointroduce notation. Notation In our course, we will deal exclusively with real functions. A scalarfunction will be denoted as $f : \\RR^n \\rightarrow \\RR$, $f(\\bb{x})$, orsimply $f$. A vector-valued function will be denoted in bold, as$\\bb{f} : \\RR^n \\rightarrow \\RR^m$, or component-wise as$\\bb{f}(\\bb{x}) = (f_1(\\bb{x}), \\dots, f_m(\\bb{x}))^\\Tr$. A scalarfunction of a matrix variable, $f : \\RR^{m \\times n} \\rightarrow \\RR$,will be denoted as $f(\\bb{A})$, and a matrix-valued function of avector, $f : \\RR^n \\rightarrow \\RR^{m \\times k}$ as $\\bb{F}(\\bb{x})$.Derivatives of a scalar function of one variable will be denoted as$f’(x)$, $f’‘(x)$, etc. An $n$-times continuously differentiablefunction will be said $\\mathcal{C}^n$ ($f \\in \\mathcal{C}^n$). In mostcases, we will tacitly assume that a function is sufficiently smooth forat least the first-order derivative to exist. First-order derivative of a function of one variable Before proceeding to multivariate functions, let us remind ourselves a few basicnotions of univariate calculus. A $\\mathcal{C}^1$ function $f(x)$ can beapproximated linearly around some point $x=x_0$.  Incrementing the argument by$dx$, the function itself changes by the amount that we denote by$\\Delta f = f(x_0+dx) - f(x_0)$, while the linear approximation changes by theamount denoted by $df$. For a sufficiently small $dx$ (more formally, in thelimit $|dx| \\rightarrow 0$), it can be shown that $\\Delta f = df + o(dx)$1.This means that for an infinitesimally small increment $dx$, the linearapproximation of the function becomes exact. In this limit, $df$ is called thedifferential of $f$, and the slope of the linear approximation, is called thefirst-order derivative of $f$, denoted $\\displaystyle{\\frac{df}{dx} =f’(x_0)}$.  Another way to express this fact is through the first-order Taylorexpansion of $f$ around $x_0$: \\[f(x_0+dx) = f(x_0) + f'(x_0) dx + O(dx^2),\\]which essentially says that a linear function whose value at $x_0$ matches thatof $f(x_0)$, and whose slope matches that of $f$ (expressed by $f’(x_0)$)approximates $f$ around $x_0$ up to some second-order error. Gradient We can extend the previous discussion straightforwardly to the$n$-dimensional case. Let $f$ now be a $\\mathcal{C}^1$ function on$\\RR^n$. The surface the function creates in $\\RR^{n+1}$ can beapproximated by an $n$-dimensional tangent plane (the multidimensionalanalog of linear approximation). Fixing a point $\\bb{x}_0$ and making asmall step $\\dx = (dx_1,\\dots,dx_n)^\\Tr$ (note that now $\\dx$ is avector), it can be shown that the change in the value of the linearapproximation is given by \\[df = \\frac{\\partial f}{\\partial x_1} dx_1 + \\cdots + \\frac{\\partial f}{\\partial x_n} dx_n,\\]where $\\frac{\\partial f}{\\partial x_i}$ denotes the partial derivativeof $f$ at $\\bb{x}_0$. The latter formula is usually known as the totaldifferential. Arranging the partial derivatives into a vector$\\displaystyle{\\bb{g} = \\left( \\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)^\\Tr }$,the total differential can be expressed as the inner product$df = \\langle \\bb{g}, \\dx \\rangle$. The object $\\bb{g}$ appearing in theinner product is called the gradient of $f$ at point $\\bb{x}_0$, andwill be denoted by $\\nabla f (\\bb{x}_0)$ (the symbol $\\nabla$,graphically a rotated capital Delta, is pronounced “nabla”, from thegrecized Hebrew “nevel” for “harp”; $\\nabla$ is sometimes called thedel operator). While we can simply define the gradient as the vectorof partial derivatives, we will see that the definition through theinner product can often be more useful. Directional derivative In this course, we will often encounter situations where we areinterested in the behavior of a function along a line (formally, we saythat $f(\\bb{x})$ is restricted to the one-dimensional linear subspace$\\mathcal{L} = { \\bb{x}_0 + \\alpha \\bb{r} : \\alpha \\in \\RR }$, where$\\bb{x}_0$ is some fixed point, and $\\bb{r}$ is a fixed direction). Letuse define a new function of a single variable $\\alpha$,$\\varphi(\\alpha) = f(\\bb{x}_0 + \\alpha \\bb{r})$. Note that we can findthe first-order derivative of $\\varphi$, arriving at the followingimportant notion: \\[f_{\\bb{r}}'(\\bb{x}_0) = \\left. \\frac{d}{d\\alpha} f(\\bb{x}_0 + \\alpha \\bb{r}) \\right|_{\\alpha=0}  =\\varphi'(0)\\]which is called the directional derivative of $f$ at $\\bb{x}_0$ in thedirection $\\bb{r}$. The same way a derivative measures the rate of change of a function, adirectional derivative measures the rate of change of a multivariatefunction when we make a small step in a particular direction. Denoting $\\bb{g} = \\nabla f(\\bb{x}_0)$ and using our definition of thegradient as the inner product, we can write \\[d\\varphi = df = \\bb{g}^\\Tr\\dx = \\bb{g}^\\Tr (d\\alpha \\bb{r}) = d\\alpha (\\bb{g}^\\Tr \\bb{r}).\\]Identifying in the latter quantity an inner product of $d\\alpha$ withthe scalar $\\bb{g}^\\Tr \\bb{r}$, we can say that $\\bb{g}^\\Tr \\bb{r}$ isthe gradient of $\\varphi(\\alpha)$ at $\\alpha=0$, which coincides withthe first-order derivative, $\\varphi’(0) = \\bb{g}^\\Tr \\bb{r}$, as$\\varphi$ is a function of a single variable. We can summarize thisresult as the following: Property. The directional derivative of $f$ at $\\bb{x}_ {0}$ in the direction$\\bb{r}$ is obtained by projecting the gradient at $\\bb{x}_ {0}$ onto thedirection $\\bb{r}$, $f’_ {\\bb{r}} = {\\bb{r}}^\\Tr \\nabla f(\\bb{x}_ {0})$. Hessian In the case of a function of a single variable, we saw that thedifferential of $f$ was given by $df = f’(x) dx$. However, thefirst-order derivative $f’(x)$ is also a function of $x$, and we canagain express its differential as $df’ = f’‘(x) dx$, where $f’‘(x)$denotes the second-order derivative. This notion can be extended to themultivariate case. Recall our definition of the gradient through theinner product, \\(df = \\bb{g}^\\Tr \\dx.\\) Thinking of the gradient as of avector-valued function on $\\RR^n$,$\\bb{g}(\\bb{x}) = (g_1(\\bb{x}),\\dots,g_n(\\bb{x}))^\\Tr$, we can write \\[\\left\\{ \\begin{array}{ccc}  dg_1 &amp; = &amp; \\bb{h}^\\Tr_1 \\dx \\\\  \\vdots &amp;   &amp; \\vdots \\\\  dg_n &amp; = &amp; \\bb{h}^\\Tr_n \\dx,   \\end{array}\\right.\\]with each $\\bb{h}_i$ being the gradient of the $i$-thcomponent of the gradient vector $\\bb{g}$, \\[\\bb{h}_i = \\left( \\frac{\\partial g_i }{\\partial x_1}, \\dots, \\frac{\\partial g_i }{\\partial x_n} \\right)^\\Tr =\\left( \\frac{\\partial^2 f }{\\partial x_1 \\partial x_i}, \\dots, \\frac{\\partial^2 g_i }{\\partial x_n \\partial x_i} \\right)^\\Tr.\\]Denoting by $\\bb{H} = (\\bb{h}_ 1,\\dots,\\bb{h}_ n)$, we can write compactly$\\dg = \\bb{H}^\\Tr \\bb{dx}$. The $n\\times n$ matrix $\\bb{H}$ containingall the second-order partial derivatives of $f$ as its elements iscalled the Hessian of $f$ at point $\\bb{x}$, and is also denoted2as $\\nabla^2 f(\\bb{x})$. We tacitly assumed that $f$ is $\\mathcal{C}^2$in order for the second-order derivatives to exist. A nice property of$\\mathcal{C}^2$ functions is that partial derivation is commutative,meaning that the order of taking second-order partial derivatives can beinterchanged:$\\displaystyle{h_{ij} = \\frac{\\partial^2 f }{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} = h_{ji} }$.Algebraically, this implies that the Hessian matrix is symmetric, and wecan write \\[\\dg = \\bb{H} \\bb{dx}.\\]Second-order directional derivative Recall that we have previously considered the restriction of amultivariate function $f$ to a line,$\\varphi(\\alpha) = f(\\bb{x}_ 0 + \\alpha \\bb{r})$. This gave rise to thefirst-order directional derivative $f_{\\bb{r}}(\\bb{x}_ 0) = \\varphi’(0)$.In a similar way, we define the second-order directional derivative at$\\bb{x}_ 0$ in the direction $\\bb{r}$ as \\[\\begin{aligned}f''_{\\bb{rr}}(\\bb{x}_0) &amp;=&amp; \\varphi''(0) = \\left. \\frac{d^2}{d\\alpha^2} f(\\bb{x}_0 + \\alpha\\bb{r}) \\right|_{\\alpha=0}= \\left. \\frac{d}{d\\alpha} f'_{\\bb{r}}(\\bb{x}_0 + \\alpha\\bb{r}) \\right|_{\\alpha=0}.\\end{aligned}\\]Considering $f’_{\\bb{r}}(\\bb{x}) = \\bb{r}^\\Tr \\bb{g}(\\bb{x})$ as afunction of $\\bb{x}$, we can write its differential as \\[df'_{\\bb{r}} = \\bb{r}^\\Tr \\dg = \\bb{r}^\\Tr \\bb{H}(\\bb{x}_0) \\dx = \\bb{r}^\\Tr \\bb{H}(\\bb{x}_0) \\bb{r} d\\alpha,\\]from where \\[f''_{\\bb{rr}} = \\bb{r}^\\Tr \\bb{H} \\bb{r}.\\]In other words, in order to get the second-order directional derivative in thedirection $\\bb{r}$, one has to evaluate the quadratic form $\\bb{r}^\\Tr \\bb{H}\\bb{r}$. Derivatives of linear and quadratic functions Let $\\bb{y} = \\bb{A}\\bb{x}$ be a general linear operator defined by an$m \\times n$ matrix. Its differential is given straightforwardly by \\[\\dy = \\bb{A}(\\bb{x} + \\dx) - \\bb{A}\\bb{x} = \\bb{A}\\dx.\\]Using this result, we will do a small exercise deriving gradients and Hessiansof linear and quadratic functions. As we will see, it is often convenient tostart with evaluating the differential of a function. Our first example is a linear function of the form$f(\\bb{x}) = \\bb{b}^\\Tr \\bb{x}$, where $\\bb{b}$ is a constant vector.Note that this function is a particular case of the previous result(with $\\bb{A} = \\bb{b}^\\Tr$), and we can write $df = \\bb{b}^\\Tr \\dx$.Comparing this to the general definition of the gradient,$df = \\bb{g}^\\Tr(\\bb{x}) \\dx$, we deduce that the gradient of $f$ isgiven by $\\nabla f(\\bb{x}) = \\bb{b}$. Note that the gradient of a linearfunction is constant – this generalizes the case of a linear function ofone variable, $f(x)= bx$, which has a constant derivative $f’(x) = b$. Our second example is a quadratic function of the form$f(\\bb{x}) = \\bb{x}^\\Tr \\bb{A} \\bb{x}$, where $\\bb{A}$ is an$n \\times n$ matrix. We again compute the differential by definition, \\[\\begin{aligned}df &amp;=&amp; f(\\bb{x}+\\dx) -f(\\bb{x}) = (\\bb{x}+\\dx)^\\Tr \\bb{A} (\\bb{x}+\\dx)- \\bb{x}^\\Tr \\bb{A} \\bb{x} \\nonumber\\\\   &amp;=&amp; \\bb{x}^\\Tr \\bb{A} \\bb{x} + \\dx^\\Tr \\bb{A} \\bb{x} + \\bb{x}^\\Tr \\bb{A} \\dx + \\dx^\\Tr \\bb{A} \\dx - \\bb{x}^\\Tr \\bb{A} \\bb{x} \\nonumber\\\\   &amp;=&amp; \\dx^\\Tr \\bb{A} \\bb{x} + \\bb{x}^\\Tr \\bb{A} \\dx + \\dx^\\Tr \\bb{A} \\dx.\\end{aligned}\\]Note that in the limit $| \\dx | \\rightarrow 0$, the third term(quadratic in $|\\dx|$) goes to zero much faster than the first twoterms (linear in $\\dx$), and can be therefore neglected3, leading to \\[df = \\dx^\\Tr \\bb{A} \\bb{x} + \\bb{x}^\\Tr \\bb{A} \\dx =\\dx^\\Tr \\bb{A} \\bb{x} + (\\bb{x}^\\Tr \\bb{A} \\dx)^\\Tr =\\dx^\\Tr(\\bb{A}^\\Tr + \\bb{A})\\bb{x}.\\]Again, recognizing in the latter expression an inner product with $\\dx$,we conclude that $\\nabla f(\\bb{x}) = (\\bb{A}^\\Tr + \\bb{A})\\bb{x}$. For asymmetric $\\bb{A}$, the latter simplifies to$\\nabla f(\\bb{x}) = 2\\bb{A} \\bb{x}$. Note that the gradient of aquadratic function is a linear function; furthermore, the latterexpression generalizes the univariate quadratic function $f(x) = ax^2$,whose first-order derivative $f’(x) = 2ax$ is linear. Since the gradient $\\bb{g}(\\bb{x}) = (\\bb{A}^\\Tr +\\bb{A})\\bb{x}$ of thequadratic function is linear, its differential is immediately given by$\\dg = (\\bb{A}^\\Tr +\\bb{A})\\dx$, from where we conclude that the Hessianof $f$ is $\\bb{H}(\\bb{x}) = \\bb{A}^\\Tr +\\bb{A}$ (or $2\\bb{A}$ in thesymmetric case). Note that the Hessian of a quadratic function isconstant, which coincides with the univariate case $f’’ (x) = 2a$. In the sequel, we will see more complicated examples of gradients andHessians. For a comprehensive reference on derivatives of matrix andvector expressions, the Matrix Cookbook4 is highly advisable. Multivariate Taylor expansion We have seen the Taylor expansion of a function of one variable as a wayto obtain a linear approximation. This construction can be generalizedto the multivariate case, as we show here, limiting the expansion tosecond order. Theorem: Second-order Taylor expansion.Let $f$ be a $\\mathcal{C}^2$ function on $\\RR^n$, $\\bb{x}$ some point,and $\\bb{r}$ a sufficient small vector. Then, \\[f(\\bb{x}+\\bb{r}) =f(\\bb{x}) + \\bb{g}^\\Tr (\\bb{x}) \\bb{r} + \\frac{1}{2} \\bb{r}^\\Tr \\bb{H}(\\bb{x}) \\bb{r} + O(\\|\\bb{r}\\|^3).\\]The theorem say that up to a third-order error term, the function can beapproximated around $\\bb{x}$ by a quadratic function$q(\\bb{r}) = f + \\bb{g}^\\Tr \\bb{r} + \\frac{1}{2} \\bb{r}^\\Tr \\bb{H} \\bb{r}$(note that the function is quadratic in $\\bb{r}$, as $\\bb{x}$ isconstant, and so are $f=f(\\bb{x})$, $\\bb{g}$, and $\\bb{H}$). Out of allpossible quadratic approximations of $f$, the approximation described by$q(\\bb{r}) \\approx f(\\bb{x} + \\bb{r})$ is such that its value, slope,and curvature at $\\bb{x}$ (equivalently, at $\\bb{r} = \\bb{0}$) matchthose of $f$. The latter geometric quantities are captured,respectively, by the values of the function, its gradient, and itsHessian; in order to match the value, slope, and curvature of $f$, $q$has to satisfy $q(\\bb{0}) = f(\\bb{x})$,$\\nabla q(\\bb{0}) = \\nabla f(\\bb{x})$, and$\\nabla^2 q(\\bb{0}) = \\nabla^2 f(\\bb{x})$ (note that the gradient andthe Hessian of $q$ are w.r.t $\\bb{r}$, whereas the derivatives of $f$are w.r.t. $\\bb{x}$). To see that the later equalities hold, we firstobserve that $q(\\bb{0}) = f(\\bb{x})$. Next, using the fact that$q(\\bb{r})$ is quadratic, its gradient and Hessian (w.r.t. $\\bb{r}$) aregiven by $\\nabla q(\\bb{r}) = \\bb{g} + \\bb{H} \\bb{r}$ and$\\nabla^2 q(\\bb{r}) = \\bb{H} \\bb{r}$. Substituting $\\bb{r} = \\bb{0}$yields $\\nabla q(\\bb{0}) = \\bb{g}$ and $\\nabla^2 q(\\bb{r}) = \\bb{H}$. Gradient of a function of a matrix The notion of gradient can be generalized to functions of matrices. Let$f : \\RR^{m \\times n} \\rightarrow \\RR$ be such function evaluated atsome $\\bb{X}$. We can think of an equivalent function on $\\RR^{mn}$evaluated at $\\bb{x} = \\vec(\\bb{X})$, for which the gradient is definedsimply as the $mn$-dimensional vector of all partial derivatives. We cantherefore think of the gradient of $f(\\bb{X})$ at $\\bb{X}$ as of the$m \\times n$ matrix \\[\\bb{G}(\\bb{X}) = \\left(                   \\begin{array}{ccc}                     \\frac{\\partial f }{\\partial x_{11} } &amp; \\cdots &amp; \\frac{\\partial f }{\\partial x_{1n} } \\\\                     \\vdots &amp; \\ddots &amp; \\vdots \\\\                     \\frac{\\partial f }{\\partial x_{m1} } &amp; \\cdots &amp; \\frac{\\partial f }{\\partial x_{mn} } \\\\                   \\end{array}                 \\right).\\]Previously, we have seen that an “external” definition of the gradientthrough an inner product is often more useful. Such a definition is alsovalid for matrix arguments. Recall our definition of the standard innerproduct on the space of $m\\times n$ matrices as\\(\\langle \\bb{A}, \\bb{B} \\rangle = \\sum_{ij} a_{ij} b_{ij} = \\trace(\\bb{A}^\\Tr \\bb{B}),\\)for $\\bb{A},\\bb{B} \\in \\RR^{m \\times n}$. Using the total differentialformula yields \\[df = \\sum_{ij} \\frac{\\partial f }{\\partial x_{ij}} dx_{ij} = \\langle \\bb{G}, \\dX \\rangle,\\]where $\\dX$ is now an $m\\times n$ matrix. The matrix $\\bb{G}$ appearingin the above identity can be defined as the gradient of $f$. Gradient of a nonlinear function We finish this brief introduction by deriving the gradient of a morecomplicated function of the form \\[f(\\bb{X}) = \\bb{c}^\\Tr \\varphi( \\bb{X}^\\Tr \\bb{a} + \\bb{b}),\\]where$\\bb{X} \\in \\RR^{m\\times n}$, $\\bb{a} \\in \\RR^n$,$\\bb{b},\\bb{c} \\in \\RR^m$, and $\\varphi$ is a $\\mathcal{C}^1$ functionapplied element-wise. We will encounter such functions during the coursewhen dealing with nonlinear regression and classification applications.In machine learning, functions of this form constitute buildingblocks of more complicated functions called artificial neural networks.As before, we proceed by computing differentials and using the chainrule. Denoting $\\bb{u} = \\bb{X}^\\Tr \\bb{a} + \\bb{b}$, we have \\[\\varphi(\\bb{u}) = \\left(                    \\begin{array}{c}                      \\varphi(u_1) \\\\                      \\vdots \\\\                      \\varphi(u_m) \\\\                    \\end{array}                  \\right).\\]Since $\\varphi$ is applied element-wise to$\\bb{u}$, the differential of $\\bb{\\varphi} = \\varphi(\\bb{u})$ is givenby \\[\\dphi = \\left(                    \\begin{array}{c}                      \\varphi'(u_1) du_1\\\\                      \\vdots \\\\                      \\varphi'(u_m) du_m \\\\                    \\end{array}                  \\right) =                   \\underbrace{\\left(                    \\begin{array}{ccc}                      \\varphi'(u_1) &amp;  &amp;  \\\\                       &amp; \\ddots &amp;  \\\\                       &amp;  &amp; \\varphi'(u_m) \\\\                    \\end{array}                  \\right)}_{\\bb{\\Phi}'} \\du = \\bb{\\Phi}' \\du.\\]Next, we consider the function $\\bb{u}(\\bb{X}) = \\bb{X}^\\Tr \\bb{a} + \\bb{b}$;since it is linear in $\\bb{X}$, its differential is given by $\\du = \\dX^\\Tr\\bb{a}$. Finally, we consider the function $f(\\bb{\\varphi}) = \\bb{c}^\\Tr\\bb{\\varphi}$, which is linear in $\\bb{\\varphi}$ and has the differential$df = \\bb{c}^\\Tr \\dphi$. Combining these results and using simple properties of the matrix traceyields \\[\\begin{aligned}df &amp;=&amp; \\bb{c}^\\Tr \\dphi = \\bb{c}^\\Tr \\bb{\\Phi}' \\du = \\bb{c}^\\Tr\\bb{\\Phi}' \\dX^\\Tr \\bb{a} \\\\&amp;=&amp; \\trace\\left( \\bb{c}^\\Tr\\bb{\\Phi}' \\dX^\\Tr \\bb{a} \\right) = \\trace\\left( \\dX^\\Tr \\bb{a}\\bb{c}^\\Tr\\bb{\\Phi}'  \\right) \\\\&amp;=&amp; \\langle \\dX, \\bb{a}\\bb{c}^\\Tr\\bb{\\Phi}'\\rangle.\\end{aligned}\\]In the latter expression, we recognize in the second argument of the innerproduct the gradient of $f$ w.r.t. $\\bb{X}$, \\[\\nabla f(\\bb{X}) = \\bb{a}\\bb{c}^\\Tr\\bb{\\Phi}'.\\]            The little-$o$ notation means that there exists some function of$dx$, $o(dx)$, going faster to zero than $dx$ (i.e.,$\\displaystyle{\\frac{o(dx)}{dx}} \\rightarrow 0$), but the exact formof this function is unimportant. On the other hand, the big-$O$notation, as in $O(dx^2)$, stands for some function that grows withthe same rate as $dx^2$ (i.e.,$\\displaystyle{\\lim_{|dx|\\rightarrow 0} \\frac{dx^2}{O(dx^2)} &lt; \\infty }$). &#8617;               Some people find the following abuse of notation helpful: Thinkingof the gradient of $f$ as of a differential operator of the form“$\\displaystyle{\\nabla = \\left(                            \\begin{array}{c}                            \\frac{\\partial }{\\partial x_1}                             \\vdots                             \\frac{\\partial }{\\partial x_n}                             \\end{array}                        \\right)}$”applied to $f$, the Hessian can be expressed by applying theoperator “$\\displaystyle{\\nabla^2 = \\nabla \\nabla^\\Tr =\\left(    \\begin{array}{c}    \\frac{\\partial }{\\partial x_1}     \\vdots     \\frac{\\partial }{\\partial x_n}     \\end{array}\\right)\\left(\\textstyle{    \\frac{\\partial }{\\partial x_1}},\\dots, \\textstyle{\\frac{\\partial }{\\partial x_n}}\\right) =\\left(    \\begin{array}{ccc}        \\frac{\\partial^2 }{\\partial x_1 \\partial x_1} &amp; \\cdots &amp; \\frac{\\partial^2 }{\\partial x_1 \\partial x_n}         \\vdots &amp;  \\ddots &amp; \\vdots         \\frac{\\partial^2 }{\\partial x_n \\partial x_1} &amp; \\cdots &amp; \\frac{\\partial^2 }{\\partial x_n \\partial x_n}     \\end{array}\\right)}$”. &#8617;               This “explanation” can be written rigorously using limits. Anotherway of getting the same result is the well-known rule of“differential of a product”, $d(fg) = df\\, g + f \\, dg$, which canbe generalized to the multivariate case as follows: Let $h$ be ascalar function given as the inner product of two vector-valuedfunctions, $h(\\bb{x}) = \\bb{f}^\\Tr(\\bb{x}) \\bb{g}(\\bb{x})$. Then,$dh = \\df^\\Tr \\bb{g} + \\bb{f}^\\Tr \\dg$. &#8617;               http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/supplements/multivariate_calculus/",
        "teaser":null},{
        "title": "Probability and statistics: a survival guide",
        "excerpt":"Random variables Probability measure We start with a few elementary (and simplified) definitions from thetheory of probability. Let us fix a sample space $\\Omega = [0,1]$. ABorel set on $\\Omega$ is a set that can be formed from open intervalsof the form $(a,b), 0 \\le a&lt;b \\le 1$, through the operations ofcountable union, countable intersection, and set difference. We willdenote the collection of all Borel sets in $\\Omega$ as $\\Sigma$. It ispretty straightforward to show that $\\Sigma$ contains the empty set, isclosed under complement, and is closed under countable union. Such a setis known as $\\sigma$-algebra and its elements (subsets of$\\mathbb{R}$) are referred to as events. A probability measure $P$ on $\\Sigma$ is a function$P : \\Sigma \\rightarrow [0,1]$ satisfying $P(\\emptyset) = 0$,$P(\\mathbb{R}) = 1$ and additivity for every countable collection${ E _n \\in \\Sigma }$, \\[P\\left(  \\bigcup _n E _n \\right) = \\sum _{n} P(E _n).\\]Random variables A random variable $\\mathpzc{X}$ is a measurable map$\\mathpzc{X} : \\Omega \\rightarrow \\mathbb{R}$, i.e., a function suchthat for every $a$,${ \\mathpzc{X} \\le a } = { \\alpha : \\mathpzc{X}(\\alpha) \\le a  } \\in \\Sigma$.The map $\\mathpzc{X}$ pushes forward the probability measure $P$; thepushforward measure $\\mathpzc{X} _\\ast P$ is given by \\[(\\mathpzc{X} _\\ast P)(A) = P(\\mathpzc{X}^{-1}(A)),\\]where$\\mathpzc{X}^{-1}(A) = { \\alpha  : X(\\alpha) \\in A }$ is the preimageof $A \\subseteq \\mathbb{R}$. (In short, we can write$\\mathpzc{X} _\\ast P = P\\mathpzc{X}^{-1}$). This pushforward probabilitymeasure $\\mathpzc{X} _\\ast P$ is usually referred to as the probabilitydistribution (or the law) of $\\mathpzc{X}$. When the range of $\\mathpzc{X}$ is finite or countably infinite, therandom variable is called discrete and its distribution can bedescribed by the probability mass function (PMF): \\[f _{\\mathpzc{X}}(x) = P(\\mathpzc{X}=x),\\]which is a shorthand for$P(  {\\alpha : \\mathpzc{X}(\\alpha) = x } )$. Otherwise, $\\mathpzc{X}$is called a continuous random variable. Any random variable can bedescribed by the cumulative distribution function (CDF) \\[F _{\\mathpzc{X}}(x) = P({\\mathpzc{X} \\le x}),\\]which is a shorthandfor$F _{\\mathpzc{X}}(x) = P(  {\\alpha : \\mathpzc{X}(\\alpha) \\le x } )$. If$X$ is absolutely continuous, the CDF can be described by the integral \\[F _{\\mathpzc{X}}(x) = \\int _{-\\infty}^x f _{\\mathpzc{X}}(x') dx',\\]wherethe integrand $f _{\\mathpzc{X}}$ is known as the probability densityfunction (PDF)1. Uniform distribution and uniformization A random variable $\\mathpzc{U}$ is said to be uniformly distributed on$[0,1]$ (denoted as $\\mathpzc{U} \\sim \\mathcal{U}[0,1]$) if \\[P(\\mathpzc{U} \\in [a,b]) = b-a = \\lambda([a,b]).\\]In other words, themap $\\mathpzc{U}$ pushes forward the standard Lebesgue measure on$[0,1]$, $\\mathpzc{U} _\\ast P = \\lambda$. The corresponding CDF is$F _\\mathpzc{U}(u) = \\max{ 0, \\min{ 1, u } }$. Let $\\mathpzc{X}$ besome other random variable characterized by the CDF $F _\\mathpzc{X}$. Wedefine $\\mathpzc{U} = F _\\mathpzc{X}(\\mathpzc{X})$. Let us pick anarbitrary $x \\in \\mathbb{R}$ and let $u = F _\\mathpzc{X}(x) \\in [0,1]$.From monotonicity of the CDF, it follows that $\\mathpzc{U} \\le u$ if andonly if $\\mathpzc{X} \\le x$. Hence,$F _\\mathpzc{U}(u) = P(\\mathpzc{U} \\le u) = P(\\mathpzc{X} \\le x) = F _\\mathpzc{X}(x) = u$.We conclude that by transforming a random variable with its own CDFuniformizes it on the interval $[0,1]$. Applying the relation in inverse direction, let$\\mathpzc{U} \\sim \\mathcal{U}[0,1]$ and let $F$ be a valid CDF. Then,the random variable $\\mathpzc{X} = F^{-1}(\\mathpzc{U})$ is distributedwith the CDF $F _\\mathpzc{U} = F$. Expectation The expected value (a.k.a. the expectation or mean) of a randomvariable $\\mathpzc{X}$ is given by \\[\\mathbb{E} \\mathpzc{X} = \\int _{\\mathbb{R}} \\mathrm{id}\\, d(\\mathpzc{X} _\\ast P) =  \\int _{\\Omega} \\mathpzc{X}(\\alpha) d\\alpha,\\]where the integral is the Lebesgue integral w.r.t. the measure $P$;whenever a probability density function exists, the latter can bewritten as \\[\\mathbb{E} \\mathpzc{X} = \\int _{\\mathbb{R}} x  f _{\\mathpzc{X}}(x) dx.\\]Note that due to the linearity of integration, the expectation operator$\\mathbb{E}$ is linear. Using the Lebesgue integral notation, we canwrite for $E \\in \\Sigma$ \\[P(E) = \\int _E dP = \\int _\\mathbb{R} \\ind _E \\, dP = \\mathbb{E}  \\ind _E,\\]where \\[\\ind _E(\\alpha) = \\left\\{ \\begin{array}{ccc} 1 &amp; : &amp; \\alpha \\in E \\\\ 0 &amp; : &amp; \\mathrm{otherwise} \\end{array}   \\right.\\]is the indicator function of $E$, which is by itself arandom variable. This relates the expectation of the indicator of anevent to its probability. Moments For any measurable function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$,$\\mathpzc{Z} = g(\\mathpzc{X})$ is also a random variable with theexpectation \\[\\mathbb{E} \\mathpzc{Z} = \\mathbb{E} g(\\mathpzc{X}) =  \\int _{\\mathbb{R}} g\\, dP =  \\int _{\\mathbb{R}} g(x) f _{\\mathpzc{X}}(x) dx.\\]Such an expectation is called a moment of $\\mathpzc{X}$. Particularly,the $k$-th order moment is obtained by setting $g(x) = x^k$, \\[\\mu _{k}(\\mathpzc{X}) = \\mathbb{E} \\mathpzc{X}^k.\\]The expected valueitself is the first-order moment of $\\mathpzc{X}$, which is oftendenoted simply as $\\mu _\\mathpzc{X} = \\mu _{1}(\\mathpzc{X})$. Thecentral $k$-th order moment is obtained by setting$g(x) = (x - \\mu _\\mathpzc{X})^k$, \\[m _{k}(\\mathpzc{X}) = \\mathbb{E} ( \\mathpzc{X}  - \\mathbb{E}  \\mathpzc{X})^k.\\]A particularly important central second-order moment is the variance \\[\\sigma _\\mathpzc{X}^2 = \\mathrm{Var}\\, \\mathpzc{X} = m _2 = \\mathbb{E} ( \\mathpzc{X}  - \\mathbb{E}  \\mathpzc{X})^2 = \\mu _2  ( \\mathpzc{X} ) - \\mu^2 _\\mathpzc{X}.\\]Random vectors Joint and marginal distributions A vector $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ ofrandom variables is called a random vector. Its probabilitydistribution is defined as before as the pushforward measure$P = \\mathpzcb{X} _\\ast \\lambda$ Its is customary to treat $\\mathpzcb{X}$as a collection of $n$ random variables and define their joint CDF as \\[F _{\\mathpzcb{X}}(\\bb{x}) = P({\\mathpzcb{X} \\le \\bb{x}}) = P(\\mathpzc{X} _1 \\le x _1, \\dots, \\mathpzc{X} _n \\le x _n) = P(\\{ \\mathpzc{X} _1 \\le x _1 \\} \\times  \\dots \\times \\{ \\mathpzc{X} _n \\le x _n \\}).\\]As before, whenever the following holds \\[F _{\\mathpzcb{X}}(\\bb{x}) = \\int _{-\\infty}^{x _1} \\cdots \\int _{-\\infty}^{x _n}  f _{\\mathpzcb{X}}(x _1',\\dots, x _n') dx' _1 \\cdots dx' _n,\\]the integrand $f _{\\mathpzcb{X}}$ is called the joint PDF of$\\mathpzcb{X}$. The more rigorous definition as the Radon-Nikodymderivative \\[f _{\\mathpzcb{X}} = \\frac{d(\\mathpzc{X} _\\ast P) }{ d\\lambda}\\]staysunaltered, only that now $\\lambda$ is the $n$-dimensional Lebesguemeasure. Note that the joint CDF of the sub-vector$(\\mathpzc{X} _2, \\dots, \\mathpzc{X} _n)$ is given by \\[\\begin{aligned}F _{\\mathpzc{X} _2, \\cdots, \\mathpzc{X} _n } (x _2,\\dots,x _n) &amp;=&amp; P(\\mathpzc{X} _2 \\le x _2, \\dots, \\mathpzc{X} _n \\le x _n) = P(\\mathpzc{X} _1 \\le \\infty, \\mathpzc{X} _2 \\le x _2, \\dots, \\mathpzc{X} _n \\le x _n)  \\\\&amp;=&amp; F _{\\mathpzc{X} _1, \\cdots, \\mathpzc{X} _n } (\\infty, x _2,\\dots,x _n).\\end{aligned}\\]Such a distribiution is called marginal w.r.t. $\\mathpzc{X} _1$ and theprocess of obtaining it by substituting $x _1 = \\infty$ into$F _{\\mathpzcb{X}}$ is called marginalization. The corresponding actionin terms of the PDF consists of integration over $x _1$, \\[\\begin{aligned}f _{\\mathpzc{X} _2, \\cdots, \\mathpzc{X} _n } (x _2,\\dots,x _n) &amp;=&amp; \\int _\\mathbb{R} f _{\\mathpzc{X} _1, \\cdots, \\mathpzc{X} _n } (x _1, x _2,\\dots,x _n) dx _1.\\end{aligned}\\]Statistical independence A set $\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n$ of random variables iscalled statistically independent if their joint CDF iscoordinate-separable, i.e., can be written as the following tensorproduct \\[F _{\\mathpzc{X} _1, \\cdots, \\mathpzc{X} _n} = F _{\\mathpzc{X} _1} \\otimes \\cdots \\otimes F _{\\mathpzc{X} _n}.\\]An alternative definion can be given in terms of the PDF (whenever itexists): \\[f _{\\mathpzc{X} _1, \\cdots, \\mathpzc{X} _n} = f _{\\mathpzc{X} _1} \\otimes \\cdots \\otimes f _{\\mathpzc{X} _n}.\\]We will see a few additional alternative definitions in the sequel. Let$\\mathpzc{X}$ and $\\mathpzc{Y}$ be statistically-independent randomvariables with a PDF and let $\\mathpzc{Z} = \\mathpzc{X}+\\mathpzc{Y}$.Then, \\[\\begin{aligned}F _\\mathpzc{Z}(z) &amp;=&amp; P(\\mathpzc{Z} \\le z) = P(X+Y \\le z) = \\int _{\\mathbb{R}} \\int _{\\infty}^{z-y} f _{\\mathpzc{X}\\mathpzc{Y}}(x,y) dxdy \\\\&amp;=&amp; \\int _{\\mathbb{R}} \\int _{\\infty}^{z} f _{\\mathpzc{X}\\mathpzc{Y}}(x'-y,y) dx' dy,\\end{aligned}\\]where we changed the variable $x$ to $x’ = x+y$. Differentiating w.r.t.$z$ yields \\[\\begin{aligned}f _\\mathpzc{Z}(z)  &amp;=&amp; \\frac{dF _\\mathpzc{Z}(z)}{dz} = \\int _{\\mathbb{R}} \\frac{\\partial}{\\partial z} \\int _{\\infty}^{z} f _{\\mathpzc{X}\\mathpzc{Y}}(x'-y,y) dx' dy = \\int _{\\mathbb{R}}  f _{\\mathpzc{X}\\mathpzc{Y}}(z-y,y)  dy.\\end{aligned}\\]Since $\\mathpzc{X}$ and $\\mathpzc{Y}$ are statistically-independent, wecan substitute$f _{\\mathpzc{X}\\mathpzc{Y}} = f _{\\mathpzc{X}} \\otimes f _{\\mathpzc{Y}}$yielding \\[\\begin{aligned}f _\\mathpzc{Z}(z)  &amp;=&amp; \\int _{\\mathbb{R}}  f _{\\mathpzc{X}}(z-y) f _{\\mathpzc{Y}}(y)  dy = (f _{\\mathpzc{X}} \\ast f _{\\mathpzc{Y}} )(z).\\end{aligned}\\]This result is known as the convolution theorem. Limit theorems Given independent identically distributed (i.i.d.) variables$\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n$ with mean $\\mu$ and variance$\\sigma^2$, we define their sample average as \\[\\mathpzc{S} _n = \\frac{1}{n}( \\mathpzc{X} _1 + \\cdots + \\mathpzc{X} _n ).\\]Note that $\\mathpzc{S} _n$ is also a random variable with$\\mu _{\\mathpzc{S} _n} = \\mu$ and$\\displaystyle{\\sigma^2 _{\\mathpzc{S} _n} = \\frac{\\sigma^2}{n}}$. It isstraightforward to see that the variance decays to zero in the limit$n \\rightarrow \\infty$, meaning that $\\mathpzc{S} _n$ approaches adeterministic variable $\\mathpzc{S} = \\mu$. However, a much strongerresult exists: the (strong) law of large numbers states that in thelimit $n \\rightarrow \\infty$, the sample average converges inprobability to the expected value, i.e., \\[P\\left(  \\lim _{n \\rightarrow \\infty} \\mathpzc{S} _n = \\mu  \\right) = 1.\\]This fact is often denoted as$\\mathpzc{S} _n \\mathop{\\rightarrow}^P \\mu$. Furthermore, defining thenormalized deviation from the limit$\\mathpzc{D} _n = \\sqrt{n}(\\mathpzc{S} _n - \\mu)$, the central limittheorem states that $\\mathpzc{D} _n$ converges in distribution to$\\mathcal{N}(0,\\sigma^2)$, that is, its CDF converges pointwise to thatof the normal distribution. This is often denoted as$\\mathpzc{D} _n \\mathop{\\rightarrow}^D \\mathcal{N}(0,\\sigma^2)$. A slightly more general result is known as the delta method instatistics: if $g :  \\mathbb{R} \\rightarrow \\mathbb{R}$ is a$\\mathcal{C}^1$ function with non-vanishing derivative, then by theTaylor theorem, \\[g(\\mathpzc{S} _n) = g(\\mu) + g'(\\nu)(\\mathpzc{S} _n-\\mu) + \\mathcal{O}(| \\mathpzc{S} _n-\\mu |^2),\\]where $\\nu$ lies between $\\mathpzc{S} _n$ and $\\mu$. Since by the law oflarge numbers $\\mathpzc{S} _n \\mathop{\\rightarrow}^P \\mu$, we also have$\\nu \\mathop{\\rightarrow}^P \\mu$; since $g’$ is continuous,$g’(\\nu) \\mathop{\\rightarrow}^P g’(\\mu)$. Rearranging the terms andmultiplying by $\\sqrt{n}$ yields \\[\\sqrt{n}( g(\\mathpzc{S} _n) - g(\\mu) ) = g'(\\nu) \\sqrt{n}( \\mathpzc{S} _n) - \\mu ) = g'(\\nu) \\mathpzc{D} _n,\\]from where (formally, by invoking the Slutsky theorem): \\[\\sqrt{n}( g(\\mathpzc{S} _n) - g(\\mu) ) \\mathop{\\rightarrow}^D \\mathcal{N}(0,g^{\\prime} (\\mu)^2 \\sigma^2).\\]Joint moments Given a measurable function$\\bb{g} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, a (joint) moment of arandom vector $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ is \\[\\mathbb{E} \\bb{g}(\\mathpzcb{X}) = \\int \\bb{g}(\\bb{x}) dP = \\left(\\begin{array}{c}  \\int g _1(\\bb{x}) dP \\\\\\vdots \\\\ \\int g _m(\\bb{x}) dP\\end{array}\\right)=\\left(\\begin{array}{c} \\int _{\\mathbb{R}^n} g _1(\\bb{x}) f _{\\mathpzcb{X}}(\\bb{x}) d\\bb{x}  \\\\\\vdots \\\\\\int _{\\mathbb{R}^n} g _m(\\bb{x}) f _{\\mathpzcb{X}}(\\bb{x}) d\\bb{x}\\end{array}\\right);\\]the last term migh be undefined if the PDF does not exist.The mean of a random vector is simply$\\bb{\\mu} _\\mathpzcb{X}  = \\mathbb{E} \\mathpzcb{X}$. Of particularimportance are the second-order joint moments of pairs of randomvariables, \\[r _{\\mathpzc{X}\\mathpzc{Y}} = \\mathbb{E} \\mathpzc{X}\\mathpzc{Y}\\]andits central version \\[\\sigma^2 _{\\mathpzc{X}\\mathpzc{Y}} = \\mathrm{Cov}(\\mathpzc{X},\\mathpzc{Y}) = \\mathbb{E} \\left( (\\mathpzc{X} - \\mathbb{E} \\mathpzc{X} )(\\mathpzc{Y}  - \\mathbb{E} \\mathpzc{Y}) \\right) = r _{\\mathpzc{X}\\mathpzc{Y}} - \\mu _\\mathpzc{X} \\mu _\\mathpzc{Y}.\\]The latter quantity is known as the covariance of $\\mathpzc{X}$ and$\\mathpzc{Y}$. Two random variables $\\mathpzc{X}$ and $\\mathpzc{Y}$ with$r _{\\mathpzc{X}\\mathpzc{Y}} = 0$ are called orthogonal2 The variables with $\\sigma^2 _{\\mathpzc{X}\\mathpzc{Y}} = 0$ are calleduncorrelated. Note that for a statistically independent pair$(\\mathpzc{X},\\mathpzc{Y})$, \\[\\begin{aligned}\\sigma^2 _{\\mathpzc{X}\\mathpzc{Y}} &amp;=&amp; \\int _{\\mathbb{R}^2} (x-\\mu _\\mathpzc{X}) (y-\\mu _\\mathpzc{Y}) d((\\mathpzc{X} \\times \\mathpzc{Y}) _\\ast P) = \\int _{\\mathbb{R}} (x-\\mu _\\mathpzc{X}) d(\\mathpzc{X} _\\ast P) \\, \\int _{\\mathbb{R}} (y-\\mu _\\mathpzc{Y}) d(\\mathpzc{Y} _\\ast P) \\\\&amp;=&amp; \\mathbb{E} (\\mathpzc{X} - \\mathbb{E} \\mathpzc{X} ) \\cdot \\mathbb{E} (\\mathpzc{Y}  - \\mathbb{E} \\mathpzc{Y}) = 0.\\end{aligned}\\]However, the converse is not true, i.e., lack of correlation does notgenerally imply statistical independence (with the notable exception ofnormal variables). If $\\mathpzc{X}$ and $\\mathpzc{Y}$ are uncorrelatedand furthermore one of them is zero-mean, then they are also orthogonal(and the other way around). In general, the correlation matrix of a random vector$\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ is given by \\[\\bb{R} _{\\mathpzcb{X}} = \\mathbb{E}  \\mathpzcb{X} \\mathpzcb{X}^\\Tr;\\]its $(i,j)$-th element is$(\\bb{R} _{\\mathpzcb{X}}) _{ij} = \\mathbb{E} \\mathpzc{X} _i \\mathpzc{X} _j$.Similarly, the covariance matrix is defined as the central counterpartof the above moment, \\[\\bb{C} _{\\mathpzcb{X}} = \\mathbb{E}  (\\mathpzcb{X} - \\bb{\\mu} _\\mathpzcb{X} ) (\\mathpzcb{X} - \\bb{\\mu} _\\mathpzcb{X} )^\\Tr;\\]its $(i,j)$-th element is$(\\bb{C} _{\\mathpzcb{X}}) _{ij} =\\mathrm{Cov}( \\mathpzc{X} _i , \\mathpzc{X} _j)$.Given another random vector$\\mathpzcb{Y} = (\\mathpzc{Y} _1, \\dots, \\mathpzc{Y} _m)$, thecross-correlation and cross-covariance matrices are defined as$\\bb{R} _{\\mathpzcb{X}\\mathpzcb{Y}} = \\mathbb{E}  \\mathpzcb{X} \\mathpzcb{Y}^\\Tr$and$\\bb{C} _{\\mathpzcb{X}\\mathpzcb{Y}} = \\mathbb{E}  (\\mathpzcb{X} - \\bb{\\mu} _\\mathpzcb{X} ) (\\mathpzcb{Y} - \\bb{\\mu} _\\mathpzcb{Y} )^\\Tr$,respectively. Linear transformations Let $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ be an$n$-dimensional random vector, $\\bb{A}$ and $m \\times n$ deterministicmatrix, and $\\bb{b}$ and $m$-dimensional deterministic vector. We definea random vector $\\mathpzcb{Y} = \\bb{A} \\mathpzcb{X} + \\bb{b} $ as theaffine transformation of $\\mathpzcb{X}$. Using linearity of theexpectation operator, it is straightforward to show that \\[\\begin{aligned}\\bb{\\mu} _\\mathpzcb{Y}  &amp;=&amp; \\mathbb{E}(\\bb{A} \\mathpzcb{X} + \\bb{b}) = \\bb{A} \\bb{\\mu} _\\mathpzcb{X} + \\bb{b} \\\\\\bb{C} _\\mathpzcb{Y}  &amp;=&amp; \\mathbb{E}(\\bb{A} \\mathpzcb{X} - \\bb{A} \\bb{\\mu} _\\mathpzcb{X} ) (\\bb{A} \\mathpzcb{X} - \\bb{A} \\bb{\\mu} _\\mathpzcb{X} )^\\Tr = \\bb{A} \\bb{C} _\\mathpzcb{X} \\bb{A}^\\Tr \\\\\\bb{C} _{\\mathpzcb{X} \\mathpzcb{Y}}  &amp;=&amp; \\mathbb{E}(\\mathpzcb{X} - \\bb{\\mu} _\\mathpzcb{X} ) (\\bb{A} \\mathpzcb{X} - \\bb{A} \\bb{\\mu} _\\mathpzcb{X} )^\\Tr  = \\bb{C} _\\mathpzcb{X} \\bb{A}^\\Tr.\\end{aligned}\\]Estimation Let $\\mathpzcb{X}$ be a latent $n$-dimensional random vector, and let$\\mathpzcb{Y}$ be a statistically related $m$-dimensional observation(measurement). For example $\\mathpzcb{Y}$ can be a linearly transformedversion of $\\mathpzcb{X}$ corrupted by additive random noise,$\\mathpzcb{Y} = \\bb{A}\\mathpzcb{X} + \\mathpzcb{N}$. We might attemptusing the information $\\mathpzcb{Y}$ contains about $\\mathpzcb{X}$ inorder to estimate $\\mathpzcb{X}$. For that purpose, let us construct adeterministic function $\\bb{h} : \\RR^m \\rightarrow \\RR^n$ that we aregoing to call an estimator. Supplying a realization$\\mathpzcb{Y} = \\bb{y}$ to this estimator will produce a deterministicvector $\\hat{\\bb{x}} = \\bb{h}(\\bb{y})$, which is referred to as theestimate of $\\mathpzcb{X}$ given the measurement $\\bb{y}$. With someabuse of notation, we will henceforth denote $\\bb{h}(\\bb{y})$ as$\\hat{\\bb{x}}(\\bb{y})$. Note that supplying the random observationvector $\\mathpzcb{Y}$ to $\\hat{\\bb{x}}$ produces the random vector$\\hat{\\mathpzcb{X}} = \\hat{\\bb{x}}(\\mathpzcb{Y})$; here thedeterministic function $\\hat{\\bb{x}}$ acts as a random variabletransformation. Ideally, $\\hat{\\mathpzcb{X}}$ and $\\mathpzcb{X}$ should coincide;however, unless the measurement is perfect, there will be a discrepancy$\\mathpzcb{E} = \\hat{\\mathpzcb{X}} - \\mathpzcb{X}$ which we will referto as the error vector. Maximum likelihood For the sake of simplicity of exposition, let us focus on a very commonestimation setting with a linear forward model and an additivestatisticaly independent noise, i.e., \\[\\mathpzcb{Y} = \\bb{A}\\mathpzcb{X} + \\mathpzcb{N},\\]where $\\bb{A}$ isa deterministic $m \\times n$ matrix and $\\mathpzcb{N}$ is independent of$\\mathpzcb{X}$. In this case, we can assert that the distribution of themeasurement $\\mathpzcb{Y}$ given the latent signal $\\mathpzcb{X}$ issimply the distribution of $\\mathpzcb{N}$ at$\\mathpzcb{N} = \\mathpzcb{Y}- \\bb{A} \\mathpzcb{X}$, \\[P _{\\mathpzcb{Y} | \\mathpzcb{X}}( \\bb{y} | \\bb{x}  ) = P _{\\mathpzcb{N}}( \\bb{y} - \\bb{A} \\bb{x} ).\\]Assuming i.i.d. noise (i.e., that the $N _i$’s are distributedidentically and independently of each other), the latter simplifies to aproduct of one-dimensional measures. Note that this is essentially aparametric family of distributions – each choice of $\\bb{x}$ yields adistribution $P _{\\mathpzcb{Y} | \\mathpzcb{X} = \\bb{x}}$ of$\\mathpzcb{Y}$. For the time being, let us treat the notation$\\mathpzcb{Y} | \\mathpzcb{X}$ just as a funny way of writing. Given an estimate $\\hat{\\bb{x}}$ of the true realization $\\bb{x}$ of$\\mathpzcb{X}$, we can measure its “quality” by measuring some distance$D$ from $P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}}}$ to the truedistribution $P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}$ that created$\\mathpzcb{Y}$, and try to minimize it. Our estimator of $\\bb{x}$ cantherefore be written as \\[\\hat{\\bb{x}} = \\mathrm{arg}\\min _{\\hat{\\mathpzcb{X}}} D(P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}  ||  P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}}} ).\\]Note that we treat the quantity to be estimated as a deterministicparameter rather than a stochastic quantity. A standard way of measuring distance3 between distributions is theso-called Kullback-Leibler (KL) divergence. To define it, let $P$ and$Q$ be two probability measures (such that $P$ is absolutely continuousw.r.t. $Q$). Then, the KL divergence from Q to P is defined as \\[D(P || Q) = \\int _{} \\, \\log \\frac{dP}{dQ} \\, dP.\\]In other words, itis the expectation of the logarithmic differences between theprobabilities $P$ and $Q$ when the expectation is taken over $P$. Thedivergence can be thought of as an (asymmetric) distance between the twodistributions. Let us have a closer look at the minimization objective \\[D(P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}  ||  P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}}}  ) = \\mathbb{E} _{ \\mathpzcb{Y} \\sim P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}   }   \\log\\left(  \\frac{P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}  }{ P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}} }  } \\right) =\\mathbb{E} _{ \\mathpzcb{Y} \\sim P _{\\mathpzcb{Y} | \\mathpzcb{X} = \\bb{x}}   }   \\log P _{\\mathpzcb{Y} | \\mathpzcb{X} = \\bb{x}}  -\\mathbb{E} _{ \\mathpzcb{Y} \\sim P _{\\mathpzcb{Y} | \\mathpzcb{X} = \\bb{x}}   }   \\log P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}} }.\\]Note that the first term (that can be recognized as the entropy of$\\log P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}$) does not depend on theminimization variable; hence, we have \\[\\hat{\\bb{x}} = \\mathrm{arg}\\min _{\\hat{\\bb{x}}} \\, \\mathbb{E} _{ \\mathpzcb{Y} \\sim P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}   }   \\left( - \\log P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}}} \\right).\\]Let us now assume that $N$ realization ${ \\bb{y} _1, \\dots, \\bb{y} _N }$of $\\mathpzcb{Y}$ are observed. In this case, we can express the jointp.d.f of the observations as the product of$f _\\mathpzcb{N} (\\bb{y} _i - \\bb{A} \\bb{x})$ or, taking the negativelogarithm, \\[-\\frac{1}{N} \\log f _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}} (\\bb{y} _1,\\dots, \\bb{y} _N ) = - \\frac{1}{N} \\sum _{i=1}^N \\log f _\\mathpzcb{N} (\\bb{y} _i - \\bb{A} \\bb{x} ) = L(\\bb{y} _1,\\dots,\\bb{y} _N | \\bb{x}).\\]This function is known as the negative log likelihood function. By thelaw of large numbers, when $N$ approaches infinity, \\[L(\\bb{y} _1,\\dots,\\bb{y} _N | \\bb{x}) \\rightarrow \\mathbb{E} _{ \\mathpzcb{Y} \\sim P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}   }   \\left( - \\log P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}}} \\right).\\]Behold our minimization objective! To recapitulate, recall that we started with minimizing the discrepancybetween the latent parametric distribution that generated theobservation and that associated with our estimator. However, a closerlook at the objective revealed that it is the limit of the negative loglikelihood when the sample size goes to infinity. The minimization ofthe Kullback-Leibler divergence is equivalent to maximization of thelikelihood of the data coming from a specific parametric distribution, \\[\\hat{\\bb{x}} = \\mathrm{arg}\\max _{\\hat{\\bb{x}}} \\, P(  \\mathpzcb{Y}=\\bb{y} |  \\mathpzcb{X}=\\bb{x} ).\\]For this reason, the former estimator is called maximum likelihood(ML). Conditioning Before treating maximum a posteriori estimation, we need to brieflyintroduce the important notion of conditioning and conditionaldistributions. Recall our construction of a probability space comprisingthe triplet $\\Omega$ (the sample space), $\\Sigma$ (the Borel sigmaalgebra), and $P$ (the probability measure). Let $X$ be a randomvariable and $B \\subset \\Sigma$ a sub sigma-algebra of $\\Sigma$. We canthen define the conditional expectation of $\\mathpzc{X}$ given $B$ asa random variable $\\mathpzc{Z} = \\mathbb{E} \\mathpzc{X} | B$ satisfyingfor every $E \\in B$ \\[\\int _E \\mathpzc{Z} dP = \\int _E \\mathpzc{X} dP.\\](we are omitting some technical details such as, e.g., integrabilitythat $\\mathpzc{X}$ has to satisfy). Given another random variable $\\mathpzc{Y}$, we say that it generates asigma algebra $\\sigma(\\mathpzc{Y})$ as the set of pre-images of allBorel sets in $\\mathbb{R}$, \\[\\sigma(\\mathpzc{Y}) = \\{ \\mathpzc{Y}^{-1}(A) : A \\in \\mathbb{B}(\\mathbb{R}) \\}.\\]We can then use the previous definition to define the conditionalexpectation of $\\mathpzc{X}$ given $\\mathpzc{Y}$ as \\[\\mathbb{E} \\mathpzc{X} | \\mathpzc{Y} = \\mathbb{E} \\mathpzc{X} | \\sigma(\\mathpzc{Y}).\\]Conditional distribution Recall that expectation applied to indicator functions can be used todefine probability measures. In fact, for every $E \\in \\Sigma$, we mayconstruct the random variable $\\ind _E$, leading to$P(E) = \\mathbb{E} \\ind _E$. We now repeat the same, this time replacing$\\mathbb{E} $ with $\\mathbb{E} \\cdot | \\mathpzc{Y}$. For every$E \\in \\Sigma$, \\[\\varphi(\\mathpzc{Y}) = \\mathbb{E} \\, E | \\mathpzc{Y}\\]is a random variable that can be thought of as a transformation of therandom variable $\\mathpzc{Y}$ by the function $\\varphi$. We denote thisfunction as $P(E |\\mathpzc{Y})$ and refer to it as the (regular)conditional probability of event $E$ given $\\mathpzc{Y}$. It is easyto show that for every measurable set $B \\subset \\mathbb{R}$, \\[\\int _B P(E | \\mathpzc{Y}=y) (\\mathpzc{Y} _\\ast P)(dy) = P(E \\cap \\{ \\mathpzc{Y} \\in B \\});\\]Substituting $E = { \\mathpzc{X} \\in B}$ yields the conditionaldistribution of random variable $X$ given $\\mathpzc{Y}$, \\[P _{\\mathpzc{X} | \\mathpzc{Y}} (  B  | \\mathpzc{Y}=y) = P(\\mathpzc{X} \\in B | \\mathpzc{Y}=y).\\]It can be easily shown that $P _{\\mathpzc{X} | \\mathpzc{Y}}$ is a validprobability measure on $\\Sigma$ and for every pair of measurable sets$A$ and $B$, \\[\\int _B P _{\\mathpzc{X} | \\mathpzc{Y}} (A | \\mathpzc{Y}=y) (\\mathpzc{Y} _\\ast P)(dy) = P(\\{ \\mathpzc{X} \\in A  \\} \\cap \\{ \\mathpzc{Y} \\in B \\}).\\]If density exists, $P _{\\mathpzc{X} | \\mathpzc{Y}}$ can be describedusing the conditional p.d.f. $f _{\\mathpzc{X} | \\mathpzc{Y}}$ and thelatter identity can be rewritten in the form \\[\\int _A \\left( \\int _B f _{\\mathpzc{X} | \\mathpzc{Y}} (x | y) f _\\mathpzc{Y}(y) dy  \\right)  dx = P(\\{ \\mathpzc{X} \\in A  \\} \\cap \\{ \\mathpzc{Y} \\in B \\}) = \\int _A \\int _B f _{\\mathpzc{XY} } (x, y) dxdy.\\]This essentially means that$f _{\\mathpzc{XY} } (x, y) = f _{\\mathpzc{X} | \\mathpzc{Y}} (x | y)  f _\\mathpzc{Y}(y)$.Integrating w.r.t. $y$ yields the so-called total probability formula \\[f _{\\mathpzc{X} } (x) = \\int _\\mathbb{R} f _{\\mathpzc{XY} } (x, y) dy = \\int _\\mathbb{R} f _{\\mathpzc{X|Y} } (x|y)  f _\\mathpzc{Y}(y) dy.\\]We can also immediately observe that if $\\mathpzc{X}$ and $\\mathpzc{Y}$are statistically independent, we have \\[f _{\\mathpzc{XY} } (x, y) = f _{\\mathpzc{X} }(x) f _{\\mathpzc{Y} } (y) =  f _{\\mathpzc{X} | \\mathpzc{Y}} (x | y)  f _\\mathpzc{Y}(y),\\]from where $f _{\\mathpzc{X} | \\mathpzc{Y}} = f _{\\mathpzc{X}}$. In thiscase, conditioning on $\\mathpzc{Y}$ does not change our knowledge of$\\mathpzc{X}$. Bayes’ theorem One of the most celebrate (and useful) results related to conditionaldistributions is the following theorem named after Thomas Bayes.Exchanging the roles of $\\mathpzc{X}$ and $\\mathpzc{Y}$, we have \\[f _{\\mathpzc{XY} }  = f _{\\mathpzc{X} | \\mathpzc{Y}}   f _\\mathpzc{Y} = f _{\\mathpzc{Y} | \\mathpzc{X}}   f _\\mathpzc{X};\\]re-arranging the terms, we have \\[f _{\\mathpzc{Y} | \\mathpzc{X}} = f _{\\mathpzc{X} | \\mathpzc{Y}} \\, \\frac{  f _\\mathpzc{X} }{  f _\\mathpzc{Y} };\\]in terms of probability measures, the equivalent form is \\[P _{\\mathpzc{Y} | \\mathpzc{X}} = P _{\\mathpzc{X} | \\mathpzc{Y}} \\, \\frac{  dP _\\mathpzc{X} }{  dP _\\mathpzc{Y} }.\\]Law of total expectation Note that treating the conditional density$f _{\\mathpzc{X}|\\mathpzc{Y}}(x|y)$ just as a funnily-decorated p.d.f.with the argument $x$, we can write the following expectation integral \\[\\mathbb{E} (\\mathpzc{X}|\\mathpzc{Y}=y) = \\int _{\\RR} x f _{\\mathpzc{X}|\\mathpzc{Y}}(x|y) dx.\\]With (a very accepted) abuse of notation, we denote it as“$\\mathbb{E} (\\mathpzc{X}|\\mathpzc{Y}=y)$”. Note, however, that this isa very different object from $\\mathbb{E} \\, \\mathpzc{X}|\\mathpzc{Y}$ –while the former is a deterministic value, the latter is a randomvariable (a transformation of $\\mathpzc{Y}$). In order to construct$\\mathbb{E} \\, \\mathpzc{X}|\\mathpzc{Y}$ out of$\\mathbb{E} (\\mathpzc{X}|\\mathpzc{Y}=y)$, we define the map$\\varphi : y \\mapsto \\mathbb{E} (\\mathpzc{X}|\\mathpzc{Y}=y)$ and applyit to the random variable $\\mathpzc{Y}$, obtaining$\\mathbb{E} \\, \\mathpzc{X}|\\mathpzc{Y} = \\varphi(Y)$. Again, with aslight abuse of notation, we can write this as \\[\\varphi(Y) = \\mathbb{E} \\, \\mathpzc{X}|\\mathpzc{Y} = \\int _{\\RR} x f _{\\mathpzc{X}|\\mathpzc{Y}}(x|Y) dx.\\]Let us now take a regular expectation of the transformed variable$\\varphi(Y)$, which can be viewed as a generalized moment of$\\mathpzc{Y}$, \\[\\mathbb{E}\\, \\varphi(\\mathpzc{Y}) = \\int _{\\RR} \\varphi(y) f _{\\mathpzc{Y}} (y) dy =  \\int _{\\RR}  \\left(\\int _{\\RR} x f _{\\mathpzc{X}|\\mathpzc{Y}}(x|y) dx  \\right)  f _{\\mathpzc{Y}} (y) dy.\\]Rearranging the integrands and using$f _{\\mathpzc{XY} }  = f _{\\mathpzc{X} | \\mathpzc{Y}}   f _\\mathpzc{Y}$, weobtain \\[\\mathbb{E}\\, \\varphi(\\mathpzc{Y}) = \\int _{\\RR^2} x f _{\\mathpzc{XY} }(x,y) dx dy = \\mathbb{E}\\, X.\\]Stated differently, \\[\\mathbb{E}\\left(  \\mathbb{E}\\, X|Y \\right)  = \\mathbb{E}\\, X.\\]Thisresult is known as the smoothing theorem or the law of totalexpectation and can be thought of as an integral version of the law oftotal probability. Maximum a posteriori Recall that in maximum likelihood estimation we treated $\\mathpzcb{X}$as a deterministic parameter and tried to maximize the conditionalprobability $P(\\mathpzcb{Y} | \\mathpzcb{X})$. Let us now think of$\\mathpzcb{X}$ as of a random vector and maximize its probability giventhe data, \\[\\hat{\\bb{x}}(\\bb{y}) = \\mathrm{arg}\\max _{ \\hat{\\bb{x}} } P _{\\mathpzcb{X} | \\mathpzcb{Y} } ( \\mathpzcb{X} =  \\hat{\\bb{x}} | \\mathpzcb{Y} = \\bb{y}).\\]Invoking the Bayes theorem yields \\[P _{\\mathpzcb{X} | \\mathpzcb{Y}} = P _{\\mathpzcb{Y} | \\mathpzcb{X} } \\, \\frac{ dP _{\\mathpzcb{X}} }{dP _{\\mathpzcb{Y}} }\\]In the Bayesian jargon, $P _{\\mathpzcb{X}}$ is called the priorprobability, that is, our initial knowledge about $\\mathpzcb{X}$ beforeany observation thereof was obtained; $P _{\\mathpzcb{X} | \\mathpzcb{Y}}$is called the posterior probability having accounted for themeasurement $\\mathpzcb{Y}$. Note that the term$P _{\\mathpzcb{Y} | \\mathpzcb{X}}$ is our good old likelihood. Since weare maximizing the posterior probability, the former estimator is calledmaximum a posteriori (MAP). Taking negative logarithm, we obtain \\[-\\log P _{\\mathpzcb{X} | \\mathpzcb{Y}} = -\\log P _{\\mathpzcb{Y} | \\mathpzcb{X} } -\\log P _\\mathpzcb{X} +\\log P _{\\mathpzcb{Y}} = L(\\mathpzcb{Y} | \\mathpzcb{X}) - \\log P _{\\mathpzcb{X}} + \\mathrm{const}.\\]This yields the following expression for the MAP estimator \\[\\bb{h}(\\bb{Y}) = \\mathrm{arg}\\min _{ \\hat{\\bb{x}} } L(\\mathpzcb{Y} |  \\hat{\\bb{x}} ) - \\log P _\\mathpzcb{X} ( \\hat{\\bb{x}} ).\\]The minimization objective looks very similar to what we had in the MLcase; the only difference is that now a prior term is added. In theabsence of a good prior, a uniform prior is typically assumed, whichreduces MAP estimation to ML estimation. Minimum mean squared error Another sound way of constructing the estimator function $\\hat{\\bb{x}}$is by minimizing some error criterion related to the error vector$\\mathcal{E}(\\mathpzcb{E})$. A very common pragmatic choice is the meansquared error (MSE) criterion, \\[\\mathcal{E}(\\mathpzcb{E}) = \\mathbb{E} \\, \\|  \\mathpzcb{E} \\| _2^2,\\]leading to the following optimization problem: \\[\\hat{\\bb{x}}^{\\mathrm{MMSE}}  = \\mathrm{arg} \\min _{ \\bb{h} : \\RR^m \\rightarrow \\RR^n }  \\mathbb{E} \\, \\| \\bb{h}( \\mathpzcb{Y} ) - \\mathpzcb{X}  \\| _2^2.\\]The resulting estimator is called minimum mean squared error (or MMSE)estimator. Since the squared norm is coordinate separable, we caneffectively solve for each dimension of $\\hat{\\bb{x}}^{\\mathrm{MMSE}}$independently, finding the best (in the MSE sense) estimator of $X _i$given $\\mathpzcb{Y}$, \\[\\hat{x} _i^{\\mathrm{MMSE}} = \\mathrm{arg} \\min _{ h : \\RR^m \\rightarrow \\RR }  \\mathbb{E} \\, ( h( \\mathpzcb{Y} ) - X _i  )^2.\\]The minimization objective can be written explicitly as \\[\\begin{aligned} \\mathbb{E} \\, ( h( \\mathpzcb{Y} ) - X _i  )^2 &amp;=&amp;   \\mathbb{E} \\, \\left( \\mathbb{E}\\, (  h( \\mathpzcb{Y} ) - X _i  )^2 | \\mathpzcb{Y} \\right) =  \\mathbb{E} \\, \\left( \\mathbb{E}\\,  h^2 ( \\mathpzcb{Y} ) | \\mathpzcb{Y}  - 2 \\mathbb{E}\\,  h ( \\mathpzcb{Y} ) X _i | \\mathpzcb{Y}   + \\mathbb{E}\\, X^2 _i | \\mathpzcb{Y}  \\right)    \\\\  &amp;=&amp;     \\mathbb{E} \\, \\left(  h^2 ( \\mathpzcb{Y} )  - 2   \\mathbb{E}\\, X _i | \\mathpzcb{Y} \\cdot h ( \\mathpzcb{Y} )   + \\mathbb{E}\\, X^2 _i | \\mathpzcb{Y}  \\right) \\\\    &amp;=&amp; \\int _{\\RR^m}  (  h^2 ( \\bb{y} )  - 2   \\mathbb{E} (X _i | \\mathpzcb{Y}=\\bb{y}) \\cdot h ( \\mathpzcb{Y} )   + \\mathbb{E} ( X^2 _i | \\mathpzcb{Y}=\\bb{y})    )  f _{\\mathpzcb{Y}} (\\bb{y}) d\\bb{y}.\\end{aligned}\\]The latter integral is minimized iff its non-negative integrand isminimized at every point $\\bb{y}$. Let us fix $\\bb{y}$ and define$a = h(\\bb{y})$. The expression to minimize is \\[\\varphi(a) = a^2  - 2a\\,   \\mathbb{E} (\\mathpzc{X} _i | \\mathpzcb{Y}=\\bb{y})   + \\mathbb{E} ( \\mathpzc{X}^2 _i | \\mathpzcb{Y}=\\bb{y});\\]note that this is a convex quadratic function with the minimizer givenby \\[a^\\ast = \\mathbb{E} (\\mathpzc{X} _i | \\mathpzcb{Y}=\\bb{y}).\\]Fromhere we conclude that \\[h(\\bb{Y}) =  \\mathbb{E} \\mathpzc{X} _i | \\mathpzcb{Y};\\]consequently,the MMSE estimator of $\\mathpzcb{X}$ given $\\mathpzcb{Y}$ is given bythe conditional expectation \\[\\hat{\\mathpzcb{X}}^{\\mathrm{MMSE}}  =  \\mathbb{E} \\, \\mathpzcb{X} | \\mathpzcb{Y}.\\]The error vector produced by the MMSE estimator is given by$\\mathpzcb{E} =  \\mathbb{E} \\, \\mathpzcb{X} | \\mathpzcb{Y} - \\mathpzcb{X}$.Taking the expectation yields \\[\\mathbb{E}\\, \\mathpzcb{E} =  \\mathbb{E} \\left( \\mathbb{E} \\, \\mathpzcb{X} | \\mathpzcb{Y} \\right) - \\mathbb{E}\\, \\mathpzcb{X} =  \\mathbb{E}\\, \\mathpzcb{X} -  \\mathbb{E}\\, \\mathpzcb{X}  = \\bb{0}.\\]In other words, the estimation error is zero mean – a property oftenstated by saying that the MMSE estimator is unbiased. Since the MSE is equivalent (isomorphic) to Euclidean length, MMSEestimation can be viewed as the minimization of the length of the vector$\\mathpzcb{E}$ over the subspace of vectors of the form$\\hat{\\mathpzcb{X}} = \\bb{h}( \\mathpzcb{Y}  )$ with$\\bb{h} : \\RR^m \\rightarrow \\RR^m$. We known from Euclidean geometrythat the minimum length is obtained by the orthogonal projection of$\\mathpzcb{X}$ onto the said subspace, meaning that $\\hat{\\mathpzcb{X}}$is an MMSE estimator iff its error vector $\\mathpzcb{E}$ is orthogonalto every $\\bb{h}( \\mathpzcb{Y}  )$, that is, \\[\\mathbb{E}\\, \\left( (\\hat{\\mathpzcb{X}} - \\mathpzcb{X} )  \\bb{h}^\\Tr ( \\mathpzcb{Y}  ) \\right)  = \\bb{0}\\]for every $\\bb{h} : \\RR^m \\rightarrow \\RR^m$. Best linear estimator Sometimes the functional dependence of$\\hat{\\mathpzcb{X}}^{\\mathrm{MMSE}} $ on $\\mathpzcb{Y}$ might be toocomplicated to compute. In that case, it is convenient to restrict thefamily of functions to some simple class such as that of linear (moreprecisely, affine) functions of the form$\\bb{h}(\\bb{y}) = \\bb{A} \\bb{y} + \\bb{b}$. The MMSE estimator restrictedto such a subspace of functions is known as the best linear estimator(BLE), and its optimal parameters $\\bb{A}$ and $\\bb{b}$ are found byminimizing \\[\\min _{ \\bb{A}, \\bb{b} }  \\mathbb{E} \\, \\| \\bb{A} \\mathpzcb{Y} + \\bb{b} - \\mathpzcb{X}  \\| _2^2.\\]Note that since$\\mathbb{E} \\mathpzcb{E} =   \\bb{A} \\mathbb{E}\\, \\mathpzcb{Y} + \\bb{b} - \\mathbb{E}\\, \\mathpzcb{X}$,we can always zero the estimator bias by setting$\\bb{b} = \\bb{\\mu} _{\\mathpzcb{X}} -  \\bb{A}\\bb{\\mu} _{\\mathpzcb{Y}}$.With this choice, the problem reduces to \\[\\min _{ \\bb{A} }  \\mathbb{E} \\, \\| \\bb{A} (\\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}} ) - ( \\mathpzcb{X}  - \\bb{\\mu} _{\\mathpzcb{X}} )  \\| _2^2\\]or, equivalently, \\[\\min _{ \\bb{A} }  \\, \\mathbb{E} \\, \\mathrm{tr} \\left( (\\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}} )^\\Tr \\bb{A}^\\Tr \\bb{A} (\\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}} ) \\right) - 2 \\mathbb{E} \\,\\mathrm{tr} \\left(      (\\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}} )^\\Tr \\bb{A}^\\Tr   (\\mathpzcb{X} - \\bb{\\mu} _{\\mathpzcb{X}} )     \\right).\\]Manipulating the order ofmultiplication under the trace, exchaging its order with that of theexpectation operator, and moving the constants outside the expectationyields the following minimization objective: \\[\\begin{aligned}\\varphi(\\bb{A}) &amp;=&amp; \\mathrm{tr} \\left(   \\bb{A} \\mathbb{E}   (\\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}} )  (\\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}} )^\\Tr \\bb{A}^\\Tr  -2 \\bb{A} \\mathbb{E}   (\\mathpzcb{X} - \\bb{\\mu} _{\\mathpzcb{X}} )  (\\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}} )^\\Tr    \\right) \\\\   &amp;=&amp;  \\mathrm{tr} \\left(   \\bb{A} \\bb{C} _{\\mathpzcb{Y}}  \\bb{A}^\\Tr  -2 \\bb{A}  \\bb{C} _{\\mathpzcb{X} \\mathpzcb{Y}}   \\right).\\end{aligned}\\]Note that this is a convex (since$\\bb{C} _{\\mathpzcb{Y}}  \\succ 0$) quadratic function. In order to findits minimizer, we differentiate w.r.t. the parameter $\\bb{A}$ and equatethe gradient to zero: \\[0 = \\nabla \\varphi(\\bb{A}) = 2\\bb{A} \\bb{C} _{\\mathpzcb{Y}} - 2\\bb{C} _{\\mathpzcb{X} \\mathpzcb{Y}}.\\]The optimal parameter is obtained as$\\bb{A} = \\bb{C} _{\\mathpzcb{X} \\mathpzcb{Y}}\\bb{C} _{\\mathpzcb{Y}}^{-1}$. Combining this result with the expression for $\\bb{b}$, the best linearestimator is \\[\\hat{\\mathpzcb{X}}^{\\mathrm{BLE}} =  \\bb{C} _{\\mathpzcb{X} \\mathpzcb{Y}}\\bb{C} _{\\mathpzcb{Y}}^{-1} (  \\mathpzcb{Y} - \\bb{\\mu} _{\\mathpzcb{Y}}  )  + \\bb{\\mu} _{\\mathpzcb{X}} .\\]As the moregeneral MMSE estimator, BLE is also unbiased and enjoys theorthogonality property, meaning that $\\hat{\\mathpzcb{X}}$ is an MMSEestimator iff its error vector $\\mathpzcb{E}$ is orthogonal to everyaffine function of $\\mathpzcb{Y}  )$, that is, \\[\\mathbb{E}\\, \\left( (\\hat{\\mathpzcb{X}} - \\mathpzcb{X} )  ( \\bb{A}\\mathpzcb{Y}  + \\bb{b} )^\\Tr \\right)  = \\bb{0}\\]for every $\\bb{A} \\in \\RR^{m \\times n}$ and $\\bb{b} \\in \\RR^m$.             To be completely rigorous, the proper way to define the PDF is byfirst equipping the image of the map $\\mathpzc{X}$ with the Lebesguemeasure $\\lambda$ that assigns to every interval $[a,b]$ its length$b-a$. Then, we invoke the Radon-Nikodym theorem saying that if$\\mathpzc{X}$ is absolutely continuous w.r.t. $\\lambda$, thereexists a measurable function $f : \\mathbb{R} \\rightarrow [0,\\infty)$such that for every measurable $A \\subset \\mathbb{R}$,$\\displaystyle{(\\mathpzc{X} _\\ast P)(A) =P(\\mathpzc{X}^{-1}(A)) = \\int _A f d\\lambda}$.$f$ is called the Radon-Nikodym derivative and denoted by$\\displaystyle{f = \\frac{d(\\mathpzc{X} _\\ast P)}{d\\lambda}} $. It isexactly our PDF. &#8617;               In fact, $r _{\\mathpzc{X}\\mathpzc{Y}}$ can be viewed as an innerproduct on the space of random variables. This creates a geometryisomorphic to the standard Euclidean metric in $\\mathbb{R}^n$. Usingthis construction, the Cauchy-Schwarz inequality immediatelyfollows:$| r _{\\mathpzc{X}\\mathpzc{Y}} | \\le \\sigma _\\mathpzc{X} \\sigma _\\mathpzc{Y}$. &#8617;               Actually, not a true metric (which what the term distanceimplies, but rather an asymmetric form thereof, formally termed adivergence. &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/supplements/probability_and_statistics/",
        "teaser":null},{
        "title": "Tutorial 1: Python and tensor basics",
        "excerpt":"Material The tutorial notebook can be viewedhere. To run it yourself we recommend cloning the course tutorials repo. Students are encouraged to install the provided conda environment and playwith the examples from the notebook. Video (requires Technion account)         (video link) Extra Resources       conda download andinstallation.         Detailed notebooks for learning pythonhttps://github.com/jerry-git/learn-python3         Good introduction to numpy (presentation + notebooks)https://github.com/gertingold/euroscipy-numpy-tutorial         Python official docshttps://docs.python.org/3.7/.         numpy official docshttps://docs.scipy.org/doc/numpy/reference/         PyTorch official docshttps://pytorch.org/docs/stable/   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_01/",
        "teaser":null},{
        "title": "Tutorial 2: Supervised Learning",
        "excerpt":"Topics: Supervised learning framework, binary and multiclass logisticregression, pytorch and autograd basics. Material The tutorial notebook can be viewed here. To run it yourself we recommend cloning the course tutorials repo. If you have previously cloned the repo, run git pullconda env update -f environment.ymlconda activate cs236781jupyter labto update your environment and run jupyter. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_02/",
        "teaser":null},{
        "title": "Tutorial 3: Multilayer Perceptron",
        "excerpt":"MLP model, activations, backprop, loss functions and optimization in PyTorch Material The tutorial notebook can be viewed here. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_03/",
        "teaser":null},{
        "title": "Tutorial 4: Convolutional Neural Nets",
        "excerpt":"Convolutional and pooling layers, architectures, spatial classification,residual nets. Material The tutorial notebook can be viewed here. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_04/",
        "teaser":null},{
        "title": "Tutorial 5: Optimization",
        "excerpt":"Descent-based methods, backpropagation, automatic differentiation, bi-leveloptimization, time-series analysis with CNNs. Material The tutorial notebook can be viewedhere. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_05/",
        "teaser":null},{
        "title": "Tutorial 6: Sequence Models",
        "excerpt":"RNN implementation, sentiment analysis, Temporal Convolution Networks. Material The tutorial notebook can be viewed here. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_06/",
        "teaser":null},{
        "title": "Tutorial 7: Attention",
        "excerpt":"Attention mechanisms, sequence-to-sequence models, machine translation. Material The tutorial notebook can be viewed here. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_07/",
        "teaser":null},{
        "title": "Tutorial 8: Transfer learning and domain adaptation",
        "excerpt":"Transfer learning definition and contexts, fine-tuning pre-trained models,unsupervised domain adaptation via an adversarial approach. Material The tutorial notebook can be viewed here. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_08/",
        "teaser":null},{
        "title": "Tutorial 9: Deep reinforcement learning",
        "excerpt":"The RL setting, openAI Gym, Deep q-learning for Atari games. Material The tutorial notebook can be viewed here. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_09/",
        "teaser":null},{
        "title": "Tutorial 10: Geometric deep learning",
        "excerpt":"Filters on graphs, graph convolution layers, semi-supervised node classification Notebook The tutorial notebook can be viewedhere. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_10/",
        "teaser":null},{
        "title": "Tutorial 11: CUDA Kernels",
        "excerpt":"The CUDA programming model, numba, implementing CUDA kernels in python, thread synchronization, shared memory Notebook The tutorial notebook can be viewed here. Video (requires Technion account)         (video link) ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236781/semesters/w22/tutorials/tutorial_11/",
        "teaser":null}]
