<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.13.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Lecture 11: Learning on Non-Euclidean Domains | CS236781: Deep Learning</title>
<meta name="description" content="Toeplitz operators, manifolds, graphs, convolution, spectral CNN">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="CS236781: Deep Learning">
<meta property="og:title" content="Lecture 11: Learning on Non-Euclidean Domains">
<meta property="og:url" content="https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_11/">


  <meta property="og:description" content="Toeplitz operators, manifolds, graphs, convolution, spectral CNN">











  

  


<link rel="canonical" href="https://vistalab-technion.github.io/cs236781/semesters/w22/lecture_notes/lecture_11/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "VISTA Lab",
      "url": "https://vistalab-technion.github.iocs236781/semesters/w22",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/cs236781/semesters/w22/feed.xml" type="application/atom+xml" rel="alternate" title="CS236781: Deep Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/cs236781/semesters/w22/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single text-justify wide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/cs236781/semesters/w22/">CS236781: Deep Learning</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/cs236781/semesters/w22/info/" >Info</a>
            </li><li class="masthead__menu-item">
              <a href="/cs236781/semesters/w22/lectures/" >Lectures</a>
            </li><li class="masthead__menu-item">
              <a href="/cs236781/semesters/w22/tutorials/" >Tutorials</a>
            </li><li class="masthead__menu-item">
              <a href="/cs236781/semesters/w22/assignments/" >Assignments</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Lecture 11: Learning on Non-Euclidean Domains">
    <meta itemprop="description" content="Toeplitz operators, manifolds, graphs, convolution, spectral CNN">
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Lecture 11: Learning on Non-Euclidean Domains
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  103 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Contents</h4></header>
              <ul class="toc__menu">
  <li><a href="#convolution-on-euclidean-domains">Convolution on Euclidean domains</a>
    <ul>
      <li><a href="#eigenvectors-of-toeplitz-operators">Eigenvectors of Toeplitz operators</a></li>
      <li><a href="#fourier-transform">Fourier transform</a></li>
    </ul>
  </li>
  <li><a href="#non-euclidean-domains">Non-Euclidean domains</a>
    <ul>
      <li><a href="#manifolds">Manifolds</a>
        <ul>
          <li><a href="#fields">Fields</a></li>
          <li><a href="#differential">Differential</a></li>
          <li><a href="#gradient-and-divergence">Gradient and divergence</a></li>
          <li><a href="#laplacian">Laplacian</a></li>
        </ul>
      </li>
      <li><a href="#graphs">Graphs</a>
        <ul>
          <li><a href="#difference-operators">Difference operators</a></li>
          <li><a href="#graph-laplacian">Graph Laplacian</a></li>
        </ul>
      </li>
      <li><a href="#fourier-transform-on-non-euclidean-domains">Fourier transform on non-Euclidean domains</a></li>
      <li><a href="#convolution-on-non-euclidean-domains">Convolution on non-Euclidean domains</a></li>
    </ul>
  </li>
  <li><a href="#spectral-cnn">Spectral CNN</a>
    <ul>
      <li><a href="#strided-convolution">Strided convolution</a></li>
      <li><a href="#spatial-localization">Spatial localization</a></li>
    </ul>
  </li>
  <li><a href="#spatial-cnn">Spatial CNN</a></li>
</ul>
            </nav>
          </aside>
        
        <p>All learning settings we have encountered thus far had a common
(sometimes, tacit) property: they assumed Euclidean geometry of the
data. For example, we could compute standard inner products, subtract
one vector from another, apply matrices to vectors, etc. Data like time
signal and images were further discretized on regular Cartesian grids,
and we could apply operations like convolution by simply sliding the
same window over the signal and computing inner products.</p>

<p>Most of these apparently straightforward notions become less
straightforward when the domain underlying the data is no longer
Euclidean. Such kinds of data arise in a long list of applications. For
example, in social networks, user information can be modeled as signals
on a graph. Sensor networks are also modeled as graphs of distributed
interconnected sensors, whose readings are time-dependent signals on the
graph. In neuroscience, graph models are used to represent anatomical
and functional structures of the brain. In biology, graphs are a common
way to express interactions between genes, proteins, etc. In computer
graphics and vision, three-dimensional geometric objects are often
represented as Riemannian manifolds (surfaces) endowed with attributes
such as color texture.</p>

<p>It is important to distinguish between two very distinct tasks: learning
<em>on</em> non-Euclidean domains vs. learning <em>of</em> non-Euclidean domains. We
encountered the latter problem when discussing unsupervised learning,
where our goal was to discover (learn) the latent manifold from which
the data are sampled. The former problem of analyzing signals on graphs
and manifolds is what is going to occupy us in this lecture. In what
follows, we wil briefly review the main properties of the basic
ingredients of CNNs on Euclidean domain, specifically, the convolution
operator and pooling. We will then describe how to generalize these
notions to graphs.</p>

<h1 id="convolution-on-euclidean-domains">Convolution on Euclidean domains</h1>

<p>The main ingredient of a CNN is a convolutional layer, describing a
mapping between $m$-dimensional input signals to $n$-dimensional output
signals (we use the term <em>signal</em> to generalize the notion of a
sequence, allowing its elements to be indexed by a $d$-dimensional
multi-index; this notion includes images and higher-dimensional signals
besides time series). Formally, a convolutional layer accepts an
$m$-dimensional <em>vector-valued</em> (infinitely supported) signal
$\bb{x} = (\bb{x}^1,\dots, \bb{x}^m) = \{ (x _{\bb{k}}^1,\dots, x _{\bb{k}}^m) \} _{ {\bb{k}} \in \mathbb{Z}^d}$,
each input dimension of which is called a <em>channel</em> or <em>feature map</em>.
The layer produces an $n$-dimensional (infinitely supported) signal
$\bb{y} = (\bb{y}^1,\dots, \bb{y}^n)  = \{ (y _{\bb{k}}^1,\dots, y _{\bb{k}}^n) \} _{ {\bb{k}} \in \mathbb{Z}^d}$
by applying a bank of filters,</p>

\[\bb{y}^j = \varphi \left( \sum _{i=1}^m \bb{w}^{ij} \ast \bb{x}^{i} + b _j   \right) ,\]

<p>Explicitly, the action of the convolution
$\bb{z}^j = \bb{w}^{ij} \ast \bb{x}^i$ can be written as</p>

\[z^j _{\bb{k}} =    \sum _{i=1}^m \sum _{\bb{p} \in \mathbb{Z}^d } w^{ij} _{\bb{p}} x^i _{\bb{k}-\bb{p}}.\]

<p>Note the $d$-dimensional multi-indices in the sum.</p>

<h2 id="eigenvectors-of-toeplitz-operators">Eigenvectors of Toeplitz operators</h2>

<p>Since the convolution operation is the main ingredient of a CNN, let us
dedicate some attention to listing a few of its properties that will be
instrumental in the generalization of CNNs to non-Euclidean domains. As
we have already seen, any linear shift-invariant<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> (Toeplitz) operator
$\mathcal{W}$ can be represented as the convolution</p>

\[\mathcal{W}\bb{x} = \bb{x} \ast \bb{w}.\]

<p>The action of any linear
operator on a vector consists of scaling and rotating the vector.
However, there are some privileged directions where no rotation occurs;
such directions are called the eigenvectors of the operator.
Specifically, for Toeplitz operators, given the input signal</p>

\[\bb{\phi}^{\bb{\xi}} _{\bb{n}} = e^{i\, 2\pi \bb{\xi} ^\Tr \bb{n}}\]

<p>parametrized by the vector $\bb{\xi} \in [0,1]^d$, the output of the
operator is</p>

\[(\bb{w} \ast \bb{\phi}^{\xi}) _{\bb{n}} = \sum _{ {\bb{k}}}  w _{\bb{n}} {\phi}^{\bb{\xi}} _{ {\bb{n}}-{\bb{k}}} = 
 \sum _{ {\bb{k}}}  w _{\bb{k}} e^{i\, 2\pi \bb{\xi} ^\Tr (\bb{n} - \bb{k})} =  e^{i\, 2\pi \bb{\xi} ^\Tr \bb{n} } \sum _{ {\bb{k}}}  w _{\bb{k}} e^{-i\, 2\pi \bb{\xi} ^\Tr \bb{k}}.\]

<p>Recalling the standard inner product on $\ell^2$,</p>

\[\langle  \bb{x}, \bb{y} \rangle = \sum _{\bb{k}} x _{\bb{k}} y^\ast _{\bb{k}},\]

<p>we can express</p>

\[\sum _{ {\bb{k}}}  w _{\bb{k}} e^{-i\, 2\pi \bb{\xi} ^\Tr  \bb{k} } =  \sum _{ {\bb{k}}}  w _{\bb{k}}  {\phi}^{\bb{\xi}} _{-k}  =  \sum _{ {\bb{k}}}  w _{\bb{k}}  \left({\phi}^{ {\bb{\xi}}} _{\bb{k}} \right)^\ast = \langle  \bb{w}, \bb{\phi}^{\bb{ {\bb{\xi}}}} \rangle = \hat{\bb{w}}({\bb{\xi}}).\]

<p>In these terms, the output is given by</p>

\[\bb{w} \ast \bb{\phi}^{\bb{\xi}} = \hat{\bb{w}}(\xi) \bb{\phi}^{\bb{\xi}},\]

<p>which means that $\bb{\phi}^{\bb{\xi}}$ is an eigenvector of
$\mathcal{W}$ with the corresponding eigenvalues
$\hat{\bb{w}}(\bb{\xi})$. Note that while the <em>eigenvalues</em> depend on
the specific operator (embodied in the sequence $\bb{w}$ called the
<em>kernel</em> of the operator), the <em>eigenvectors</em> are always the same:
$\{\bb{\phi}^{\bb{\xi}}\} _{\bb{\xi} \in [0,1]^d}$.</p>

<h2 id="fourier-transform">Fourier transform</h2>

<p>The function</p>

\[\hat{\bb{w}}(\bb{\xi}) = \langle  \bb{w}, \bb{\phi}^{\xi} \rangle _{\ell^2(\mathbb{Z}^d)} = \sum _{\bb{k}}  w _{\bb{k}} e^{-i\, 2\pi \bb{\xi} ^\Tr \bb{k} }\]

<p>is called the <em>(forward) Fourier transform</em> of the sequence $\bb{w}$. It
is customary to define the operator
$\mathcal{F} : \ell^2(\mathbb{Z}^d) \rightarrow L^2([0,1]^d)$ mapping
$\bb{w}$ to $\hat{\bb{w}}$, and refer to the argument $\bb{\xi}$ of the
latter as to <em>frequency</em>. The inverse map
$\mathcal{F}^{-1} : L^2([0,1]^d) \rightarrow \ell^2(\mathbb{Z}^d)$,
called the <em>inverse Fourier transform</em>, is given by</p>

\[\bb{w} = \mathcal{F}^{-1} \hat{\bb{w} } = \int _{[0,1]^d} \hat{\bb{w}}(\bb{\xi})  \bb{\phi}^{\bb{\xi}} d\bb{\xi} =
 \int _{[0,1]^d} \hat{\bb{w}}(\bb{\xi})  e^{i\, 2\pi \bb{\xi} ^\Tr \bb{n}} d\bb{\xi}.\]

<p>To prove this, observe that</p>

\[\begin{aligned}
 \int _{[0,1]^d} \hat{\bb{w}}(\bb{\xi})  e^{i\, 2\pi \bb{\xi} ^\Tr \bb{n}} d\bb{\xi} &amp;=&amp;  
  \int _{[0,1]^d}   \left( \sum _{\bb{k}}  w _{\bb{k}} e^{-i\, 2\pi \bb{\xi} ^\Tr \bb{k}}  \right)  e^{i\, 2\pi \bb{\xi} ^\Tr\bb{n}}  d\bb{\xi} \\
 &amp;=&amp; \sum _{\bb{k}}  w _{\bb{k}} \left(   \int _{[0,1]^d}  e^{i\, 2\pi \bb{\xi} ^\Tr (\bb{n}-\bb{k})} d\bb{\xi} \right).\end{aligned}\]

<p>Since the complex exponentials
$e^{i\, 2\pi \bb{\xi} ^\Tr (\bb{n}- \bb{k})}$ have an integer number of
periods on the domain $[0,1]^d$, the latter integral is zero unless
$\bb{n}-\bb{k} = \bb{0}$, in which case it is exactly $1$. Hence,</p>

\[\int _{[0,1]^d} \hat{\bb{w}}(\bb{\xi})  e^{i\, 2\pi \bb{\xi} ^\Tr \bb{n}} d\bb{\xi} = \sum _{\bb{k}}  w _{\bb{k}} \delta _{\bb{n}-\bb{k}} =  w _{\bb{n}}.\]

<p>Note that the inverse Fourier transform can also be written as</p>

\[(\mathcal{F}^{-1} \hat{\bb{w}} ) _{\bb{n}}  = \langle  \hat{\bb{w}},  (\bb{\phi}^{\bb{n}})^\ast  \rangle _{L^2(\mathbb{Z}^d)},\]

<p>which emphasizes the above orthonormality property by essentially
stating that $\mathcal{F}^{-1} = \mathcal{F}^\ast$, where
$\mathcal{F}^\ast$ denotes the adjoint operator. Geometrically, this
means that the Fourier transform is <em>unitary</em>, a generalized form of
rotation. The fact that rotations preserve distances leads to the
celebrate <em>Plancherel identity</em></p>

\[\langle  \bb{f}, \bb{g} \rangle _{\ell^2(\mathbb{Z}^d)} = \langle  \hat{\bb{f}}, \hat{\bb{g}} \rangle _{L^2([0,1]^d)},\]

<p>and, in the particular case of $\bb{g} = \bb{f}$, even more celebrate
<em>Parseval’s identity</em></p>

\[\|  \bb{f} \| _{\ell^2(\mathbb{Z}^d)} = \| \hat{\bb{f}} \| _{L^2([0,1]^d)}.\]

<p>We can think of the Fourier transform as the transformation of a signal
$\bb{x}$ to the (joint) eigenbasis of (all) Toeplitz operators, that is,
describing $\bb{x}$ as a linear combination of the eigenvectors
$\bb{\phi}^{\bb{\xi}}$,</p>

\[\bb{x} = \int _{-\pi}^\pi \hat{\bb{x}}(\bb{\xi})  \bb{\phi}^{\bb{\xi}} d\bb{\xi},\]

<p>with $\hat{\bb{x}}(\bb{\xi})$ serving as the coordinates in the
eigenbasis. In the signal processing parlance, $\bb{x}$ in the standard
basis is called the <em>impulse response</em>, while in the Fourier basis it is
referred to as the <em>frequency response</em>. The act of converting the
impules response to the frequency response is referred to as <em>analysis</em>,
while the inverse is referred to as <em>synthesis</em>.</p>

<p>The notion of the Fourier transform allows to apply the operator
$\mathcal{W}$ in its eigenbasis. Indeed, describing the input sequence
as a linear combination of the eigenvectors,</p>

\[\bb{x} = \int _{-\pi}^\pi \hat{\bb{x}}(\xi)  \bb{\phi}^{\bb{\xi}} d\bb{\xi},\]

<p>we obtain</p>

\[\bb{w} \ast \bb{x} = \mathcal{W}  \int _{[0,1]^d} \hat{\bb{x}}(\bb{\xi})  \bb{\phi}^{\bb{\xi}} d\bb{\xi} =  
 \int _{[0,1]^d} \hat{\bb{x}}(\bb{\xi}) \mathcal{W} \bb{\phi}^{\bb{\xi}} d\bb{\xi} =  \int _{[0,1]^d} \hat{\bb{w}}(\bb{\xi}) \hat{\bb{x}}(\bb{\xi}) \bb{\phi}^{\bb{\xi}} d\bb{\xi}.\]

<p>The latter can be written as</p>

\[\bb{w} \ast \bb{x} = \mathcal{F}^{1-} ( \mathcal{F}\bb{w}  \cdot \mathcal{F} \bb{x} )\]

<p>or, schematically,</p>

\[\bb{w} \ast \bb{x}  \mathop{\longleftrightarrow}^{\mathcal{F}} \hat{\bb{w}} \cdot \hat{\bb{x}}.\]

<p>This result is known as the <em>convolution theorem</em>, stating that
convolution becomes pointwise product in the Fourier (frequency) domain.
This result is the consequence of the fact that the Fourier transform
diagonalizes Toeplitz operators (convolution). In fact, operating in the
Fourier domain, we can <em>define</em> any Toeplitz operator as a diagonal
linear operator (point-wise product), fully defined by the function
$\hat{\bb{w}}(\bb{\xi})$.</p>

<h1 id="non-euclidean-domains">Non-Euclidean domains</h1>

<p>In order to generalize the notion of a CNN to the non-Euclidean case,
let us first define two types of non-Euclidean domains: manifolds and
graphs (we will think of the latter as of some sort of discretization of
the former).</p>

<h2 id="manifolds">Manifolds</h2>

<p>A topological space $\mathcal{M}$ is called a $d$-dimensional <em>manifold</em>
if every point $p$ in it has a neighborhood topologically equivalent
(homeomorphic) to $\RR^d$. The latter space is referred to as the
<em>tangent</em> space at point $p$, denoted as $T _p \mathcal{M}$. The disjoint
union of all tangent spaces is called the <em>tangent bundle</em>, denoted as
$T\mathcal{M}$. Since each point $p$ is now associated with a linear
space $T _p \mathcal{M}$, we can endow the latter with an inner product
$\langle \cdot, \cdot \rangle _{T _p \mathcal{M}} : T _p \mathcal{M}\times T _p \mathcal{M}\rightarrow T _p \mathcal{M}$
(which we assume to depend smoothly on $p$, without further defining
preciselt what it means). This inner product is called a <em>Riemannian
metric</em> and a manifold endowed with it is called a <em>Riemannian
manifold</em>. The metric allows to (locally) measure lengths and angles.</p>

<h3 id="fields">Fields</h3>

<p>A <em>scalar field</em> on $\mathcal{M}$ is a function of the form
$f : \mathcal{M}\rightarrow \RR$. A <em>(tangent) vector field</em> is a map
$F : \mathcal{M}\rightarrow T \mathcal{M}$ assigning to every point
$p \in \mathcal{M}$ a tangent vector $F(p) \in T _p \mathcal{M}$. Tangent
vectors formalize the notion of infinitesimal displacements that we
routinely use in calculus on Euclidean domains. Next, we define the
Hilbert spaces of scalar and vector fields on $\mathcal{M}$ through the
following standard inner products:</p>

\[\begin{aligned}
\langle  f, g \rangle _{L^2(\mathcal{M}) } &amp;=&amp; \int _{\mathcal{M}} f(p) g(p) dp; \\
\langle  F, G \rangle _{L^2(T \mathcal{M}) } &amp;=&amp; \int _{\mathcal{M}}  \langle F(p) , G(p) \rangle _{T _p \mathcal{M}} dp,\end{aligned}\]

<p>where the integration is performed w.r.t. the $d$-dimensional volume
element $dp$ induced by the metric.</p>

<h3 id="differential">Differential</h3>

<p>The notion of a derivative in calculus describes how the value of a
function changes with an infinitesimal change of its argument. One of
the big differences distinguishing calculus from differential geometry
is a lack of a global vector space structure on the manifold, making
expressions like $f(p+dp)$ meaningless. The conceptual leap that is
required to generalize calculus to manifolds is the need to express all
notions locally in the tangent spaces.</p>

<p>In order to construct calculus on a manifold, we define the
<em>differential</em> of $f$ as the operator
$df : T \mathcal{M}\rightarrow \RR$ on tangent vectors. At every
$p \in \mathcal{M}$, the differential is defined as the linear
functional (a.k.a. $1$-form in the differential geometry jargon)</p>

\[df(p) : v \mapsto \langle \nabla f(p), v \rangle _{T _p \mathcal{M}},\]

<p>$v \in T _p \mathcal{M}$. A vector field $F$ generalizes the notion of
small displacements. In fact, we can write</p>

\[df(p)F(p) = \langle \nabla f(p), F(p) \rangle _{T _p \mathcal{M}},\]

<p>as
the extension of the regular notion of directional derivative in
Euclidean spaces,</p>

\[df = f(\bb{p}+d\bb{p}) = \langle \nabla f(\bb{p}), d\bb{p}  \rangle = \frac{\partial f(\bb{p})}{\partial p _1}dp _1 + \cdots + \frac{\partial f(\bb{p})}{\partial p _d} dp _d.\]

<h3 id="gradient-and-divergence">Gradient and divergence</h3>

<p>The operator $\nabla f(p) : \mathcal{M}\rightarrow T\mathcal{M}$
appearing in the definition of the differential generalizes the notion
of the <em>gradient</em> defining the direction of the steepest increase of
$f$; the main difference is that on a manifold the latter direction is
given by tangent vector. The gradient can be viewed as an operator of
the form $\nabla : L^2(\mathcal{M}) \rightarrow L^2(T\mathcal{M})$
mapping scalar fields to vector fields. Its adjoint is called the
<em>divergence</em> operator,
$\mathrm{div} f(p) : L^2(T \mathcal{M}) \rightarrow L^2(\mathcal{M})$
mapping vector fields to scalar fields and satisfying</p>

\[\langle F, \nabla f\rangle _{L^2(T\mathcal{M})} = \langle \nabla^\ast F,  f\rangle _{L^2(\mathcal{M})} = \langle -\mathrm{div}\, F,  f\rangle _{L^2(\mathcal{M})}\]

<p>(note the minus sign!). As vector fields can be thought of as a model of
a flow on the manifold, the divergence operator measures the net flow at
a point.</p>

<h3 id="laplacian">Laplacian</h3>

<p>The <em>Laplacian</em> (a.k.a. the <em>Laplace-Beltrami operator</em>)
$\Delta : L^2(\mathcal{M}) \rightarrow L^2(\mathcal{M})$ is defined as</p>

\[\Delta  = \nabla^\ast \nabla = -\mathrm{div} \nabla.\]

<p>The Laplacian
of a scalar field $f$ at point $p$ can be interpreted as the difference
between the average value of the field on an infinitesimal sphere around
$p$ and the value of $f(p)$.</p>

<p>By virtue of the adjoint relation between the gradient and the negative
divergence, the Laplacian is self-adjoint (symmetric), that is, for
every scalar field $f$,</p>

\[\langle \nabla f, \nabla f \rangle _{L^2(T\mathcal{M})} = \langle \nabla^\ast \nabla f,  f \rangle _{L^2(\mathcal{M})} = \langle \Delta f,  f \rangle _{L^2(\mathcal{M})}\]

<p>and</p>

\[\langle \nabla f, \nabla f \rangle _{L^2(T\mathcal{M})} = \langle f,   \nabla^\ast \nabla  f \rangle _{L^2(\mathcal{M})} = \langle  f,  \Delta f \rangle _{L^2(\mathcal{M})}.\]

<p>The expression $\langle \Delta f,  f \rangle _{L^2(\mathcal{M})}$ is
known as the <em>Dirichlet energy</em> of the field $f$ and measures the
“smoothness” of the field on $\mathcal{M}$. Physically, it can be
interpreted as the potential energy due to the bending of an elastic
body.</p>

<h2 id="graphs">Graphs</h2>

<p>We will limit our attention to undirected graphs and view them as a
discrete analog of manifolds. We define the <em>vertex</em> set
$V = \{ 1,\dots, n\}$ (it can be any set containing $n$ objects, which
we canonically map to the above set of natural numbers from $1$ to $n$);
the <em>edge set</em> and the <em>edge</em> set $E \subseteq V \times V$. An
undirected graph has $(i,j) \in E \Leftrightarrow (j,i) \in E$. We
further define the vertex weights as the function
$a : V \rightarrow (0,\infty)$ and the edge weights as
$w : E \rightarrow \RR _+$ (in fact, $w$ can be defined on the entire
$V \times V$ with $w _{ij} = 0$ meaning $(i,j) \notin E$). We refer to
the tuple $\mathcal{G}= (V,E,a,w)$ as to a <em>weighted undirected graph</em>.</p>

<h3 id="difference-operators">Difference operators</h3>

<p>A <em>vertex</em> field is a function of the form $f : V \rightarrow \RR$,
while an <em>edge field</em> is a function of the form $F : E \rightarrow \RR$.
Vertex and edge fields on a graph are the discrete analogs of scalar and
vector fields on a manifold (under the tacit assumption that
$F _{ij} = -F _{ji}$ for technical reason we are not goind to detail). As
in the case of manifolds, we define the two Hilbert spaces, $\ell^2(V)$
and $\ell^2(E)$ through the corresponding inner products</p>

\[\begin{aligned}
\langle  f, g \rangle _{\ell^2(V) } &amp;=&amp; \sum _{i \in V} a _i f _i g _i ; \\
\langle  F, G \rangle _{\ell^2(E) } &amp;=&amp; \sum _{(i,j) \in E} w _{ij} F _{ij} G _{ij};\end{aligned}\]

<p>note that the weights play the role of discrete volume elements we had
before in the integrals on manifolds.</p>

<p>The graph <em>gradient</em> is the operator
$\nabla : \ell^2(V) \rightarrow \ell^2(E)$ defined by</p>

\[(\nabla f) _{ij} = f _i - f _j.\]

<p>Note that the resulting edge field is,
by definition, alternating, that is,
$(\nabla f) _{ij} = -(\nabla f) _{ji}$. Analogously to manifolds, the
adjoint operator, the graph <em>divergence</em>
$\mathrm{div} : \ell^2(E) \rightarrow \ell^2(V)$ is defined as</p>

\[(\mathrm{div}\, F) _i = \frac{1}{a _i} \sum _{(i,j) \in E} w _{ij} F _{ij}.\]

<p>It is straightforward to verify that</p>

\[\langle F, \nabla f\rangle _{\ell^2(E)} = \langle \nabla^\ast F,  f\rangle _{\ell^2(V)} = \langle -\mathrm{div}\, F,  f\rangle _{\ell^2(V)}.\]

<h3 id="graph-laplacian">Graph Laplacian</h3>

<p>Having the gradient and the divergence operators defined, we define the
graph <em>Laplacian</em> $\Delta :  \ell^2(V) \rightarrow \ell^2(V)$ as
$\Delta = \nabla^\ast \nabla = - \mathrm{div}\, \nabla$, or, explicitly,</p>

\[(\Delta f) _i = \frac{1}{a _i} \sum _{(i,j) \in E } w _{ij} (f _i - f _j).\]

<p>Observe how this expression manifests the meaning of the Laplacian as
the difference between the value of a field at a vertex and the
(weighed) average of its values in the surrounding.</p>

<p>Since the vertex set is finite, it is convenient to represent the
Laplacian as an $n \times n$ matrix. For that purpose, we denote the
edge weights by the $n \times n$ matrix $\bb{W} = (w _{ij})$, the vertex
weights by the diagonal matrix
$\bb{A} = \mathrm{diag}\{ a _1,\dots, a _n \}$, and by
$\bb{D} =  \mathrm{diag}\left\{ \sum _{j: j\ne i} w _{ij}  \right\}$ the
<em>vertex degree</em> matrix. In this notation, the graph Laplacian is given
by</p>

\[\bb{\Delta} = \bb{A}^{-1} (  \bb{D} - \bb{W} ).\]

<p>Different choices of $\bb{A}$ lead to different definitions of a
Laplacian. For $\bb{A} = \bb{I}$, the <em>unnormalized graph Laplacian</em></p>

\[\bb{\Delta} _{\mathrm{un}} =  \bb{D} - \bb{W}\]

<p>is obtained. The choice
$\bb{A} = \bb{D}^{-1}$ leads to the <em>random walk Laplacian</em></p>

\[\bb{\Delta} _{\mathrm{rw}} =  \bb{I} - \bb{D}^{-1} \, \bb{W}.\]

<p>The
term $\bb{D}^{-1} \, \bb{W}$ in the definition of the above operator can
be interpreted as a transition probability of random walks on the graph,
hence the name. Finally, when the graph is used as a discrete
approximation of the underlying continuous manifold (as is the case of
simplicial complexes a.k.a. <em>meshes</em>), its weight matrices $\bb{A}$ and
$\bb{W}$ are obtained from the discretized metric of the manifold.</p>

<h2 id="fourier-transform-on-non-euclidean-domains">Fourier transform on non-Euclidean domains</h2>

<p>Thus far, we have constructed two types of non-Euclidean domains,
manifolds and graphs, that both had a similarly defined Laplacian
operator. Next, we are going to use the Laplacian to define an analog of
Fourier analysis. For convenience, we are going to construct the Fourier
transform on manifolds; the construction for graphs is straighforwardly
similar.</p>

<p>The Laplacian, being a self-adjoint operator, admits an <em>orthogonal
eigendecomposition</em></p>

\[\Delta \phi _i = \lambda _i \phi _i.\]

<p>The
eigenvalues $\lambda _i$ (called the <em>spectrum</em> of the Laplacian) can be
furthermore shown to be non-negative, a manifestation of the fact that
the Laplacian is a postive semi-definite operator (by analogy, think of
a matrix defined through $\bb{\Delta} = \bb{\nabla}^\Tr \bb{\nabla}$).
On Euclidean domains, the eigenfunctions of the Laplacian are simply
complex exponentials.</p>

<p>A scalar field $f \in L^2(\mathcal{M})$ can be represented in the
Laplacian eigenbasis as</p>

\[f = \sum _{i \ge 0} \hat{f} _i \phi _i\]

<p>with the
coordinates $\hat{\bb{f}} = \{ \hat{f} _i \} _{i \in \mathbb{Z}}$. Because
of orthonormality of the eigenfunctions, the coefficients $\hat{f} _i$
are given by</p>

\[\hat{f} _i = \langle f, \phi _i \rangle _{L^2(\mathcal{M}) }.\]

<p>We will
call the operator
$\mathcal{F} : L^2(\mathcal{M}) \rightarrow \ell^2(\mathbb{Z})$ defined
as</p>

\[\mathcal{F} f = \{  \langle f, \phi _i \rangle _{L^2(\mathcal{M}) } \} _{i \in \mathbb{Z}}\]

<p>as the <em>Fourier transform</em> (analysis) on $\mathcal{M}$. The inverse
(synthesis) transform
$\mathcal{F}^{-1} : \ell^2(\mathbb{Z}) \rightarrow L^2(\mathcal{M})$ is
given by</p>

\[\mathcal{F}^{-1} \hat{\bb{f}} = \sum _{i \ge 0} \hat{f} _i \phi _i.\]

<p>As
before, it is easy to show that the above two operations are adjoint
w.r.t. the standard inner products on $L^2(\mathcal{M})$ and
$\ell^2(\mathbb{Z})$.</p>

<h2 id="convolution-on-non-euclidean-domains">Convolution on non-Euclidean domains</h2>

<p>Recall that one of the principal properties the Fourier transform
enjoyed on Euclidean domains was the fact that it diagonalized Toeplitz
operators. In fact, we had the property</p>

\[\mathcal{F}(\bb{f} \ast \bb{g}) =  \mathcal{F}\bb{f} \cdot \mathcal{F} \bb{g}.\]

<p>Unfortunately, the absense of a trivially defined translation group on
general non-Euclidean domains does not allow to generalize convolution,
which makes the left-hand-side of the above equation undefined. However,
the right-hand-side, being simply an element-wise product of frequency
responses, is perfectly defined, so we will use it to <em>define</em>
convolution on the non-Euclidean domain as</p>

\[f \ast g = \mathcal{F}^{-1} (  \mathcal{F} f \cdot \mathcal{F} g ) = \sum _{i \ge 0}  \langle f, \phi _i \rangle _{L^2(\mathcal{M}) }  \langle g, \phi _i \rangle _{L^2(\mathcal{M}) } \phi _i.\]

<p>The standard shift-invariance (or, more precisely,
translation-equivariance) property of convolution on Euclidean domains
is lost of course. Using a signal processing metaphor, it can be
interpreted as a position-dependent filter, with the impulse response
that can differ significantly at different locations in the domain.</p>

<p>When dealing with discrete domain such as graphs (which we will
henceforth assume for convenience), the Fourier transform and its
inverse have a matrix form. Note that the eigendecomposition of the
Laplacian $\bb{\Delta}$ can be written as
$\bb{\Delta} = \bb{\Phi} \bb{\Lambda} \bb{\Phi}^\Tr$, where $\bb{\Phi}$
has the eigenvectors as its columns and
$\bb{\Lambda} = \mathrm{\diag}\{ \lambda _1,\dots,\lambda _n\}$.
Representing vertex fields as $n$-dimensional column vectors, the
analysis (the forward transform) can be written as</p>

\[\hat{\bb{f}} = \mathcal{F} \bb{f} = \bb{\Phi}^\Tr \bb{f} = ( \langle \bb{f}, \bb{\phi} _1  \rangle, \dots, \langle \bb{f}, \bb{\phi} _n  \rangle )^\Tr;\]

<p>likewise, the synthesis operator (the inverse transform) assume the form</p>

\[\bb{f} = \mathcal{F}^{-1} \hat{\bb{f}} = \bb{\Phi} \hat{\bb{f}}  = \bb{\phi} _1 \hat{f} _1 + \dots + \bb{\phi} _n \hat{f} _n.\]

<p>In this notation, the convolution of two fields $\bb{f}$ and $\bb{g}$
can be written as</p>

\[\bb{f} \ast \bb{g} = \bb{\Phi} ( (\bb{\Phi} \bb{f}) \odot (\bb{\Phi} \bb{g}) ),\]

<p>where $\odot$ denotes the Hadamard (element-wise) product.</p>

<p>Convolution on Euclidean domains was an operation commuting with any
translation-equivariant (Toeplitz) operator, including the Laplacian. In
generalizing it to non-Euclidean domains, we only demanded commutativity
with the Laplacian.</p>

<h1 id="spectral-cnn">Spectral CNN</h1>

<p>The spectral definition of a convolution-like operation on a
non-Euclidean domain allows to parametrize the action of a filter as</p>

\[\mathcal{W} \bb{f} = \bb{\Phi} \hat{\bb{W}} \bb{\Phi}^\Tr \bb{f},\]

<p>where $\hat{\bb{W}}$ is a diagonal weight matrix containing the filter’s
frequency response on the diagonal. In the space domain, it amounts to
applying the operator $\bb{W} = \bb{\Phi} \hat{\bb{W}} \bb{\Phi}^\Tr$ to
$\bb{f}$, by computing the inner products of $\bb{f}$ with every row of
$\bb{W}$ and stacking the resulting numbers into a vertex field.
Different weight matrices $\hat{\bb{W}}$ realize different such
operators.</p>

<p>Note that the definition is basis-dependent: a change in the domain,
and, consequently, in $\bb{\Phi}$ may translate the same $\hat{\bb{W}}$
into a completely different operator. Therefore, this construction must
assume the domain fixed; if we learn the weights $\hat{\bb{W}}$, they
will typically generalize rather poorly even to similarly-looking
domains. Siuch a complication did not exist on Euclidean domains.</p>

<p>Armed with the notion of a generalized convolution on non-Euclidean
domains, we can mimick the construction of a regular CNN. For this
purpose, we construct a <em>spectral convolutional layer</em> accepting an
$m$-dimensional vertex field $\bb{x} = (\bb{x}^1,\dots,\bb{x}^m)$ and
outputting an $m’$-dimensional vertex field
$\bb{y}= (\bb{y}^1,\dots,\bb{y}^{m’})$, whose $i$-the dimension is
defined according to</p>

\[\bb{y} _j = \varphi\left( \sum _{i=1}^m \bb{\Phi} \hat{\bb{W}}^{ij} \bb{\Phi}^\Tr \bb{x}^i \right),\]

<p>where $\varphi$ is an element-wise non-linearity such as ReLU, and
$\hat{\bb{W}}^{ij}$, are diagonal matrices parametrizing the filters of
the layer.</p>

<h2 id="strided-convolution">Strided convolution</h2>

<p>Recall that a typical Euclidean CNN architecture used strided
convolutions of the form</p>

\[(\downarrow _{\bb{p}} \left( \bb{w} \ast \bb{x} \right)) _{\bb{k}} =  (\bb{w} \ast \bb{x} ) _{ \bb{p} \odot \bb{k} } = \sum _{ (i _1,\dots,i _d) } w _{i _1,\dots,i _d} \, x _{p _1 k _1 - i _1,\dots,p _d k _d-i _d  },\]

<p>where $\bb{p} = (p _1,\dots,p _d)$ is a $d$-dimensional vector of strides.
This can be thought of projecting the result of the convolution
$\bb{w}\ast\bb{x}$ performed on $\mathbb{Z}^d$ onto the coarser domain
$\downarrow _{\bb{p}} \mathbb{Z}^d$. The subsampling operator
$\downarrow _{\bb{p}}$ can be thought of as a projection of a signal on
$\mathbb{Z}^d$ onto $\downarrow _{\bb{p}} \mathbb{Z}^d$.</p>

<p>The non-Euclidean analog can be constructed along the same lines. Let
$\mathcal{G}$ be the original domain of size $n$ with the Laplacian
$\bb{\Delta} = \bb{\Phi} \bb{\Lambda} \bb{\Phi}^\Tr$, and let
$\tilde{\mathcal{G}}$ be its coarsened (sub-sampled) version containing
$\tilde{n} = \alpha n &lt; n$ vertices. We denote by
$\tilde{\bb{\Delta}} = \tilde{\bb{\Phi}} \tilde{\bb{\Lambda}} \tilde{\bb{\Phi}}^\Tr$
the corresponding Laplacian and its eigendecomposition. To keep the
previous notation, we denote by
$\downarrow _\alpha : \mathcal{G} \rightarrow \tilde{\mathcal{G}}$ the
projection onto the coarse domain, i.e., $\downarrow _\alpha$ maps a
vertex field on $\mathcal{G}$ to a vertex field on
$\tilde{\mathcal{G}}$. In matrix form, $\downarrow _\alpha$ is an
$\tilde{n} \times n$ matrix whose $i$-th row encodes the position of the
$i$-th vertex of the coarse domain $\tilde{\mathcal{G}}$ in the fine
domain $\mathcal{G}$.</p>

<p>The eigenvectors $\bb{\Phi}$ and $\tilde{\bb{\Phi}}$ of the fine and the
coarse Laplacians, $\bb{\Delta}$ and $\tilde{\bb{\Delta}}$, satisfy the
following multi-resolution property:</p>

\[\tilde{\bb{\Phi}} \, \approx \,\, \downarrow _\alpha \bb{\Phi} \bb{P} _{\alpha},\]

<p>where the $n \times \tilde{n}$ matrix</p>

\[\bb{P} _\alpha = \left( \begin{array}{c} \bb{I} _{\alpha n} \\ \bb{0} \end{array} \right)\]

<p>denotes the projection onto the lowest $\tilde{n} = \alpha n$
frequencies. This property essentially means that only the first
$k = \alpha n$ components of the spectrum can be retained. Thus, the
strided convolutional layer assumes the form</p>

\[\bb{y} _j = \varphi\left( \sum _{i=1}^m \tilde{\bb{\Phi}} _k \hat{\bb{W}}^{ij} \bb{\Phi} _k^\Tr \bb{x}^i \right),\]

<p>where $\bb{\Phi} _k = (\bb{\phi} _1,\dots\bb{\phi} _k)$ is the <em>truncated</em>
eigenbasis of the fine Laplacian containing the first $k$ eigenvectors,
and the weight matrices $\hat{\bb{W}}^{ij}$ are now
$\tilde{n} \times \tilde{n}$. The layer accepts an $m$-dimensional
vertex field $\bb{x} = (\bb{x}^1,\dots,\bb{x}^m)$ on $\mathcal{G}$ as
the input and produces and $m’$-dimensional vertex field
$\bb{y} = (\bb{y}^1,\dots,\bb{y}^{m’})$ on $\tilde{\mathcal{G}}$ as the
output.</p>

<h2 id="spatial-localization">Spatial localization</h2>

<p>Note that in our construction of a spectral convolutional layer, each
weight matrix has $k=\mathcal{O}(n)$ degrees of freedom, so that each
layer has $\mathcal{O}(nmm’)$ degrees of freedom, unlike the regular
CNN, in which the layer was parametrized in the spatial domain by a
fixed-size kernel with the number of parameters independent on the
domain size $n$. In order to keep the number of parameters under control
and avoid overfitting, we would like to impose spatial localization onto
the weights $\hat{\bb{W}}^{ij}$, that is, ensure that the vertex fields
defined by every row of the operator
$\bb{W} = \bb{\Phi} \hat{\bb{W}} \bb{\Phi}^\Tr$ are spatially localized.</p>

<p>On a Euclidean domain, the spatial localization of a signal
$w : \mathbb{Z} \rightarrow \mathbb{R}$ is controlled by the decay of
its moments, defined as</p>

\[\mu _p^2(w) = \sum _{k \in \mathbb{Z}}  k^{2p} w^2 _k = \| k^p \cdot w _k  \| _{\ell^2(\mathbb{Z}) }^2.\]

<p>The faster $\mu _p^2(w)$ vanishes as $p$ increases, the more localized is
$w$. From</p>

\[\frac{\partial}{\partial \xi} \mathcal{F}w  = \frac{\partial}{\partial \xi} \left( \sum _{k \in \mathbb{Z}} w _k e^{-i 2\pi \xi k} \right) =   \sum _{k \in \mathbb{Z}}  -i 2\pi k w _k e^{-i 2\pi \xi k} =  -i 2\pi  \mathcal{F}(k w _k)\]

<p>we obtain the property</p>

\[k^p \cdot w _k  \, \mathop{\longleftrightarrow}^{\mathcal{F}} \, \left( \frac{i}{2\pi} \right)^p \frac{\partial^p \hat{w}}{\partial \xi^p}.\]

<p>Invoking Parseval’s identity,</p>

\[\mu _p^2(w) = \| k^p \cdot w _k  \| _{\ell^2(\mathbb{Z}) }^2 =  \left\|   \left( \frac{i}{2\pi} \right)^p \frac{\partial^p \hat{w}}{\partial \xi^p}  \right\| _{L^2([0,1]) }^2 = \frac{1}{(2\pi)^{2p}} \int _{[0,1]} \left|  \frac{\partial^p \hat{w}(\xi) }{\partial \xi^p} \right|^{2}   d\xi.\]

<p>This result implies that fast decay of $\mu _p^2(w)$ implies fast decay
of the derivatives of $\hat{w}$, or, said differently, localization in
the spatial domain is equivalent to smoothness in the frequency domain
(the fact that smoothness is opposite to localization brings forth the
renowned Heisenberg’s uncertainty principle). Smoothness of the
frequency response $\hat{w}$ can be asserted by representing it in an
underdetermined smooth basis or, equivalently, specifying it only at a
small set of frequencies and completing the rest via some smooth
interpolation.</p>

<p>This idea can be generalized to non-Euclidean domains. The only
complication is that while in $\RR^d$ we had a trivia notion of
smoothness arising in the spectrum, since the similarity between two
basis functions $\phi^{\bb{\xi}} = e^{i 2 \pi \bb{x}^\Tr \bb{\xi}}$ and
$\phi^{\bb{\xi}’} = e^{i 2 \pi \bb{x}^\Tr \bb{\xi}’}$ could be
quantified as the distance $|\bb{\xi} - \bb{\xi}’ |$, there is not
such a standard notion in the spectrum of a general non-Euclidean
domain. A formal way to define smoothness is by constructing a dual
graph whose weights $w^\ast _{ij}$ reflect the similarity between the
eigenvectors $\bb{\phi} _i$ and $\bb{\phi} _j$ of the Laplacian of the
original (primal) graph. The question of how to define such a dual graph
the smoothness on which will lead to maximal localization on the primal
graph is still open. However, empirical evidence shows that at least in
some cases, the simple definition of
$w^\ast _{ij} = | \lambda _i - \lambda _j|$ leads to reasonable
localization.</p>

<p>With this notion of smoothness in mind, we fix a set of $q$ smooth basis
functions $\beta _1(\lambda), \dots, \beta _q(\lambda)$ (e.g., cubic
splines) and sample them at
$\lambda \in \{ \lambda _1,\dots, \lambda _k \}$. We arrange the samples
into a $k \times q$ matrix $\bb{B}$ with the elements
$b _{rs} = \beta _s(\lambda _r)$. The spectral weight matrices
$\hat{\bb{W}}^{ij}$ can now be defined as</p>

\[\hat{\bb{W}}^{ij} = \mathrm{diag}\{ \bb{B} \bb{\alpha}^{ij}  \},\]

<p>where $\bb{\alpha}^{ij}$ are $q$-dimensional interpolation coefficients.
In order to render the layer complexity independent of the domain size,
one has to choose $q = \mathcal{O}(1)$.</p>

<h1 id="spatial-cnn">Spatial CNN</h1>

<p>One of the main disadvantages of the spectral construction of a
convolutional layer is its high computational complexity. The
multiplication by $\bb{\Phi}$ and $\bb{\Phi}^\Tr$ in the forward and
backward passes require $\mathcal{O}(n^2)$ operations, which quickly
becomes prohibitively expensive for large domains. Unlike Euclidean
domains on which the forward and inverse Fourier transforms can be
carried out using FFT in $\mathcal{O}(n \log n)$ operations, no such
fast algorithms exist for general non-Euclidean domains. In what
follows, we will reformulate the convolutional layer in a way free of
the costly Laplacian eigendecomposition and explicit projection on its
basis.</p>

<p>Let us substitute
$\hat{\bb{W}} = \mathrm{diag}\{ \bb{B} \bb{\alpha}  \}$ and examine the
spatial representation of the linear part of the layer:</p>

\[\begin{aligned}
\bb{W} &amp;=&amp; \bb{\Phi} _k \hat{\bb{W}} \bb{\Phi} _k^\Tr  = \bb{\Phi} _k \left( \begin{array}{ccc}
\sum _{i = 1}^q \alpha _i \beta _i(\lambda _1) &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp;  \sum _{i = 1}^q \alpha _i \beta _i(\lambda _n) 
\end{array}   \right) \bb{\Phi} _k^\Tr \end{aligned}\]

<p>(note that we
assumed $\beta _i(\lambda) = 0$ for $\lambda &gt; \lambda _k$). Denoting by</p>

\[b(\lambda) =  \sum _{i = 1}^q \alpha _i \beta _i(\lambda),\]

<p>we have</p>

\[\bb{W} = \bb{\Phi} \, \mathrm{diag}\{ b(\lambda _1), \dots, b(\lambda _n) \} \, \bb{\Phi}^\Tr.\]

<p>(note that we assumed $b(\lambda) = 0$ for $\lambda &gt; \lambda _k$).</p>

<p>Since $b(\lambda)$ is typically a polynomial (of degree $3$ in case of
cubic splines), let us examine how to rewrite it directly in the spatial
domain. Let $\bb{\Delta} = \bb{\Phi} \bb{\Lambda} \bb{\Phi}^\Tr$ be the
eigendecomposition of the Laplacian, and suppose we woud like to compute
$\bb{\Delta}^p$ for some integer power $p$. Then,</p>

\[\bb{\Delta}^p =
\bb{\Phi} \bb{\Lambda} \bb{\Phi}^\Tr  \cdots   \bb{\Phi} \bb{\Lambda} \bb{\Phi}^\Tr =
\bb{\Phi} \bb{\Lambda}^p \bb{\Phi}^\Tr = \bb{\Phi} \, \mathrm{diag}\{ \lambda _1^p, \dots, \lambda _n^p \} \, \bb{\Phi}^\Tr.\]

<p>Using linearity, we can conclude that for any polynomial</p>

\[b(\lambda) = \sum _{i=0}^{r} \alpha _i \lambda^i,\]

<p>one has</p>

\[b(\bb{\Delta}) =  \sum _{i=0}^{r} \alpha _i \bb{\Delta}^i =
\bb{\Phi} \, \mathrm{diag} \left\{ b(\lambda _1),\dots, b(\lambda _n) \right\} \, \bb{\Phi}^\Tr.\]

<p>In other words, the expensive right-hand-side can be simply evaluated as
applying the polynomial $b$ directly to the Laplacian. The Laplacian is
typically a sparse $n \times n$ matrix with $\mathcal{O}(1)$ non-zero
entries in every row. In such cases, computing its powers takes
$\mathcal{O}(n)$ operations, and the entire calculation is
$\mathcal{O}(nr)$. Also note that since the Laplacian is a local
operator acting on $1$-rings, its highest power $\bb{\Delta}^{r}$ will
act on $r$-rings, keeping the operator $b(\bb{\Delta})$ spatially
localized.</p>

<p>Using this observation, we can reformulate the convolutional layer
directly in the spatial domain as</p>

\[\bb{y} _j = \varphi\left( \sum _{i=1}^m  \sum _{k=0}^r \alpha _k^{ij} \bb{\Delta}^k  \bb{x}^i \right),\]

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The term <em>shift-invariant</em> is so abundant in the signal processing
and machine learning literature that we will not even attempt to
change this unfortunate fact. However, it is worth noting that the
correct mathematical term would be <em>shift-equivariant</em>. In general,
let $f : \mathbb{U} \rightarrow \mathbb{V}$ be an operator mapping
from some domain $\mathbb{U}$ to some co-domain $\mathbb{V}$, and
let $\mathcal{G}$ be a group of transformations that can be applied
both to the domain and the co-domain. The operator $f$ is said
<em>invariant</em> to the action of $\mathcal{G}$ if $f \circ \tau = f$ for
every $\tau \in \mathcal{G}$. On the other hand, the operator is
<em>equivariant</em> if $f \circ \tau = \tau \circ f$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/cs236781/semesters/w22/lecture_notes/lecture_07/" class="pagination--pager" title="Lecture 7: Reinforcement learning
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    <div class="page__footer">
      <footer>
        
<!-- Technion and VISTA logos --><script>

var logo_element = '\
<div class="technion-logo"> \
    <a href="https://cs.technion.ac.il"> \
        <img src="/cs236781/semesters/w22/assets/images/cs_technion-logo.png" alt="Technion"> \
    </a> \
</div> \
';

document
    .querySelector('.masthead__inner-wrap')
    .insertAdjacentHTML('afterbegin', logo_element);

var logo_element = '\
<div class="vista-logo"> \
    <a href="https://vista.cs.technion.ac.il" > \
        <img src="/cs236781/semesters/w22/assets/images/vista-logo-bw.png" alt="VISTA"> \
    </a> \
</div> \
';

var footerNodes = document.getElementsByTagName("FOOTER")
var footerNode = footerNodes[footerNodes.length - 1];
footerNode.insertAdjacentHTML('afterend', logo_element);

</script>
<!-- Mathjax support --><!-- see: http://haixing-hu.github.io/programming/2013/09/20/how-to-use-mathjax-in-jekyll-generated-github-pages/ -->
<!-- also: http://docs.mathjax.org/en/latest/tex.html for defning mathjax macros -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noErrors: { disabled: true },
      Macros: {
        // Each def here is an array: [<macro>, <num_params>]
        // Aviv's defs
        bold: ["{\\bf #1}",1],
        m: ["\\boldsymbol {#1}",1],             // matrix
        mt: ["\\boldsymbol {#1}^\\top",1],      // transposed matrix
        v: ["\\boldsymbol {#1}",1],             // vector
        vt: ["\\boldsymbol {#1}^\\top",1],      // transposed vector
        diag: ["\\mathop{\\mathrm{diag}}"],
        trace: ["\\mathop{\\mathrm{tr}}"],
        rank: ["\\mathop{\\mathrm{rank}}"],
        set: ["\\mathbb {#1}",1],
        rvar: ["\\mathrm{#1}",1],               // random variable
        rvec: ["\\boldsymbol{\\mathrm{#1}}",1], // random vector

        // Alex's defs
        bm: ["{\\bf #1}",1],
        bb: ["{\\bm{\\mathrm{#1}}}",1],
        spn: ["\\mathrm{span}\\left\\{ {#1} \\right\\}",1],
        vec: ["\\mathrm{vec}"],
        dx:  ["\\bb{dx}"], dX:  ["\\bb{dX}"], dy:  ["\\bb{dy}"], du:  ["\\bb{du}"],
        df:  ["\\bb{df}"], dg:  ["\\bb{dg}"],
        dphi:  ["\\bb{d\\varphi}"],
        Tr: ["\\top"],
        RR: ["\\set{R}"],
        mathpzc: ["\\rvar{#1}", 1],
        mathpzcb: ["\\rvec{#1}", 1],
        ind: ["\\unicode{x1D7D9}"]
      }
    },

  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
    processEscapes: true,
  },

  "HTML-CSS": {
     fonts: ["TeX"]
  },

});
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- Copyright notice support on single pages -->
<script>
var copyright_element = '\
    <p class="page__meta" style="margin-top: -0.5em;"> \
    <i class="far fa-copyright"></i> \
    Prof. Alex Bronstein \
    </p> \
';

first_header = document.getElementsByTagName('header')[0]
first_header.insertAdjacentHTML('beforeend', copyright_element);
</script>


        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://vista.cs.technion.ac.il"><i class="fas fa-fw fa-link" aria-hidden="true"></i> VISTA Lab</a></li>
        
      
        
          <li><a href="https://github.com/vistalab-technion"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    <li><a href="/cs236781/semesters/w22/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 VISTA Lab. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/cs236781/semesters/w22/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"></script>




<script src="/cs236781/semesters/w22/assets/js/lunr/lunr.min.js"></script>
<script src="/cs236781/semesters/w22/assets/js/lunr/lunr-store.js"></script>
<script src="/cs236781/semesters/w22/assets/js/lunr/lunr-en.js"></script>





  </body>
</html>